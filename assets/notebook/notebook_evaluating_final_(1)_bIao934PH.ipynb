{"cells": [{"metadata": {"id": "68a649cb-7042-4198-a06d-57eff7f73b23", "scrolled": true}, "cell_type": "code", "source": "%pip install python-dotenv # Install the missing module 'dotenv'\n%pip install transformers datasets evaluate peft trl bitsandbytes accelerate\n%pip install huggingface\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting python-dotenv\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.0.1\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (4.17.0)\nCollecting datasets\n  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (2022.3.15)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: sacremoses in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.0.53)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (4.66.4)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (1.5.3)\nCollecting requests (from transformers)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (3.9.5)\nCollecting huggingface-hub<1.0,>=0.1.0 (from transformers)\n  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: psutil in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from peft) (5.9.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from peft) (2.0.1)\nCollecting safetensors (from peft)\n  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\nCollecting tokenizers<0.20,>=0.19 (from transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nCollecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.1.0->transformers)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nCollecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: click in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sacremoses->transformers) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sacremoses->transformers) (1.1.1)\nDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m149.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading rich-13.8.1-py3-none-any.whl (241 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: xxhash, typing-extensions, shtab, safetensors, requests, pyarrow, mdurl, fsspec, docstring-parser, dill, multiprocess, markdown-it-py, huggingface-hub, tokenizers, rich, bitsandbytes, accelerate, tyro, transformers, datasets, trl, peft, evaluate\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Uninstalling typing_extensions-4.4.0:\n      Successfully uninstalled typing_extensions-4.4.0\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2022.11.0\n    Uninstalling fsspec-2022.11.0:\n      Successfully uninstalled fsspec-2022.11.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.6\n    Uninstalling dill-0.3.6:\n      Successfully uninstalled dill-0.3.6\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.6.0\n    Uninstalling huggingface-hub-0.6.0:\n      Successfully uninstalled huggingface-hub-0.6.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.17.0\n    Uninstalling transformers-4.17.0:\n      Successfully uninstalled transformers-4.17.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwatson-nlp 4.1.3 requires pyarrow==11.0.0, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.34.2 bitsandbytes-0.43.3 datasets-3.0.0 dill-0.3.8 docstring-parser-0.16 evaluate-0.4.3 fsspec-2024.6.1 huggingface-hub-0.24.7 markdown-it-py-3.0.0 mdurl-0.1.2 multiprocess-0.70.16 peft-0.12.0 pyarrow-17.0.0 requests-2.32.3 rich-13.8.1 safetensors-0.4.5 shtab-1.7.1 tokenizers-0.19.1 transformers-4.44.2 trl-0.10.1 typing-extensions-4.12.2 tyro-0.8.10 xxhash-3.5.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {"id": "5bf69b08-6348-43d7-947f-45962e80a3fa"}, "cell_type": "code", "source": "import os\nfrom dotenv import load_dotenv\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom ibm_watson_studio_lib import access_project_or_space\n\nwslib = access_project_or_space({\n        'token': 'p-2+M7N412nNMq+LSbsjANoOoQ==;LFeF3V6i+F/jEnezq8oOQA==:l0bUpeHOW5rp8xq20UiCjJQyak+tK37f7uTyFZsV7YvvbFmQbYhtaO3KgtiCa1qahvIu57LYjESD5n0TXPH5u0ZHGef4njBD5A==',\n        'project_id': 'bdd13a82-ee92-406c-bc3d-fc0690f7cb1e'\n})\n\nwslib.download_file('config.env')\nload_dotenv('config.env')\n\n# Connection variables\napi_key = os.getenv(\"API_KEY\", None)\nibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None) \nproject_id = os.getenv(\"PROJECT_ID\", None)\ncreds = {\n    \"url\": ibm_cloud_url,\n    \"apikey\": api_key \n}\naccess_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n).get_token()\n\nprint(api_key)\n# print(access_token)\nwslib.download_file('tool.py')\nwslib.download_file('evaluating.py')\nwslib.download_file('save.py')\n", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "R4Ura8MOO50SxIpnaApHAlK5X0sJ9VyCvyPs91xTdmEK\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 2, "data": {"text/plain": "{'file_name': 'save.py', 'summary': ['loaded data', 'saved to file']}"}, "metadata": {}}]}, {"metadata": {"id": "3b3f0274-42fd-4222-9b1d-1c4d35a291a0"}, "cell_type": "code", "source": "import torch\nimport transformers\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom transformers import Trainer\nimport json\nimport torch.nn as nn\n\nimport tool\n\ntorch.cuda.is_available()", "execution_count": 4, "outputs": [{"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"id": "2fffa942-d289-42b9-9117-4a0b3a378bb6"}, "cell_type": "code", "source": "## import os\nfrom dotenv import load_dotenv\n\nload_dotenv('.env')\nprint(os.getenv(\"TOKEN_HF\"))\n# model_name ='fb-opt-125m-sql'\nmodel_name ='granite-7b-base_sql-v0.1'\n# model_name ='granite-7b-base_Original_withPrompt'", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "None\n", "name": "stdout"}]}, {"metadata": {"id": "081bc904-ee30-4444-911a-f719e782a06e"}, "cell_type": "code", "source": "from huggingface_hub import login\n\nlogin(token=\"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\")", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /home/wsuser/.cache/huggingface/token\nLogin successful\n", "name": "stdout"}]}, {"metadata": {"id": "8c5346fd-63aa-43d4-98b8-823bbf8c1218"}, "cell_type": "code", "source": "# %pip install git+https://github.com/huggingface/peft.git\n", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# import torch\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from peft import PeftModel, PeftConfig\n\n# tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-7b-base\")\n# model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-7b-base\", return_dict=True, device_map='auto',\n# #                                              torch_dtype=torch.float16, \n#                                              load_in_8bit=True\n#                                             )\n# base_model_name = 'ibm-granite/granite-7b-base'\n# sql_model = model", "execution_count": 8, "outputs": []}, {"metadata": {"id": "a14aeb0f-1c01-4e9b-9c1b-b155e88e041d"}, "cell_type": "code", "source": "# loading the model from HF\nmodel, tokenizer, sql_model = tool.load_model_from_HF(model_name, quantization='F', base_model_name = 'ibm-granite/granite-7b-base', HF_user = 'koukoudzz')\n# Set padding to be on the left side for decoder-only architecture\ntokenizer.padding_side = 'left'\n\n# Assign pad token if not already set\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "adapter_config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e58e706b510e489e932aa7f65ee36790"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "19ddb3a69c3d4da48aa4cd2b1cab3f44"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0092118ae6d1467eafc527cf3cafae33"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2b80f0c8f4e2482781b063571e86e292"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "745c380105454e218b243d436f8666ae"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cef555d612c74295ad127d1adea53853"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0d47c100fda24ca4a5dfb6d9d21aae1f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9f25d032c3784f388530ffb8ce2d47de"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6eb94f1ef8d7448d88ba9c69ec85bc84"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00006-of-00006.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "09ccf5de63e24d7d82c625beb962a562"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0b0f7e6d0c2e4e6988185ab98e3071eb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0430aed10fa74cc6bcb615d40b736ac7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/872 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2793d094d0734e3a99292b70ca1a26d5"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ae69166a9ff140c5b5c649c0c788a06d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c5340b17d7a24c48a6c9a48bcda5a012"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6ab94cc9ea34451bb05636adeec88fb5"}}, "metadata": {}}, {"output_type": "stream", "text": "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n", "name": "stderr"}, {"output_type": "display_data", "data": {"text/plain": "adapter_model.safetensors:   0%|          | 0.00/160M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9fd92c7f2452460ba48a5c18dab74cb1"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# model_name = model_name + '_withPrompt'", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_name\n!nvidia-smi", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Tue Sep 17 06:42:54 2024       \r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  Tesla V100-PCIE-16GB           Off |   00000000:AF:00.0 Off |                    0 |\r\n| N/A   33C    P0             38W /  250W |   13542MiB /  16384MiB |      0%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n                                                                                         \r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n+-----------------------------------------------------------------------------------------+\r\n", "name": "stdout"}]}, {"metadata": {"id": "13ebcadd-0a21-455d-b99f-0f31f2721380"}, "cell_type": "code", "source": "from functools import partial\n# 2- Load the dataset:\nfrom datasets import load_dataset\n\n# dataset = load_dataset(\"OussamaAzz/sql_dataset_cleaned\")\ndataset = load_dataset(\"OussamaAzz/final-sql-dataset\")\n# dataset = load_dataset(\"OussamaAzz/instruction-sql-dataset\")\n", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "README.md:   0%|          | 0.00/635 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "75e66fcb82cf4730b82d5ef0f781923b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/2.64M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4ce6d69675394c328d0bed4d64c94262"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/145k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f1d41d0fb60e417ab3047c80c13c6624"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/104k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "63a0de539e8243cb8292c0a74c42b7d7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating train split:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6ab10fe5cbd64b10a0f41dfd1ba0eaa0"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "da110dcea1cd4393a53dbedc2571b7f7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "01ee9a210ccd4502b07c3f838926e057"}}, "metadata": {}}]}, {"metadata": {"id": "7362442a-a840-4782-baeb-6d21b80fa95d"}, "cell_type": "code", "source": "def replace_eos(text, tokenizer):\n    return {\"text\": [t.replace('</s>', tokenizer.eos_token) for t in text[\"text\"]]}\n\ndef add_eos_token(text, tokenizer):\n    return {\"text\": [t + tokenizer.eos_token for t in text[\"text\"]]}\n\n# dataset['train'][\"text\"][0].replace('</s>','')/\n\nreplace_eos_with_tokenizer = partial(replace_eos, tokenizer=tokenizer)\nadd_eos_with_tokenizer = partial(add_eos_token, tokenizer=tokenizer)", "execution_count": 13, "outputs": []}, {"metadata": {"id": "52175ab7-7d8e-430f-a176-15c5d9b877ab"}, "cell_type": "code", "source": "# 4- Tokenizing the dataset:\n\n# Function to tokenize inputs and align labels\ndef tokenize_function(examples):\n    tokenized_inputs = tokenizer(examples[\"text\"],\n#                                 padding=\"max_length\",\n#                                 truncation=True,\n#                                 max_length = 512,\n#                                 return_overflowing_tokens=False,\n                                 )\n    # labels = tokenized_inputs[\"input_ids\"].copy()  # Copy input_ids to use as labels\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"],\n            \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n            }\n\ntrain_dataset = dataset['train']\nval_dataset = dataset['validation']\ntest_dataset = dataset['test']\n\n\nval_data = dataset['validation']['source']\ntest_data = dataset['test']['source']\n# test_data = dataset['test']['source']\n\ntrain_dataset = train_dataset.map(add_eos_with_tokenizer, batched=True)\nval_dataset = val_dataset.map(add_eos_with_tokenizer, batched=True)\ntest_dataset = test_dataset.map(add_eos_with_tokenizer, batched=True)\nprint(train_dataset['text'][0])\nprint(train_dataset['text'][0])\n\n\ntokenized_datasets = train_dataset.map(tokenize_function, batched=True)\ntokenized_datasets_val = val_dataset.map(tokenize_function, batched=True)\ntokenized_datasets_test = test_dataset.map(tokenize_function, batched=True)\n\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"source\"])\ntokenized_datasets_val = tokenized_datasets_val.remove_columns([\"text\", \"source\"])\ntokenized_datasets_test = tokenized_datasets_test.remove_columns([\"text\", \"source\"])\n\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntokenized_datasets_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntokenized_datasets_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7738329c34d94d8583916af280dd882a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6e3cba3d887c41d298a8097de350a298"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c557aafd07724a2b9ccf2322e62636d6"}}, "metadata": {}}, {"output_type": "stream", "text": "### QUESTION\nWhat is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\n\n### CONTEXT\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\n\n### ANSWER\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"</s>\n### QUESTION\nWhat is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\n\n### CONTEXT\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\n\n### ANSWER\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"</s>\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "be473ef5684f4b3e893be63317a26d8b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "25c0a07527c04e04a4f2febad7212cbb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5bbe2f798d194cffae25fb6f2438fcc6"}}, "metadata": {}}]}, {"metadata": {"id": "2d728129-dfdc-4ee3-ad86-2c1ccbf05f0d"}, "cell_type": "code", "source": "from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    # Get all the input_ids and attention_masks from the batch\n    input_ids = [item['input_ids'] for item in batch]\n    attention_masks = [item['attention_mask'] for item in batch]\n    \n    # Pad all sequences in the batch to the length of the longest sequence\n    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n    \n    return {\n        'input_ids': input_ids_padded,\n        'attention_mask': attention_masks_padded\n    }\n", "execution_count": 15, "outputs": []}, {"metadata": {"id": "a1fa3a0f-3429-442e-969d-501c1b9b1c04"}, "cell_type": "code", "source": "from datasets import Dataset\n\n# Assume `tokenizer` is already defined and imported\nEOS_TOKEN = tokenizer.eos_token  # Ensure this is defined\n\ndef get_prompt(data, include_answer=False):\n    if include_answer:\n        prompt = tool.create_prompt_with_answer_v2(**data) + EOS_TOKEN\n    else:\n        prompt = tool.create_prompt_v2(data['question'],data['context'])\n    \n    return prompt\n\ndef get_answer(data):\n    return [d['answer'] + EOS_TOKEN for d in data]\n\n# Function to convert data to Dataset\ndef convert_to_dataset(data, tokenizer, include_labels=True, include_answer=False):\n    # Assume `tokenizer` is already defined and imported\n    EOS_TOKEN = tokenizer.eos_token  \n    # Assuming `tool.create_prompt_with_answer_v2(**d)` returns a string\n    # text = [tool.create_prompt_with_answer_v2(**d) + EOS_TOKEN for d in data]\n    text = [get_prompt(d, include_answer) for d in data]\n    answer = []\n    # if not include_answer:\n    answer = get_answer(data)\n    \n    if include_labels:\n        # Creating new labels for the dataset\n        labels = [i for i in range(len(data))]\n        return Dataset.from_dict({\"text\": text, \"labels\": labels, \"answer\": answer})\n    else:\n        return Dataset.from_dict({\"text\": text, \"answer\": answer})\n\n# Convert train and validation data to datasets\n# train_dataset = convert_to_dataset(data, include_labels=False, tokenizer=tokenizer, include_answer=True)\n# val_dataset = convert_to_dataset(val_data, include_labels=True, tokenizer=tokenizer, include_answer=False)\n\ntrain_dataset = convert_to_dataset(train_dataset['source'], include_labels=False, tokenizer=tokenizer, include_answer=True)\nval_dataset = convert_to_dataset(val_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=False)\ntest_dataset = convert_to_dataset(test_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=False)\n\n\n# Display an example from the datasets\n# print(train_dataset[2000])\nval_data1 = val_dataset\nprint(val_data1[0])\ntokenized_datasets_val = val_data1.map(tokenize_function, batched=True)\ntokenized_datasets_val = tokenized_datasets_val.remove_columns([\"text\", \"answer\",'labels'])\nprint(tokenized_datasets_val[1])\ntokenized_datasets_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n#-------------------------------------------------------\n\ntest_data1 = test_dataset\nprint(test_data1[0])\ntokenized_datasets_test = test_data1.map(tokenize_function, batched=True)\ntokenized_datasets_test = tokenized_datasets_test.remove_columns([\"text\", \"answer\",'labels'])\nprint(tokenized_datasets_test[1])\ntokenized_datasets_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\neval_text_dataloader = DataLoader(\n#     tokenized_datasets_val,\n    tokenized_datasets_test,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=collate_fn  # Use the custom collate function\n)\n# DataLoader(tokenized_datasets_val, batch_size=8)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "{'text': '### QUESTION\\nHow many countries were sampled in the index created by The Economist, published in 2007 and ranked 2nd in the LA Ranking?\\n\\n### CONTEXT\\nCREATE TABLE table_19948664_1 (countries_sampled INTEGER, ranking_la__2_ VARCHAR, author___editor___source VARCHAR, year_of_publication VARCHAR)\\n\\n### ANSWER\\n', 'labels': 0, 'answer': 'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"</s>'}\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b9fa58145d464b0aa8d58fac0bd08440"}}, "metadata": {}}, {"output_type": "stream", "text": "{'input_ids': [1, 835, 660, 4462, 1254, 2725, 13, 5618, 338, 278, 3402, 363, 278, 3303, 3900, 29797, 5468, 29871, 29906, 29941, 29892, 29871, 29906, 29900, 29900, 29906, 29973, 13, 13, 2277, 29937, 8707, 16975, 13, 27045, 10911, 1591, 29918, 978, 29918, 29947, 29946, 313, 4830, 21748, 29892, 5120, 21748, 29892, 2635, 21748, 29897, 13, 13, 2277, 29937, 319, 3059, 29956, 1001, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'text': '### QUESTION\\nWho was the Class AAAA champion in 2006-07?\\n\\n### CONTEXT\\nCREATE TABLE table_14603057_2 (class_aAAA VARCHAR, school_year VARCHAR)\\n\\n### ANSWER\\n', 'labels': 0, 'answer': 'SELECT class_aAAA FROM table_14603057_2 WHERE school_year = \"2006-07\"</s>'}\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e7eecd6ba1fc4ef9866d112d652ea9c5"}}, "metadata": {}}, {"output_type": "stream", "text": "{'input_ids': [1, 835, 660, 4462, 1254, 2725, 13, 5328, 1784, 14165, 911, 1612, 1338, 363, 278, 22900, 411, 263, 22125, 310, 29871, 29896, 29896, 322, 3109, 1135, 29871, 29896, 16466, 29973, 13, 13, 2277, 29937, 8707, 16975, 13, 27045, 10911, 1591, 29918, 978, 29918, 29929, 313, 1182, 265, 911, 2672, 4330, 17070, 29892, 7115, 21748, 29892, 13283, 21748, 29897, 13, 13, 2277, 29937, 319, 3059, 29956, 1001, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n", "name": "stdout"}]}, {"metadata": {"id": "b90e7aae-506d-481c-a733-e37db99fcc83"}, "cell_type": "code", "source": "# # Function to tokenize inputs and align labels\n# def tokenize_function(examples):\n#     tokenized_inputs = tokenizer(examples[\"answer\"], padding=\"max_length\", truncation=True, max_length=512)\n#     labels = tokenized_inputs[\"input_ids\"].copy()  # Copy input_ids to use as labels\n#     return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"attention_mask\": tokenized_inputs[\"attention_mask\"], \"labels\": labels}\n\n# # Prepare the dataset\n# data_dict = {\n#     'answer': val_dataset[\"answer\"],\n#     'labels': val_dataset[\"labels\"],\n# }\n# dataset_val = Dataset.from_dict(data_dict)\n# tokenized_datasets = dataset_val.map(tokenize_function, batched=True)\n# tokenized_datasets_answer = tokenized_datasets.remove_columns([\"answer\"])\n# tokenized_datasets_answer.set_format(\"torch\")\n\n# eval_answer_dataloader = DataLoader(tokenized_datasets_answer, batch_size=8)\n\n", "execution_count": 17, "outputs": []}, {"metadata": {"id": "63a9f8bf-8710-4479-b234-278b88110059"}, "cell_type": "code", "source": "import re\n\ndef extract_answer(output, eos_token='</s>'):\n    # Use regex to extract the answer part from the generated text\n    pattern = rf'### ANSWER\\n(.*?)(?:{re.escape(eos_token)}|###|$)'\n    match = re.search(pattern, output, re.DOTALL)\n    if match:\n        # print('match!')\n        return match.group(1).strip()\n    return output.strip()\n\n# Example output for testing\nexample_output = tool.create_prompt_with_answer_v2(**val_data[1])\nexp2 = tool.create_prompt_with_answer_v2(**val_data[2])\n\no = extract_answer(example_output, eos_token='</s>')\no2 = extract_answer(exp2, eos_token='</s>')\nprint(o)\nprint(o2)", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "SELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"\nSELECT MAX(opponents) FROM table_20745444_1 WHERE record = \"4-0\"\n", "name": "stdout"}]}, {"metadata": {"id": "54752a96-fa0b-40db-99be-686a4f80140a"}, "cell_type": "code", "source": "# print(results)\ndef prerocees_text(text1, text2, EOS_TOKEN = '</s>'):\n    text1 =str(text1)\n    text2 =str(text2)\n    text1 = extract_answer(text1, EOS_TOKEN)\n    text2 = extract_answer(text2, EOS_TOKEN)\n    # Remove the special tokens <s> and </s> if present\n    text1 = text1.replace('<s>', '').replace('</s>', '')\n    text2 = text2.replace('<s>', '').replace('</s>', '')\n    # Remove any special tokens from the text\n    # text1 = text1.replace(EOS_TOKEN, '')\n    # text2 = text2.replace(EOS_TOKEN, '')\n\n    # text1 = text1.lower()\n    # text2 = text2.lower()\n    # Assuring same length\n    if len(text1) > len(text2):\n        text2 = text2.ljust(len(text1))\n    else:\n        text1 = text1.ljust(len(text2))\n    return text1, text2\n\nprint(prerocees_text('hahaha'+val_data[0]['answer']+'hahaha', val_data[0]['answer']))\n# len\nprint(len('hahaha'+val_data[0]['answer']+'hahaha'), len(val_data[0]['answer']))\n#after preprocesing\nt1 , t2 = prerocees_text('hahaha'+val_data[0]['answer']+'hahaha', val_data[0]['answer'])\nprint(len(t1), len(t2))\nprint(t1,'\\n'+ t2)\nt2", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "('hahahaSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"hahaha', 'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            ')\n172 160\n172 172\nhahahaSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"hahaha \nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            \n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            '"}, "metadata": {}}]}, {"metadata": {"id": "df935598-e3f8-4383-84a4-2c68f19d294a"}, "cell_type": "code", "source": "out2 = tool.create_prompt_v3(val_data[0]['question'], val_data[0]['context'])\nout2_with_answer = tool.create_prompt_with_answer_v3(**val_data[0])\n# print(out2)\n# print(out2_with_answer)\n# out2 = tool.generate_text_v2(sql_model, tokenizer,out2)\n# out2", "execution_count": 20, "outputs": []}, {"metadata": {"id": "92b54d8d-c1cb-444d-88d2-3ecc03675cbd"}, "cell_type": "code", "source": "t3 , t4 = prerocees_text(out2[0], out2_with_answer)\nprint(len(t3), len(t4))\nprint(t3,'\\n'+ t4)", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "160 160\nB                                                                                                                                                                \nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"\n", "name": "stdout"}]}, {"metadata": {"id": "8c2d009b-9683-48a4-a1d7-f55154173565"}, "cell_type": "code", "source": "# Generate the text using the model\noutput = tool.generate_text(model, tokenizer, tool.create_prompt_v2(val_data[1]['question'], val_data[1]['context']))\n\n# Remove the special tokens <s> and </s>\n# cleaned_output = output[0].replace('<s>', '').replace('</s>', '').strip()\n\n# # Display the cleaned output\n# print(cleaned_output)\n# print(tool.create_prompt_with_answer_v2(**val_data[0]))", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n", "name": "stderr"}]}, {"metadata": {"id": "3b98cd0b-ff59-49ee-ac84-5a242b35783e"}, "cell_type": "code", "source": "# Display the cleaned output\nprint(tool.create_prompt_with_answer_v2(**val_data[1]))\nprint('\\n\\noutput= \\n'+output[0]+EOS_TOKEN)", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "### QUESTION\nWhat is the format for the United States dated July 23, 2002?\n\n### CONTEXT\nCREATE TABLE table_name_84 (format VARCHAR, region VARCHAR, date VARCHAR)\n\n### ANSWER\nSELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"</s>\n\n\noutput= \n### QUESTION\nWhat is the format for the United States dated July 23, 2002?\n\n### CONTEXT\nCREATE TABLE table_name_84 (format VARCHAR, region VARCHAR, date VARCHAR)\n\n### ANSWER\nSELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"\n\n### CONTEXT\nCREATE TABLE table_name_84 (format VARCHAR, region VARCHAR, date VARCHAR)\n\n### ANSWER\nSELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\" AND label = \"jive records\"\n\n### CONTEXT\nCREATE TABLE table_name_84 (format VARCHAR, label VARCHAR, region VARCHAR, date VARCHAR)\n\n##</s>\n", "name": "stdout"}]}, {"metadata": {"id": "5550710c-dbf6-48a2-83fd-6ad2c3ce8b8a"}, "cell_type": "code", "source": "import torch\n\nDEVICE = 'cuda'\n\nfrom tqdm import tqdm\n\ndef generate_text(model, tokenizer, input_text=\"def generate():\", max_length=512, **kwargs):\n    # Set the model to evaluation mode and move it to the desired device\n    model.eval()\n    model.to(DEVICE)\n\n    # Tokenize the input text and move the tokens to the device\n    input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n\n    # Add a loading bar\n    with tqdm(total=max_length, desc=\"Generating Text\", unit=\"tokens\") as pbar:\n        # Use mixed precision for faster inference\n        with torch.no_grad():\n            with torch.cuda.amp.autocast():\n                # Generate output tokens with incremental updates for the progress bar\n                output = model.generate(**input_tokens, max_length=max_length, **kwargs)\n                pbar.update(len(output[0]))  # Update the progress bar by the number of generated tokens\n\n    # Decode the output tokens into text\n    output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n\n    return output_text\n\n\n# Example usage\n# output_text = generate_text(model, tokenizer, input_text=\"def generate():\", max_length=200)\n\n# Optional: Apply JIT compilation to model (only with PyTorch 2.x)\n# model = torch.compile(model)", "execution_count": 24, "outputs": []}, {"metadata": {"id": "6d2a951d-4b8b-4ba4-9029-b7071f1a8b55"}, "cell_type": "code", "source": "import torch\nfrom tqdm import tqdm\n\nDEVICE = 'cuda'\n\ndef generate_text_batch_from_loader(model, tokenizer, dataloader, max_length=512, **kwargs):\n    \"\"\"\n    Function to generate text in batches from a DataLoader with pre-tokenized data.\n    \n    Args:\n        model: The model used for text generation.\n        tokenizer: Tokenizer used to decode the generated tokens.\n        dataloader (DataLoader): A DataLoader providing batches of tokenized strings.\n        max_length (int): The maximum length of generated sequences.\n        kwargs: Additional keyword arguments passed to model.generate.\n        \n    Returns:\n        List of generated text for each batch.\n    \"\"\"\n    \n    # Set the model to evaluation mode and move it to the desired device\n    model.eval()\n    model.to(DEVICE)\n    \n    all_outputs = []\n\n    # Add a loading bar to show progress\n    with tqdm(total=len(dataloader), desc=\"Generating Text\", unit=\"batch\") as pbar:\n        # Process the data in batches from the dataloader\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n            # Use mixed precision for faster inference\n            with torch.no_grad():\n                with torch.cuda.amp.autocast():\n                    # Generate output tokens for the current batch\n                    output_tokens = model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        max_length=max_length,\n                        **kwargs\n                    )\n\n            # Decode the output tokens into text\n            batch_output_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n            all_outputs.extend(batch_output_texts)\n\n            # Update the progress bar\n            pbar.update(1)\n\n    return all_outputs\n\n", "execution_count": 25, "outputs": []}, {"metadata": {"id": "fc8496ff-df80-4923-a0a8-144c07162140"}, "cell_type": "code", "source": "import evaluate\n\n# Calculating similarity\ndef calculate_similarity_accuracy_v2(predicted, ground_truth, device='cuda', dataloader=None, **kwargs):\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Load accuracy metric\n    metric = evaluate.load(\"accuracy\")\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat(\n            (tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat(\n            (tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for accuracy computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Compute accuracy for each token\n    metric.add_batch(predictions=flat_predictions, references=flat_labels)\n\n    # Calculate cosine similarity between the predicted and ground truth sequences\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    print(\"\\nCosine similarity:\\n\", cosine_similarity)\n\n    accuracy = metric.compute()\n    print(\"\\nAccuracy:\\n\", accuracy)\n    \n    return cosine_similarity, accuracy['accuracy']\n\n# Example usage\nsim = calculate_similarity_accuracy_v2(output[0], tool.create_prompt_with_answer_v2(**val_data[1]), device='cuda')\nsim2 = calculate_similarity_accuracy_v2(t3, t4, device='cuda')", "execution_count": 26, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "70c94344ef954eb895d0d2007863c509"}}, "metadata": {}}, {"output_type": "stream", "text": "\nCosine similarity:\n tensor([1.], device='cuda:0')\n\nAccuracy:\n {'accuracy': 1.0}\n\nCosine similarity:\n tensor([0.0333], device='cuda:0')\n\nAccuracy:\n {'accuracy': 0.01639344262295082}\n", "name": "stdout"}]}, {"metadata": {"id": "7a4f0db9-59e9-4e6c-926e-a58c5be068f3"}, "cell_type": "code", "source": "%pip install rouge_score", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.0.0)\nCollecting nltk (from rouge_score)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: click in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (1.1.1)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (2022.3.15)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.4)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=ee23ce5b45da14f98f3562ff6518c192360d0e36509946a21e89082348975503\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: nltk, rouge_score\nSuccessfully installed nltk-3.9.1 rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {"id": "8243e71d-4940-4153-aab6-1f58482bee43"}, "cell_type": "code", "source": "import torch\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom tqdm import tqdm\nimport evaluate\nimport torch.nn as nn\n\n\n# Calculating similarity and metrics\ndef calculate_metrics(predicted, ground_truth, device='cuda', use_print=False, **kwargs):\n    predicted = extract_answer(predicted)\n    ground_truth = extract_answer(ground_truth)\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat(\n            (tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat(\n            (tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for metric computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Load metrics\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric = evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n    rouge_metric = evaluate.load(\"rouge\")\n    bleu_metric = evaluate.load(\"bleu\")\n\n    # Add batches to metrics\n    precision_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    recall_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    f1_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    accuracy_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    \n    # For ROUGE and BLEU, you usually need sequences, so they may need to be adapted depending on how they process data.\n    # Since we're dealing with token-level metrics, you can use sequences directly.\n    rouge_metric.add_batch(predictions=[predicted], references=[ground_truth])\n    bleu_metric.add_batch(predictions=[predicted], references=[[ground_truth]])\n\n    # Compute metrics\n    precision = precision_metric.compute(average='macro')\n    recall = recall_metric.compute(average='macro')\n    f1 = f1_metric.compute(average='macro')\n    accuracy = accuracy_metric.compute()\n    rouge = rouge_metric.compute()\n    bleu = bleu_metric.compute()\n\n    # Calculate cosine similarity between the predicted and ground truth sequences\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    \n    # Calculate perplexity\n    log_probs = nn.functional.log_softmax(tokens_predicted.float(), dim=-1)\n    perplexity = torch.exp(-log_probs.mean()).item()\n\n    # Exact match\n    exact_match = (flat_predictions == flat_labels).float().mean().item()\n\n    if use_print:\n    # Print results\n        print(\"\\nCosine similarity:\\n\", cosine_similarity)\n        print(\"\\nPrecision:\\n\", precision)\n        print(\"\\nRecall:\\n\", recall)\n        print(\"\\nF1 Score:\\n\", f1)\n        print(\"\\nAccuracy:\\n\", accuracy)\n        print(\"\\nROUGE Score:\\n\", rouge)\n        print(\"\\nBLEU Score:\\n\", bleu)\n        print(\"\\nPerplexity:\\n\", perplexity)\n        print(\"\\nExact Match:\\n\", exact_match)\n\n    return {\n        \"cosine_similarity\": cosine_similarity,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n        \"accuracy\": accuracy,\n        \"rouge_score\": rouge,\n        \"bleu_score\": bleu,\n        \"perplexity\": perplexity,\n        \"exact_match\": exact_match,\n    }\n\n\n# Example usage\nmetrics = calculate_metrics(output[0], tool.create_prompt_with_answer_v2(**val_data[1]), device='cuda', use_print=False)\n", "execution_count": 28, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "57a38e89859943689bbb8f64dd68facd"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "377ab3060d04419a9402f336a1d63c5f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "27201b52807c49809b1bcb66ced6fab7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e65babc3c7e1484d81b0d02d1bad6046"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "08727f40a986403dbd4d9080398cef0f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4fc722556b1f45c194dd518965c61748"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ec959be3a4fc43be88638628d45c379f"}}, "metadata": {}}]}, {"metadata": {"id": "c658e646-29a4-475d-8b90-8d03eb131aff"}, "cell_type": "code", "source": "metrics", "execution_count": 29, "outputs": [{"output_type": "execute_result", "execution_count": 29, "data": {"text/plain": "{'cosine_similarity': tensor([1.], device='cuda:0'),\n 'precision': {'precision': 1.0},\n 'recall': {'recall': 1.0},\n 'f1_score': {'f1': 1.0},\n 'accuracy': {'accuracy': 1.0},\n 'rouge_score': {'rouge1': 1.0,\n  'rouge2': 1.0,\n  'rougeL': 1.0,\n  'rougeLsum': 1.0},\n 'bleu_score': {'bleu': 1.0,\n  'precisions': [1.0, 1.0, 1.0, 1.0],\n  'brevity_penalty': 1.0,\n  'length_ratio': 1.0,\n  'translation_length': 24,\n  'reference_length': 24},\n 'perplexity': inf,\n 'exact_match': 1.0}"}, "metadata": {}}]}, {"metadata": {"id": "07337deb-ce17-47d4-8125-7f92e5cc5fa1", "scrolled": true}, "cell_type": "code", "source": "import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom tqdm import tqdm\n\n# Set padding to be on the left side for decoder-only architecture\ntokenizer.padding_side = 'left'\n\n# Assign pad token if not already set\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Ensure CUDA is available\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef generate_text_batch_from_loader(model, tokenizer, dataloader, max_length=512, use_amp=True, **kwargs):\n    \"\"\"\n    Function to generate text in batches from a DataLoader with pre-tokenized data.\n    \n    Args:\n        model: The model used for text generation.\n        tokenizer: Tokenizer used to decode the generated tokens.\n        dataloader (DataLoader): A DataLoader providing batches of tokenized strings.\n        max_length (int): The maximum length of generated sequences.\n        use_amp (bool): Whether to use automatic mixed precision for faster inference.\n        kwargs: Additional keyword arguments passed to model.generate.\n        \n    Returns:\n        List of generated text for each batch.\n    \"\"\"\n    \n    # Set the model to evaluation mode and move it to the desired device\n    model.eval()\n#     model.to(DEVICE)\n    \n    all_outputs = []\n\n    # Add a loading bar to show progress\n    with tqdm(total=len(dataloader), desc=\"Generating Text\", unit=\"batch\") as pbar:\n        # Process the data in batches from the dataloader\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n            # Use mixed precision for faster inference if available\n            with torch.no_grad():\n                if use_amp and torch.cuda.is_available():\n                    with torch.cuda.amp.autocast():\n                        # Generate output tokens for the current batch\n                        output_tokens = model.generate(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n#                             max_length=max_length,\n                            max_new_tokens =100,\n                            do_sample = False,\n                            **kwargs\n                        )\n                else:\n                    # Generate output tokens without mixed precision\n                    output_tokens = model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n#                         max_length=max_length,\n                        max_new_tokens =100,\n                        do_sample = False,\n                        **kwargs\n                    )\n\n            # Decode the output tokens into text\n            batch_output_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n            all_outputs.extend(batch_output_texts)\n\n            # Update the progress bar\n            pbar.update(1)\n\n    return all_outputs\n\n# Use eval_answer_dataloader (DataLoader) for batch generation\noutputs = generate_text_batch_from_loader(\n    model=sql_model,\n    tokenizer=tokenizer,\n    dataloader=eval_text_dataloader,  # Pass the DataLoader\n    max_length=512\n)\n\n# Print the first generated output\nprint(outputs[1])", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "Generating Text:   0%|          | 0/63 [00:00<?, ?batch/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:   2%|\u258f         | 1/63 [00:05<05:28,  5.30s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:   3%|\u258e         | 2/63 [00:10<05:20,  5.26s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:   5%|\u258d         | 3/63 [00:15<05:14,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:   6%|\u258b         | 4/63 [00:20<05:08,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:   8%|\u258a         | 5/63 [00:26<05:03,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  10%|\u2589         | 6/63 [00:31<04:57,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  11%|\u2588         | 7/63 [00:36<04:52,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  13%|\u2588\u258e        | 8/63 [00:41<04:46,  5.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  14%|\u2588\u258d        | 9/63 [00:47<04:42,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  16%|\u2588\u258c        | 10/63 [00:52<04:37,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  17%|\u2588\u258b        | 11/63 [00:57<04:32,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  19%|\u2588\u2589        | 12/63 [01:02<04:27,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  21%|\u2588\u2588        | 13/63 [01:08<04:22,  5.26s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  22%|\u2588\u2588\u258f       | 14/63 [01:13<04:16,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  24%|\u2588\u2588\u258d       | 15/63 [01:18<04:11,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  25%|\u2588\u2588\u258c       | 16/63 [01:23<04:05,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  27%|\u2588\u2588\u258b       | 17/63 [01:28<04:00,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  29%|\u2588\u2588\u258a       | 18/63 [01:34<03:54,  5.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  30%|\u2588\u2588\u2588       | 19/63 [01:39<03:49,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  32%|\u2588\u2588\u2588\u258f      | 20/63 [01:44<03:43,  5.20s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  33%|\u2588\u2588\u2588\u258e      | 21/63 [01:49<03:38,  5.20s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  35%|\u2588\u2588\u2588\u258d      | 22/63 [01:54<03:32,  5.19s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  37%|\u2588\u2588\u2588\u258b      | 23/63 [02:00<03:28,  5.20s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  38%|\u2588\u2588\u2588\u258a      | 24/63 [02:05<03:23,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  40%|\u2588\u2588\u2588\u2589      | 25/63 [02:10<03:20,  5.27s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  41%|\u2588\u2588\u2588\u2588\u258f     | 26/63 [02:16<03:15,  5.28s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n", "name": "stderr"}, {"output_type": "stream", "text": "Generating Text:  43%|\u2588\u2588\u2588\u2588\u258e     | 27/63 [02:21<03:09,  5.27s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  44%|\u2588\u2588\u2588\u2588\u258d     | 28/63 [02:26<03:04,  5.27s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  46%|\u2588\u2588\u2588\u2588\u258c     | 29/63 [02:31<02:58,  5.26s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  48%|\u2588\u2588\u2588\u2588\u258a     | 30/63 [02:37<02:53,  5.26s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  49%|\u2588\u2588\u2588\u2588\u2589     | 31/63 [02:42<02:48,  5.26s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  51%|\u2588\u2588\u2588\u2588\u2588     | 32/63 [02:47<02:42,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 33/63 [02:52<02:37,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 34/63 [02:58<02:32,  5.26s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 35/63 [03:03<02:27,  5.25s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 36/63 [03:08<02:22,  5.27s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 37/63 [03:13<02:16,  5.26s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 38/63 [03:19<02:11,  5.25s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 39/63 [03:24<02:05,  5.25s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 40/63 [03:29<02:00,  5.25s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 41/63 [03:34<01:55,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 42/63 [03:39<01:49,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 43/63 [03:45<01:44,  5.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 44/63 [03:50<01:39,  5.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 45/63 [03:55<01:33,  5.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 46/63 [04:00<01:28,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 47/63 [04:06<01:24,  5.25s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 48/63 [04:11<01:18,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 49/63 [04:16<01:13,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 50/63 [04:21<01:07,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 51/63 [04:26<01:02,  5.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 52/63 [04:32<00:57,  5.20s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 53/63 [04:37<00:52,  5.20s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n", "name": "stderr"}, {"output_type": "stream", "text": "Generating Text:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 54/63 [04:42<00:47,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 55/63 [04:47<00:41,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 56/63 [04:53<00:36,  5.21s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 57/63 [04:58<00:31,  5.23s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 58/63 [05:03<00:26,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 59/63 [05:08<00:20,  5.24s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 60/63 [05:13<00:15,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 61/63 [05:19<00:10,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 62/63 [05:24<00:05,  5.22s/batch]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nGenerating Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63/63 [05:29<00:00,  5.23s/batch]", "name": "stderr"}, {"output_type": "stream", "text": "### QUESTION\nHow many Bronze medals for the Nation with a Rank of 11 and less than 1 Silver?\n\n### CONTEXT\nCREATE TABLE table_name_9 (bronze INTEGER, rank VARCHAR, silver VARCHAR)\n\n### ANSWER\n  SELECT SUM(bronze) FROM table_name_9 WHERE rank = \"11\" AND silver < 1 AND nation = \"united states\" AND CONTEXT = \"bronze\" AND rank = \"11\" AND silver < 1 AND nation = \"united states\" AND CONTEXT = \"bronze\" AND rank = \"11\" AND silver < 1 AND nation = \"united states\" AND CONTEXT = \"bronze\n", "name": "stdout"}, {"output_type": "stream", "text": "\n", "name": "stderr"}]}, {"metadata": {"id": "29a83d63-43c3-4a8f-896e-6442f6fcebcb"}, "cell_type": "code", "source": "!nvidia-smi", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "Thu Sep 12 13:09:38 2024       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  Tesla V100-PCIE-16GB           Off | 00000000:AF:00.0 Off |                    0 |\r\n| N/A   51C    P0              50W / 250W |  14852MiB / 16384MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n+---------------------------------------------------------------------------------------+\r\n", "name": "stdout"}]}, {"metadata": {"scrolled": true, "id": "ee8fe258-0d32-43f0-9dd5-638c18f00d06"}, "cell_type": "code", "source": "# accuracy for the first 10 validation data\n# outputs = [generate_text(sql_model, tokenizer, tool.create_prompt_v3(d['question'], d['context']))[0]+EOS_TOKEN for d in val_data[:20]]\n\n# ground_truths = [tool.create_prompt_with_answer_v3(**d) for d in val_data]\nground_truths = [d['answer'] for d in test_data]\n# print(ground_truths[0])\n# Calculate the accuracy for each pair of output and ground truth\n# accuracies = [calculate_similarity_v2(output, ground_truth) for output, ground_truth in zip(outputs, ground_truths)]", "execution_count": 31, "outputs": []}, {"metadata": {"id": "c6b0a8a6-67c3-495e-bd44-391064f8596d"}, "cell_type": "code", "source": "# val_data\nprint(outputs[0])\nprint(ground_truths[0])", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "### QUESTION\nWho was the Class AAAA champion in 2006-07?\n\n### CONTEXT\nCREATE TABLE table_14603057_2 (class_aAAA VARCHAR, school_year VARCHAR)\n\n### ANSWER\n  SELECT class_aAAA FROM table_14603057_2 WHERE school_year = \"2006-07\" AND class_aAAA = \"Champion\" AND class_aAAA = \"Champion\" AND class_aAAA = \"Champion\" AND class_aAAA = \"Champion\" AND class_aAAA = \"Champion\" AND class_aAAA = \"Champion\"\nSELECT class_aAAA FROM table_14603057_2 WHERE school_year = \"2006-07\"\n", "name": "stdout"}]}, {"metadata": {"id": "f50cbb99-dc6a-4b80-8fb7-8b05b340cb9e"}, "cell_type": "code", "source": "outputs2 = outputs\n", "execution_count": 33, "outputs": []}, {"metadata": {"id": "ceab8214-1973-4317-b7f3-0824d1efe02d"}, "cell_type": "code", "source": "print(len(ground_truths))\n# ground_truths = ground_truths[:200]\n# outputs = outputs2[:200]", "execution_count": 34, "outputs": [{"output_type": "stream", "text": "500\n", "name": "stdout"}]}, {"metadata": {"id": "07568a7e-20ea-4fd6-aca2-a86b7135d2be"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"id": "b17be4c2-bbf5-453f-8cdd-5d3999480c25"}, "cell_type": "code", "source": "import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport evaluate\nimport torch.nn as nn\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef calculate_metrics(predicted, ground_truth, device='cuda', use_print=False, **kwargs):\n    predicted = extract_answer(predicted)\n    ground_truth = extract_answer(ground_truth)\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat((tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat((tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for metric computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Load metrics\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric = evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n    rouge_metric = evaluate.load(\"rouge\")\n    bleu_metric = evaluate.load(\"bleu\")\n\n    # Add batches to metrics\n    precision_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    recall_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    f1_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    accuracy_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n\n    # For ROUGE and BLEU, you usually need sequences\n    rouge_metric.add_batch(predictions=[predicted], references=[ground_truth])\n\n   # Compute BLEU with try-except to handle ZeroDivisionError\n    try:\n        if predicted and ground_truth and len(predicted) > 0 and len(ground_truth) > 0:\n            bleu = bleu_metric.compute(predictions=[predicted], references=[[ground_truth]], smooth=True)[\"bleu\"]\n        else:\n            bleu = 0.0  # Assign default score if either sequence is empty\n    except ZeroDivisionError:\n        bleu = 0.0  # In case of division by zero, assign a default value\n\n\n    # Compute other metrics\n    precision = precision_metric.compute(average='macro', zero_division=0)\n    recall = recall_metric.compute(average='macro', zero_division=0)\n    f1 = f1_metric.compute(average='macro')\n    accuracy = accuracy_metric.compute()\n    rouge = rouge_metric.compute()\n\n    # Calculate cosine similarity and normalize\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    cosine_similarity = torch.clamp(cosine_similarity.mean(), -1.0, 1.0)\n\n    # Exact match\n    exact_match = (flat_predictions == flat_labels).float().mean().item()\n\n    return {\n        \"cosine_similarity\": cosine_similarity.item(),\n        \"precision\": precision[\"precision\"],\n        \"recall\": recall[\"recall\"],\n        \"f1_score\": f1[\"f1\"],\n        \"accuracy\": accuracy[\"accuracy\"],\n        \"rouge_score\": rouge,\n        \"bleu_score\": bleu,\n        \"exact_match\": exact_match,\n    }\n\n\n# Adjusting the accumulation logic\nmetrics_accumulator = {\n    \"cosine_similarity\": 0.0,\n    \"precision\": 0.0,\n    \"recall\": 0.0,\n    \"f1_score\": 0.0,\n    \"accuracy\": 0.0,\n    \"rouge_score\": {\n        \"rouge1\": 0.0,\n        \"rouge2\": 0.0,\n        \"rougeL\": 0.0,\n        \"rougeLsum\": 0.0\n    },\n    \"bleu_score\": 0.0,\n    \"exact_match\": 0.0,\n}\n\n# Define a helper function for parallel execution\ndef process_pair(output, ground_truth):\n    return calculate_metrics(output, ground_truth, device='cuda')\n\n# Use ThreadPoolExecutor to parallelize the process\nnum_samples = len(outputs)\nwith ThreadPoolExecutor() as executor:\n    results = list(tqdm(executor.map(process_pair, outputs, ground_truths), total=num_samples))\n\n# Aggregate the metrics from each pair\nfor metrics in results:\n    metrics_accumulator[\"cosine_similarity\"] += metrics[\"cosine_similarity\"]\n    metrics_accumulator[\"precision\"] += metrics[\"precision\"]\n    metrics_accumulator[\"recall\"] += metrics[\"recall\"]\n    metrics_accumulator[\"f1_score\"] += metrics[\"f1_score\"]\n    metrics_accumulator[\"accuracy\"] += metrics[\"accuracy\"]\n    \n    for key in metrics[\"rouge_score\"]:\n        metrics_accumulator[\"rouge_score\"][key] += metrics[\"rouge_score\"][key]\n    \n    metrics_accumulator[\"bleu_score\"] += metrics[\"bleu_score\"]\n    metrics_accumulator[\"exact_match\"] += metrics[\"exact_match\"]\n\n# Calculate the average for each metric\naverage_metrics = {\n    \"cosine_similarity\": metrics_accumulator[\"cosine_similarity\"] / num_samples,\n    \"precision\": metrics_accumulator[\"precision\"] / num_samples,\n    \"recall\": metrics_accumulator[\"recall\"] / num_samples,\n    \"f1_score\": metrics_accumulator[\"f1_score\"] / num_samples,\n    \"accuracy\": metrics_accumulator[\"accuracy\"] / num_samples,\n    \"rouge_score\": {\n        key: metrics_accumulator[\"rouge_score\"][key] / num_samples for key in metrics_accumulator[\"rouge_score\"]\n    },\n    \"bleu_score\": metrics_accumulator[\"bleu_score\"] / num_samples,\n    \"exact_match\": metrics_accumulator[\"exact_match\"] / num_samples,\n}\n\n# Print the average metrics\nprint(\"Average Metrics for the Validation Samples:\")\nfor key, value in average_metrics.items():\n    if isinstance(value, dict):\n        print(f\"{key}:\")\n        for sub_key, sub_value in value.items():\n            print(f\"  {sub_key}: {sub_value:.4f}\")\n    else:\n        print(f\"{key}: {value:.4f}\")\n", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [08:35<00:00,  1.03s/it]", "name": "stderr"}, {"output_type": "stream", "text": "Average Metrics for the Validation Samples:\ncosine_similarity: 0.5519\nprecision: 0.4399\nrecall: 0.5522\nf1_score: 0.4595\naccuracy: 0.2953\nrouge_score:\n  rouge1: 0.5161\n  rouge2: 0.4891\n  rougeL: 0.5139\n  rougeLsum: 0.5141\nbleu_score: 0.3761\nexact_match: 0.2953\n", "name": "stdout"}, {"output_type": "stream", "text": "\n", "name": "stderr"}]}, {"metadata": {"id": "ab1b0f84-fae1-4f56-9617-ec876aa869a4"}, "cell_type": "code", "source": "import os\nimport pandas as pd\nfrom huggingface_hub import hf_hub_download, upload_file\nfrom datasets import Dataset\nfrom huggingface_hub import HfApi, HfFolder, Repository\n\n# Function to upload the DataFrame to Hugging Face\ndef upload_dataframe_to_huggingface(df, repo_id='koukoudzz/gpt2_sql-v0.0', path_in_repo='', local_csv_path='training_results.csv'):\n    try:\n        # Initialize the Hugging Face API\n        api = HfApi()\n\n        # Set the full path in the repository\n        path_in_repo = os.path.join(path_in_repo, local_csv_path)\n        \n        # Download the existing CSV from Hugging Face if it exists\n        try:\n            downloaded_file = hf_hub_download(repo_id=repo_id, filename=path_in_repo, repo_type=\"model\")\n            existing_df = pd.read_csv(downloaded_file)\n            print(\"Existing file found and loaded from Hugging Face Hub.\")\n        except (FileNotFoundError, pd.errors.EmptyDataError, Exception) as e:\n            existing_df = pd.DataFrame()  # If no existing file, create a fresh DataFrame\n            print(f\"No existing file found, creating a new one. Error: {e}\")\n\n        # Check if the model already exists in the existing dataframe\n        if not existing_df.empty and df[\"Model Name\"].iloc[0] in existing_df[\"Model Name\"].values:\n            model_name = df[\"Model Name\"].iloc[0]\n            idx = existing_df.index[existing_df[\"Model Name\"] == model_name].tolist()[0]\n\n            # Update the corresponding row with the new metrics\n            for col in df.columns:\n                if col in existing_df.columns:\n                    existing_df.at[idx, col] = df[col].iloc[0]\n                else:\n                    # If the column does not exist, add the new metric as a column\n                    existing_df[col] = None\n                    existing_df.at[idx, col] = df[col].iloc[0]\n        else:\n            # Append the new row if the model does not exist\n            existing_df = pd.concat([existing_df, df], ignore_index=True)\n\n        # Save the DataFrame to a CSV file locally\n        existing_df.to_csv(local_csv_path, index=False)\n    \n        # Upload the file to the Hugging Face Hub\n        upload_file(\n            path_or_fileobj=local_csv_path,\n            path_in_repo=path_in_repo,\n            repo_id=repo_id,\n            repo_type=\"model\"  # Ensure you're using the correct repo type\n        )\n        \n        print(f\"Successfully uploaded {local_csv_path} to Hugging Face Hub under the model repository {repo_id}!\")\n        return existing_df\n    except Exception as e:\n        print(f\"An error occurred while uploading the dataset to Hugging Face: {e}\")\n\n# Example of updating the dataframe with new metrics\nnew_metrics = {\n    \"Model Name\": [model_name],  # The model name you want to update\n    \"Cosine Similarity\": [average_metrics[\"cosine_similarity\"]],\n    \"Precision\": [average_metrics[\"precision\"]],\n    \"Recall\": [average_metrics[\"recall\"]],\n    \"F1 Score\": [average_metrics[\"f1_score\"]],\n    \"Accuracy\": [average_metrics[\"accuracy\"]],\n    \"ROUGE Score\": [average_metrics[\"rouge_score\"]],\n    \"BLEU Score\": [average_metrics[\"bleu_score\"]],\n    \"Exact Match\": [average_metrics[\"exact_match\"]]\n}\n\n# Convert the new metrics to a DataFrame\ndf_new_metrics = pd.DataFrame(new_metrics)\n\n# Call the upload function\ndf_final = upload_dataframe_to_huggingface(df_new_metrics,\n                                           repo_id='koukoudzz/granite-7b-base_sql-v0.0',\n                                           path_in_repo='evaluation')\n", "execution_count": 36, "outputs": [{"output_type": "display_data", "data": {"text/plain": "evaluation/training_results.csv:   0%|          | 0.00/6.62k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "438036c63c4148de80f4b1075e177ed5"}}, "metadata": {}}, {"output_type": "stream", "text": "Existing file found and loaded from Hugging Face Hub.\nSuccessfully uploaded training_results.csv to Hugging Face Hub under the model repository koukoudzz/granite-7b-base_sql-v0.0!\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "model_name", "execution_count": 37, "outputs": [{"output_type": "execute_result", "execution_count": 37, "data": {"text/plain": "'granite-7b-base_sql-v1.0'"}, "metadata": {}}]}, {"metadata": {"id": "5f52e8e4-4a42-4b7e-b9af-5f1d50c2d23d"}, "cell_type": "code", "source": "df_final", "execution_count": 38, "outputs": [{"output_type": "execute_result", "execution_count": 38, "data": {"text/plain": "                             Model Name  LoRA r  LoRA Alpha  Batch Size  \\\n0              granite-7b-base_sql-v0.0    16.0        32.0         4.0   \n1              granite-7b-base_sql-v0.1    16.0        32.0         4.0   \n2              granite-7b-base_sql-v0.2    16.0        32.0         4.0   \n3              granite-7b-base_sql-v0.3    32.0        64.0         4.0   \n4              granite-7b-base_sql-v0.4    64.0       128.0         4.0   \n5              granite-7b-base_sql-v0.5   128.0       256.0         4.0   \n6              granite-7b-base_sql-v0.6   256.0       512.0         4.0   \n7              granite-7b-base_sql-v0.7   256.0       512.0         4.0   \n8              granite-7b-base_sql-v0.8   512.0      1024.0         4.0   \n9              granite-7b-base_Original     NaN         NaN         NaN   \n10  granite-7b-base_Original_withPrompt     NaN         NaN         NaN   \n11  granite-7b-base_sql-v0.1_withPrompt     NaN         NaN         NaN   \n12  granite-7b-base_sql-v0.2_withPrompt     NaN         NaN         NaN   \n13             granite-7b-base_sql-v0.9     8.0        16.0         4.0   \n14  granite-7b-base_sql-v0.9_withPrompt     NaN         NaN         NaN   \n15             granite-7b-base_sql-v1.0     8.0        16.0         4.0   \n\n                                  LoRA Target Modules  LoRA Dropout  \\\n0   q_proj, down_proj, k_proj, gate_proj, v_proj, ...          0.00   \n1   v_proj, gate_proj, q_proj, up_proj, k_proj, o_...          0.05   \n2                              k_proj, v_proj, q_proj          0.05   \n3   up_proj, q_proj, o_proj, down_proj, v_proj, k_...          0.05   \n4   o_proj, up_proj, down_proj, q_proj, v_proj, k_...          0.05   \n5   gate_proj, k_proj, o_proj, v_proj, up_proj, q_...          0.05   \n6   gate_proj, o_proj, v_proj, k_proj, q_proj, dow...          0.05   \n7   o_proj, v_proj, up_proj, k_proj, gate_proj, q_...          0.00   \n8   o_proj, k_proj, down_proj, up_proj, q_proj, v_...          0.05   \n9                                                 NaN           NaN   \n10                                                NaN           NaN   \n11                                                NaN           NaN   \n12                                                NaN           NaN   \n13  q_proj, k_proj, o_proj, down_proj, up_proj, ga...          0.00   \n14                                                NaN           NaN   \n15  q_proj, gate_proj, up_proj, o_proj, down_proj,...          0.05   \n\n                Quantization  Training Time (s)   Memory (MB)  \\\n0   Activated: torch.float16       14136.119226  14823.136719   \n1   Activated: torch.float16        3715.422049  14914.996094   \n2   Activated: torch.float16        3163.871577  14481.886719   \n3   Activated: torch.float16        6436.688373  15071.574707   \n4   Activated: torch.float16        4129.804926   8596.098633   \n5   Activated: torch.float16        4258.871780   9252.045898   \n6   Activated: torch.float16        6320.419308  10464.280273   \n7   Activated: torch.float16        8514.288527  10417.149414   \n8   Activated: torch.float16        8101.901001  13844.272949   \n9                        NaN                NaN           NaN   \n10                       NaN                NaN           NaN   \n11                       NaN                NaN           NaN   \n12                       NaN                NaN           NaN   \n13  Activated: torch.float16       12076.062719   8027.226562   \n14                       NaN                NaN           NaN   \n15  Activated: torch.float16        4042.287440   8120.897461   \n\n    Final Eval Loss  Perplexity   Optimizer  Cosine Similarity  Precision  \\\n0          0.621448    1.861621  PagedAdamW           0.538596   0.236580   \n1          0.620473    1.859807  PagedAdamW           0.761791   0.597814   \n2          0.647661    1.911065  PagedAdamW           0.852444   0.726314   \n3          0.630098    1.877795  PagedAdamW           0.674955   0.516018   \n4          0.637664    1.892057  PagedAdamW           0.573984   0.501538   \n5          0.667237    1.948846  PagedAdamW           0.357201   0.195264   \n6          0.733728    2.082831  PagedAdamW           0.495321   0.428704   \n7          0.751166    2.119470  PagedAdamW           0.412862   0.128201   \n8          0.885569    2.424363  PagedAdamW           0.290779   0.086690   \n9               NaN         NaN         NaN           0.324330   0.074863   \n10              NaN         NaN         NaN           0.540922   0.200351   \n11              NaN         NaN         NaN           0.918859   0.831975   \n12              NaN         NaN         NaN           0.208137   0.040781   \n13         0.619540    1.858073  PagedAdamW           0.551238   0.383620   \n14              NaN         NaN         NaN           0.939515   0.866402   \n15         0.621172    1.861107  PagedAdamW           0.551889   0.439885   \n\n      Recall  F1 Score  Accuracy  \\\n0   0.237912  0.236870  0.250074   \n1   0.609623  0.601420  0.608422   \n2   0.746131  0.732175  0.740270   \n3   0.616453  0.537493  0.465920   \n4   0.609738  0.523201  0.393787   \n5   0.216856  0.201130  0.188081   \n6   0.635526  0.463588  0.222767   \n7   0.173270  0.137502  0.101896   \n8   0.108706  0.090811  0.066324   \n9   0.076483  0.075050  0.080831   \n10  0.204695  0.201463  0.225366   \n11  0.841008  0.834713  0.844340   \n12  0.040738  0.040752  0.039059   \n13  0.387880  0.384926  0.390526   \n14  0.870341  0.867626  0.878196   \n15  0.552174  0.459533  0.295267   \n\n                                          ROUGE Score  BLEU Score  Exact Match  \n0   {'rouge1': 0.5371810602661362, 'rouge2': 0.425...    0.397609     0.250074  \n1   {'rouge1': 0.8116854643666376, 'rouge2': 0.772...    0.735415     0.608422  \n2   {'rouge1': 0.8507686393867147, 'rouge2': 0.830...    0.785439     0.740270  \n3   {'rouge1': 0.6605370069224609, 'rouge2': 0.634...    0.517558     0.465920  \n4   {'rouge1': 0.591332944803146, 'rouge2': 0.5698...    0.474130     0.393787  \n5   {'rouge1': 0.20684435626557007, 'rouge2': 0.20...    0.171952     0.188081  \n6   {'rouge1': 0.4596046325862796, 'rouge2': 0.424...    0.282041     0.222767  \n7   {'rouge1': 0.3485597640902298, 'rouge2': 0.242...    0.194294     0.101896  \n8   {'rouge1': 0.13304977962087253, 'rouge2': 0.09...    0.069795     0.066324  \n9   {'rouge1': 0.2449954357289134, 'rouge2': 0.167...    0.117901     0.080831  \n10  {'rouge1': 0.6722965654131353, 'rouge2': 0.565...    0.428616     0.225366  \n11  {'rouge1': 0.9544219633306067, 'rouge2': 0.938...    0.902170     0.844340  \n12  {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...    0.000000     0.039059  \n13  {'rouge1': 0.43335724163913025, 'rouge2': 0.42...    0.409671     0.390526  \n14  {'rouge1': 0.9755926628139077, 'rouge2': 0.958...    0.933917     0.878196  \n15  {'rouge1': 0.5161066358736365, 'rouge2': 0.489...    0.376061     0.295267  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model Name</th>\n      <th>LoRA r</th>\n      <th>LoRA Alpha</th>\n      <th>Batch Size</th>\n      <th>LoRA Target Modules</th>\n      <th>LoRA Dropout</th>\n      <th>Quantization</th>\n      <th>Training Time (s)</th>\n      <th>Memory (MB)</th>\n      <th>Final Eval Loss</th>\n      <th>Perplexity</th>\n      <th>Optimizer</th>\n      <th>Cosine Similarity</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n      <th>Accuracy</th>\n      <th>ROUGE Score</th>\n      <th>BLEU Score</th>\n      <th>Exact Match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>granite-7b-base_sql-v0.0</td>\n      <td>16.0</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>q_proj, down_proj, k_proj, gate_proj, v_proj, ...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>14136.119226</td>\n      <td>14823.136719</td>\n      <td>0.621448</td>\n      <td>1.861621</td>\n      <td>PagedAdamW</td>\n      <td>0.538596</td>\n      <td>0.236580</td>\n      <td>0.237912</td>\n      <td>0.236870</td>\n      <td>0.250074</td>\n      <td>{'rouge1': 0.5371810602661362, 'rouge2': 0.425...</td>\n      <td>0.397609</td>\n      <td>0.250074</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>granite-7b-base_sql-v0.1</td>\n      <td>16.0</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>v_proj, gate_proj, q_proj, up_proj, k_proj, o_...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>3715.422049</td>\n      <td>14914.996094</td>\n      <td>0.620473</td>\n      <td>1.859807</td>\n      <td>PagedAdamW</td>\n      <td>0.761791</td>\n      <td>0.597814</td>\n      <td>0.609623</td>\n      <td>0.601420</td>\n      <td>0.608422</td>\n      <td>{'rouge1': 0.8116854643666376, 'rouge2': 0.772...</td>\n      <td>0.735415</td>\n      <td>0.608422</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>granite-7b-base_sql-v0.2</td>\n      <td>16.0</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>k_proj, v_proj, q_proj</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>3163.871577</td>\n      <td>14481.886719</td>\n      <td>0.647661</td>\n      <td>1.911065</td>\n      <td>PagedAdamW</td>\n      <td>0.852444</td>\n      <td>0.726314</td>\n      <td>0.746131</td>\n      <td>0.732175</td>\n      <td>0.740270</td>\n      <td>{'rouge1': 0.8507686393867147, 'rouge2': 0.830...</td>\n      <td>0.785439</td>\n      <td>0.740270</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>granite-7b-base_sql-v0.3</td>\n      <td>32.0</td>\n      <td>64.0</td>\n      <td>4.0</td>\n      <td>up_proj, q_proj, o_proj, down_proj, v_proj, k_...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>6436.688373</td>\n      <td>15071.574707</td>\n      <td>0.630098</td>\n      <td>1.877795</td>\n      <td>PagedAdamW</td>\n      <td>0.674955</td>\n      <td>0.516018</td>\n      <td>0.616453</td>\n      <td>0.537493</td>\n      <td>0.465920</td>\n      <td>{'rouge1': 0.6605370069224609, 'rouge2': 0.634...</td>\n      <td>0.517558</td>\n      <td>0.465920</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>granite-7b-base_sql-v0.4</td>\n      <td>64.0</td>\n      <td>128.0</td>\n      <td>4.0</td>\n      <td>o_proj, up_proj, down_proj, q_proj, v_proj, k_...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>4129.804926</td>\n      <td>8596.098633</td>\n      <td>0.637664</td>\n      <td>1.892057</td>\n      <td>PagedAdamW</td>\n      <td>0.573984</td>\n      <td>0.501538</td>\n      <td>0.609738</td>\n      <td>0.523201</td>\n      <td>0.393787</td>\n      <td>{'rouge1': 0.591332944803146, 'rouge2': 0.5698...</td>\n      <td>0.474130</td>\n      <td>0.393787</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>granite-7b-base_sql-v0.5</td>\n      <td>128.0</td>\n      <td>256.0</td>\n      <td>4.0</td>\n      <td>gate_proj, k_proj, o_proj, v_proj, up_proj, q_...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>4258.871780</td>\n      <td>9252.045898</td>\n      <td>0.667237</td>\n      <td>1.948846</td>\n      <td>PagedAdamW</td>\n      <td>0.357201</td>\n      <td>0.195264</td>\n      <td>0.216856</td>\n      <td>0.201130</td>\n      <td>0.188081</td>\n      <td>{'rouge1': 0.20684435626557007, 'rouge2': 0.20...</td>\n      <td>0.171952</td>\n      <td>0.188081</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>granite-7b-base_sql-v0.6</td>\n      <td>256.0</td>\n      <td>512.0</td>\n      <td>4.0</td>\n      <td>gate_proj, o_proj, v_proj, k_proj, q_proj, dow...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>6320.419308</td>\n      <td>10464.280273</td>\n      <td>0.733728</td>\n      <td>2.082831</td>\n      <td>PagedAdamW</td>\n      <td>0.495321</td>\n      <td>0.428704</td>\n      <td>0.635526</td>\n      <td>0.463588</td>\n      <td>0.222767</td>\n      <td>{'rouge1': 0.4596046325862796, 'rouge2': 0.424...</td>\n      <td>0.282041</td>\n      <td>0.222767</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>granite-7b-base_sql-v0.7</td>\n      <td>256.0</td>\n      <td>512.0</td>\n      <td>4.0</td>\n      <td>o_proj, v_proj, up_proj, k_proj, gate_proj, q_...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>8514.288527</td>\n      <td>10417.149414</td>\n      <td>0.751166</td>\n      <td>2.119470</td>\n      <td>PagedAdamW</td>\n      <td>0.412862</td>\n      <td>0.128201</td>\n      <td>0.173270</td>\n      <td>0.137502</td>\n      <td>0.101896</td>\n      <td>{'rouge1': 0.3485597640902298, 'rouge2': 0.242...</td>\n      <td>0.194294</td>\n      <td>0.101896</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>granite-7b-base_sql-v0.8</td>\n      <td>512.0</td>\n      <td>1024.0</td>\n      <td>4.0</td>\n      <td>o_proj, k_proj, down_proj, up_proj, q_proj, v_...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>8101.901001</td>\n      <td>13844.272949</td>\n      <td>0.885569</td>\n      <td>2.424363</td>\n      <td>PagedAdamW</td>\n      <td>0.290779</td>\n      <td>0.086690</td>\n      <td>0.108706</td>\n      <td>0.090811</td>\n      <td>0.066324</td>\n      <td>{'rouge1': 0.13304977962087253, 'rouge2': 0.09...</td>\n      <td>0.069795</td>\n      <td>0.066324</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>granite-7b-base_Original</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.324330</td>\n      <td>0.074863</td>\n      <td>0.076483</td>\n      <td>0.075050</td>\n      <td>0.080831</td>\n      <td>{'rouge1': 0.2449954357289134, 'rouge2': 0.167...</td>\n      <td>0.117901</td>\n      <td>0.080831</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>granite-7b-base_Original_withPrompt</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.540922</td>\n      <td>0.200351</td>\n      <td>0.204695</td>\n      <td>0.201463</td>\n      <td>0.225366</td>\n      <td>{'rouge1': 0.6722965654131353, 'rouge2': 0.565...</td>\n      <td>0.428616</td>\n      <td>0.225366</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>granite-7b-base_sql-v0.1_withPrompt</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.918859</td>\n      <td>0.831975</td>\n      <td>0.841008</td>\n      <td>0.834713</td>\n      <td>0.844340</td>\n      <td>{'rouge1': 0.9544219633306067, 'rouge2': 0.938...</td>\n      <td>0.902170</td>\n      <td>0.844340</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>granite-7b-base_sql-v0.2_withPrompt</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.208137</td>\n      <td>0.040781</td>\n      <td>0.040738</td>\n      <td>0.040752</td>\n      <td>0.039059</td>\n      <td>{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, ...</td>\n      <td>0.000000</td>\n      <td>0.039059</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>granite-7b-base_sql-v0.9</td>\n      <td>8.0</td>\n      <td>16.0</td>\n      <td>4.0</td>\n      <td>q_proj, k_proj, o_proj, down_proj, up_proj, ga...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>12076.062719</td>\n      <td>8027.226562</td>\n      <td>0.619540</td>\n      <td>1.858073</td>\n      <td>PagedAdamW</td>\n      <td>0.551238</td>\n      <td>0.383620</td>\n      <td>0.387880</td>\n      <td>0.384926</td>\n      <td>0.390526</td>\n      <td>{'rouge1': 0.43335724163913025, 'rouge2': 0.42...</td>\n      <td>0.409671</td>\n      <td>0.390526</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>granite-7b-base_sql-v0.9_withPrompt</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.939515</td>\n      <td>0.866402</td>\n      <td>0.870341</td>\n      <td>0.867626</td>\n      <td>0.878196</td>\n      <td>{'rouge1': 0.9755926628139077, 'rouge2': 0.958...</td>\n      <td>0.933917</td>\n      <td>0.878196</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>granite-7b-base_sql-v1.0</td>\n      <td>8.0</td>\n      <td>16.0</td>\n      <td>4.0</td>\n      <td>q_proj, gate_proj, up_proj, o_proj, down_proj,...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>4042.287440</td>\n      <td>8120.897461</td>\n      <td>0.621172</td>\n      <td>1.861107</td>\n      <td>PagedAdamW</td>\n      <td>0.551889</td>\n      <td>0.439885</td>\n      <td>0.552174</td>\n      <td>0.459533</td>\n      <td>0.295267</td>\n      <td>{'rouge1': 0.5161066358736365, 'rouge2': 0.489...</td>\n      <td>0.376061</td>\n      <td>0.295267</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {"id": "08b7d7a0-25cf-4e86-b75a-07ecc2b0746c"}, "cell_type": "code", "source": "i =6\npredicted = outputs[i]\nground_truth=ground_truths[i]\nprint(predicted+'\\n\\n'+ground_truth+'\\n')\n\ndevice = 'cuda'\npredicted = extract_answer(predicted)\nground_truth = extract_answer(ground_truth)\npredicted, ground_truth = prerocees_text(predicted, ground_truth)\n\nprint('curated = \\n'+predicted+'\\n\\n'+ground_truth)\n\n# Tokenize the predicted and ground truth sequences\ntokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to('cuda')\ntokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to('cuda')\n\n\n# Adding padding to match lengths if necessary\nlen_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\nif len_diff > 0:\n    tokens_ground_truth = torch.cat((tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\nelif len_diff < 0:\n    tokens_predicted = torch.cat((tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n# Flatten the tensors to 1D for metric computation\nflat_predictions = tokens_predicted.view(-1)\nflat_labels = tokens_ground_truth.view(-1)\nexact_match = (flat_predictions == flat_labels).float().mean().item()\n\nexact_match", "execution_count": 30, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'outputs' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m----> 2\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m[i]\n\u001b[1;32m      3\u001b[0m ground_truth\u001b[38;5;241m=\u001b[39mground_truths[i]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mground_truth\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n", "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "outputs[5]", "execution_count": 31, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'outputs' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutputs\u001b[49m[\u001b[38;5;241m5\u001b[39m]\n", "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "i=4\nprint(tool.create_prompt_with_answer_v2(**val_data[i]))\nprint('----------'*10)\nprint('------------------------------------------------Predicted------------------------------------------------\\n'+tool.generate_text(model, tokenizer, tool.create_prompt_v2(val_data[i]['question'], val_data[i]['context']))[0])", "execution_count": 42, "outputs": [{"output_type": "stream", "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n", "name": "stderr"}, {"output_type": "stream", "text": "### QUESTION\nWhat are the population and life expectancies in Brazil?\n\n### CONTEXT\nCREATE TABLE country (Population VARCHAR, LifeExpectancy VARCHAR, Name VARCHAR)\n\n### ANSWER\nSELECT Population, LifeExpectancy FROM country WHERE Name = \"Brazil\"</s>\n----------------------------------------------------------------------------------------------------\n------------------------------------------------Predicted------------------------------------------------\n### QUESTION\nWhat are the population and life expectancies in Brazil?\n\n### CONTEXT\nCREATE TABLE country (Population VARCHAR, LifeExpectancy VARCHAR, Name VARCHAR)\n\n### ANSWER\nSELECT Population, LifeExpectancy FROM country WHERE Name = \"Brazil\"\n\n### CONTEXT\nCREATE TABLE country (Population VARCHAR, LifeExpectancy VARCHAR, Name VARCHAR)\n\n### ANSWER\nSELECT Population, LifeExpectancy FROM country WHERE Name = \"Brazil\" GROUP BY Population ORDER BY LifeExpectancy DESC LIMIT 1\n\n### CONTEXT\nCREATE TABLE country (Population VARCHAR, LifeExpectancy VARCHAR, Name VARCHAR)\n\n### ANSWER\nSELECT Population, LifeExpectancy FROM country WHERE Name = \"Brazil\" GROUP BY Population ORDER BY LifeExpectancy DESC LIMIT 1 OFFSET 1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}