{"cells": [{"metadata": {}, "cell_type": "code", "source": "%pip install python-dotenv # Install the missing module 'dotenv'\n%pip install transformers datasets evaluate peft trl bitsandbytes accelerate\n%pip install huggingface\n", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Collecting python-dotenv\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.0.1\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (4.17.0)\nCollecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (2022.3.15)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: sacremoses in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.0.53)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (4.66.4)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (1.5.3)\nCollecting requests (from transformers)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (3.9.5)\nCollecting huggingface-hub<1.0,>=0.1.0 (from transformers)\n  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: psutil in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from peft) (5.9.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from peft) (2.0.1)\nCollecting safetensors (from peft)\n  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\nCollecting tokenizers<0.20,>=0.19 (from transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nCollecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.1.0->transformers)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n  Downloading rich-13.8.0-py3-none-any.whl.metadata (18 kB)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nCollecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n", "name": "stdout"}, {"output_type": "stream", "text": "Requirement already satisfied: click in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sacremoses->transformers) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sacremoses->transformers) (1.1.1)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading rich-13.8.0-py3-none-any.whl (241 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: xxhash, typing-extensions, shtab, safetensors, requests, pyarrow, mdurl, fsspec, docstring-parser, dill, multiprocess, markdown-it-py, huggingface-hub, tokenizers, rich, bitsandbytes, accelerate, tyro, transformers, datasets, trl, peft, evaluate\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Uninstalling typing_extensions-4.4.0:\n      Successfully uninstalled typing_extensions-4.4.0\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2022.11.0\n    Uninstalling fsspec-2022.11.0:\n      Successfully uninstalled fsspec-2022.11.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.6\n    Uninstalling dill-0.3.6:\n      Successfully uninstalled dill-0.3.6\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.6.0\n    Uninstalling huggingface-hub-0.6.0:\n      Successfully uninstalled huggingface-hub-0.6.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.17.0\n    Uninstalling transformers-4.17.0:\n      Successfully uninstalled transformers-4.17.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwatson-nlp 4.1.3 requires pyarrow==11.0.0, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.34.2 bitsandbytes-0.43.3 datasets-2.21.0 dill-0.3.8 docstring-parser-0.16 evaluate-0.4.2 fsspec-2024.6.1 huggingface-hub-0.24.6 markdown-it-py-3.0.0 mdurl-0.1.2 multiprocess-0.70.16 peft-0.12.0 pyarrow-17.0.0 requests-2.32.3 rich-13.8.0 safetensors-0.4.5 shtab-1.7.1 tokenizers-0.19.1 transformers-4.44.2 trl-0.10.1 typing-extensions-4.12.2 tyro-0.8.10 xxhash-3.5.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import os\nfrom dotenv import load_dotenv\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom ibm_watson_studio_lib import access_project_or_space\n\nwslib = access_project_or_space({\n        'token': 'p-2+kQvRm/s+j4gr3kN/L8nE9w==;jo3EuWo2iO+1+F0egvXSew==:ibH3fFKnohJx8UOS9q9Vf+Ewhuye4ERRlqD3CqbrUpUQvU3alsEPblep77PGbqbFrl+zlVsvRbeoVTX4TtP803SeBdM9yhyA2Q==',\n        'project_id': '512eaa42-cac0-46c4-a80c-d138ac7ccccc'\n})\n\nwslib.download_file('config.env')\nload_dotenv('config.env')\n\n# Connection variables\napi_key = os.getenv(\"API_KEY\", None)\nibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None) \nproject_id = os.getenv(\"PROJECT_ID\", None)\ncreds = {\n    \"url\": ibm_cloud_url,\n    \"apikey\": api_key \n}\naccess_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n).get_token()\n\nprint(api_key)\n# print(access_token)\nwslib.download_file('tool.py')\nwslib.download_file('evaluating.py')\nwslib.download_file('save.py')\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "kIBXc5WBDWd7IKLJhcGdzZd6Q0oY7Bzo1QBKSW2s8l_l\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 1, "data": {"text/plain": "{'file_name': 'save.py', 'summary': ['loaded data', 'saved to file']}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import torch\nimport transformers\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom transformers import Trainer\nimport json\nimport torch.nn as nn\n\nimport tool\n\ntorch.cuda.is_available()", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import os\nfrom dotenv import load_dotenv\n\nload_dotenv('.env')\nprint(os.getenv(\"TOKEN_HF\"))\n# model_name ='fb-opt-125m-sql'\nmodel_name ='TinyLlama1.1B-sql_v3'", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "None\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from huggingface_hub import login\n\nlogin(token=\"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\")", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /home/wsuser/.cache/huggingface/token\nLogin successful\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# loading the model from HF\nmodel, tokenizer, sql_model = tool.load_model_from_HF(model_name, quantization=True, base_model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0')", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "adapter_config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "931d0e52f1484be08d4be6a824c6f942"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "468e28e5c93b4ecdad4704b9d0a4ba6f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "07ef81efa6bc4dbe96f8a8b64c83c5cb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8f3928990c6a407586d7b072fbff746f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2f65ffb6595f458e895b858cac0be39e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8484ca2cccd2470b82d1b3cba11827d3"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6a48039c3e4e445ba04ababc1b0c1b69"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "de037c9fb99241739bf35eb5ca6d2c8c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "adapter_model.safetensors:   0%|          | 0.00/25.3M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "961f32b8834e46bd88f71431272cfa31"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from functools import partial\n# 2- Load the dataset:\nfrom datasets import load_dataset\n\n# dataset = load_dataset(\"OussamaAzz/sql_dataset_cleaned\")\ndataset = load_dataset(\"OussamaAzz/final-sql-dataset\")\n# dataset = load_dataset(\"OussamaAzz/instruction-sql-dataset\")\n", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading readme:   0%|          | 0.00/635 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f3ec162162914dd2856c9c6ac8d5fd2b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/2.64M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9437727184f242ccb6b7eb02f04d5084"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/145k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "07220087ad2d4306a94d66bee45c5770"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/104k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e1459f213c6d47aabac5ee02f23ee3bb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating train split:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ebc43cfee94147549967b328de6c2bd4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1f1a7093d18349ddad8c2141c7f539c3"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5c35e2ac0b304dba84116eed602115ce"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "def replace_eos(text, tokenizer):\n    return {\"text\": [t.replace('</s>', tokenizer.eos_token) for t in text[\"text\"]]}\n\ndef add_eos_token(text, tokenizer):\n    return {\"text\": [t + tokenizer.eos_token for t in text[\"text\"]]}\n\n# dataset['train'][\"text\"][0].replace('</s>','')/\n\nreplace_eos_with_tokenizer = partial(replace_eos, tokenizer=tokenizer)\nadd_eos_with_tokenizer = partial(add_eos_token, tokenizer=tokenizer)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# 4- Tokenizing the dataset:\n\n# Function to tokenize inputs and align labels\ndef tokenize_function(examples):\n    tokenized_inputs = tokenizer(examples[\"text\"],\n                                padding=\"max_length\",\n                                truncation=True,\n                                max_length = 512,\n                                return_overflowing_tokens=False,\n                                 )\n    # labels = tokenized_inputs[\"input_ids\"].copy()  # Copy input_ids to use as labels\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"],\n            \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n            }\n\ntrain_dataset = dataset['train']\nval_dataset = dataset['validation']\ntest_dataset = dataset['test']\n\nval_data = dataset['validation']['source']\ntest_data = dataset['test']['source']\n\ntrain_dataset = train_dataset.map(add_eos_with_tokenizer, batched=True)\nval_dataset = val_dataset.map(add_eos_with_tokenizer, batched=True)\ntest_dataset = test_dataset.map(add_eos_with_tokenizer, batched=True)\nprint(train_dataset['text'][0])\nprint(train_dataset['text'][0])\n\n\ntokenized_datasets = train_dataset.map(tokenize_function, batched=True)\ntokenized_datasets_val = val_dataset.map(tokenize_function, batched=True)\ntokenized_datasets_test = test_dataset.map(tokenize_function, batched=True)\n\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"source\"])\ntokenized_datasets_val = tokenized_datasets_val.remove_columns([\"text\", \"source\"])\ntokenized_datasets_test = tokenized_datasets_test.remove_columns([\"text\", \"source\"])\n\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntokenized_datasets_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntokenized_datasets_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "913864d98e274c44bb1bf846b2c70231"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "33590e9633c64558ae200c80797c62a9"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "61a3bbdfe177405ea81cf081f10cdc4d"}}, "metadata": {}}, {"output_type": "stream", "text": "### QUESTION\nWhat is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\n\n### CONTEXT\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\n\n### ANSWER\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"</s>\n### QUESTION\nWhat is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\n\n### CONTEXT\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\n\n### ANSWER\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"</s>\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "385dce457fbb435bb6bca844ae985cae"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2a75b530b98e493b927ef85a9069d1ca"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a5e99a4930d546a39679698859803047"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from datasets import Dataset\n\n# Assume `tokenizer` is already defined and imported\nEOS_TOKEN = tokenizer.eos_token  # Ensure this is defined\n\ndef get_prompt(data, include_answer=False):\n    if include_answer:\n        prompt = tool.create_prompt_with_answer_v2(**data) + EOS_TOKEN\n    else:\n        prompt = tool.create_prompt_v2(data['question'],data['context']) + EOS_TOKEN\n    \n    return prompt\n\ndef get_answer(data):\n    return [d['answer'] + EOS_TOKEN for d in data]\n\n# Function to convert data to Dataset\ndef convert_to_dataset(data, tokenizer, include_labels=True, include_answer=False):\n    # Assume `tokenizer` is already defined and imported\n    EOS_TOKEN = tokenizer.eos_token  \n    # Assuming `tool.create_prompt_with_answer_v2(**d)` returns a string\n    # text = [tool.create_prompt_with_answer_v2(**d) + EOS_TOKEN for d in data]\n    text = [get_prompt(d, include_answer) for d in data]\n    answer = []\n    # if not include_answer:\n    answer = get_answer(data)\n    \n    if include_labels:\n        # Creating new labels for the dataset\n        labels = [i for i in range(len(data))]\n        return Dataset.from_dict({\"text\": text, \"labels\": labels, \"answer\": answer})\n    else:\n        return Dataset.from_dict({\"text\": text, \"answer\": answer})\n\n# Convert train and validation data to datasets\n# train_dataset = convert_to_dataset(data, include_labels=False, tokenizer=tokenizer, include_answer=True)\n# val_dataset = convert_to_dataset(val_data, include_labels=True, tokenizer=tokenizer, include_answer=False)\n\ntrain_dataset = convert_to_dataset(train_dataset['source'], include_labels=False, tokenizer=tokenizer, include_answer=True)\nval_dataset = convert_to_dataset(val_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=False)\ntest_dataset = convert_to_dataset(test_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=False)\n\n\n# Display an example from the datasets\nprint(train_dataset[2000])\nprint(val_dataset[0])\n\n", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "{'text': '### QUESTION\\nHow much in total does customer with first name as Carole and last name as Bernhard paid?\\n\\n### CONTEXT\\nCREATE TABLE Customers (customer_id VARCHAR, first_name VARCHAR, last_name VARCHAR); CREATE TABLE Customer_Payments (amount_payment INTEGER, customer_id VARCHAR)\\n\\n### ANSWER\\nSELECT SUM(T1.amount_payment) FROM Customer_Payments AS T1 JOIN Customers AS T2 ON T1.customer_id = T2.customer_id WHERE T2.first_name = \"Carole\" AND T2.last_name = \"Bernhard\"</s></s>', 'answer': 'SELECT SUM(T1.amount_payment) FROM Customer_Payments AS T1 JOIN Customers AS T2 ON T1.customer_id = T2.customer_id WHERE T2.first_name = \"Carole\" AND T2.last_name = \"Bernhard\"</s>'}\n{'text': '### QUESTION\\nHow many countries were sampled in the index created by The Economist, published in 2007 and ranked 2nd in the LA Ranking?\\n\\n### CONTEXT\\nCREATE TABLE table_19948664_1 (countries_sampled INTEGER, ranking_la__2_ VARCHAR, author___editor___source VARCHAR, year_of_publication VARCHAR)\\n\\n### ANSWER\\n</s>', 'labels': 0, 'answer': 'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"</s>'}\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Function to tokenize inputs and align labels\ndef tokenize_function(examples):\n    tokenized_inputs = tokenizer(examples[\"answer\"], padding=\"max_length\", truncation=True, max_length=512)\n    labels = tokenized_inputs[\"input_ids\"].copy()  # Copy input_ids to use as labels\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"attention_mask\": tokenized_inputs[\"attention_mask\"], \"labels\": labels}\n\n# Prepare the dataset\ndata_dict = {\n    'answer': val_dataset[\"answer\"],\n    'labels': val_dataset[\"labels\"],\n}\ndataset_val = Dataset.from_dict(data_dict)\ntokenized_datasets = dataset_val.map(tokenize_function, batched=True)\ntokenized_datasets_answer = tokenized_datasets.remove_columns([\"answer\"])\ntokenized_datasets_answer.set_format(\"torch\")\n\neval_answer_dataloader = DataLoader(tokenized_datasets_answer, batch_size=8)", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "da398e377fe64c9fb7e7db7b77e84697"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import re\n\ndef extract_answer(output, eos_token='</s>'):\n    # Use regex to extract the answer part from the generated text\n    pattern = rf'### ANSWER\\n(.*?)(?:{re.escape(eos_token)}|###|$)'\n    match = re.search(pattern, output, re.DOTALL)\n    if match:\n        # print('match!')\n        return match.group(1).strip()\n    return output.strip()\n\n# Example output for testing\nexample_output = tool.create_prompt_with_answer_v2(**val_data[1])\nexp2 = tool.create_prompt_with_answer_v2(**val_data[2])\n\no = extract_answer(example_output, eos_token='</s>')\no2 = extract_answer(exp2, eos_token='</s>')\nprint(o)\nprint(o2)", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "SELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"\nSELECT MAX(opponents) FROM table_20745444_1 WHERE record = \"4-0\"\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# print(results)\ndef prerocees_text(text1, text2, EOS_TOKEN = '</s>'):\n    text1 =str(text1)\n    text2 =str(text2)\n    text1 = extract_answer(text1, EOS_TOKEN)\n    text2 = extract_answer(text2, EOS_TOKEN)\n    # Remove the special tokens <s> and </s> if present\n    text1 = text1.replace('<s>', '').replace('</s>', '')\n    text2 = text2.replace('<s>', '').replace('</s>', '')\n    # Remove any special tokens from the text\n    # text1 = text1.replace(EOS_TOKEN, '')\n    # text2 = text2.replace(EOS_TOKEN, '')\n\n    # text1 = text1.lower()\n    # text2 = text2.lower()\n    # Assuring same length\n    if len(text1) > len(text2):\n        text2 = text2.ljust(len(text1))\n    else:\n        text1 = text1.ljust(len(text2))\n    return text1, text2\n\nprint(prerocees_text('hahaha'+val_data[0]['answer']+'hahaha', val_data[0]['answer']))\n# len\nprint(len('hahaha'+val_data[0]['answer']+'hahaha'), len(val_data[0]['answer']))\n#after preprocesing\nt1 , t2 = prerocees_text('hahaha'+val_data[0]['answer']+'hahaha', val_data[0]['answer'])\nprint(len(t1), len(t2))\nprint(t1,'\\n'+ t2)\nt2", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "('hahahaSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"hahaha', 'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            ')\n172 160\n172 172\nhahahaSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"hahaha \nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            \n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            '"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "out2 = tool.create_prompt_v3(val_data[0]['question'], val_data[0]['context'])\nout2_with_answer = tool.create_prompt_with_answer_v3(**val_data[0])\n# print(out2)\n# print(out2_with_answer)\nout2 = tool.generate_text_v2(sql_model, tokenizer,out2)\nout2", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "['Below is an instruction (question) that describes a task, paired with an input (context) that provides further context. Write an SQL query response that appropriately completes the request.\\n\\n### QUESTION\\nHow many countries were sampled in the index created by The Economist, published in 2007 and ranked 2nd in the LA Ranking?\\n\\n### CONTEXT\\nCREATE TABLE table_19948664_1 (countries_sampled INTEGER, ranking_la__2_ VARCHAR, author___editor___source VARCHAR, year_of_publication VARCHAR)\\n\\n### ANSWER\\nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ =']"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "t3 , t4 = prerocees_text(out2[0], out2_with_answer)\nprint(len(t3), len(t4))\nprint(t3,'\\n'+ t4)", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "160 160\nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ =       \nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Generate the text using the model\noutput = tool.generate_text(model, tokenizer, tool.create_prompt_v2(val_data[1]['question'], val_data[1]['context']))\n\n# Remove the special tokens <s> and </s>\n# cleaned_output = output[0].replace('<s>', '').replace('</s>', '').strip()\n\n# # Display the cleaned output\n# print(cleaned_output)\n# print(tool.create_prompt_with_answer_v2(**val_data[0]))", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Display the cleaned output\nprint(tool.create_prompt_with_answer_v2(**val_data[1]))\nprint('\\n\\noutput= \\n'+output[0]+EOS_TOKEN)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "### QUESTION\nWhat is the format for the United States dated July 23, 2002?\n\n### CONTEXT\nCREATE TABLE table_name_84 (format VARCHAR, region VARCHAR, date VARCHAR)\n\n### ANSWER\nSELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"</s>\n\n\noutput= \n### QUESTION\nWhat is the format for the United States dated July 23, 2002?\n\n### CONTEXT\nCREATE TABLE table_name_84 (format VARCHAR, region VARCHAR, date VARCHAR)\n\n### ANSWER\nSELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"</s>\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import torch\n\nDEVICE = 'cuda'\n\ndef generate_text(model, tokenizer, input_text=\"def generate():\", max_length=200, **kwargs):\n    # Set the model to evaluation mode and move it to the desired device\n    model.eval()\n    model.to(DEVICE)\n\n    # Tokenize the input text and move the tokens to the device\n    input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n\n    # Use mixed precision for faster inference\n    with torch.no_grad():\n        with torch.cuda.amp.autocast():\n            # Generate output tokens\n            output = model.generate(**input_tokens, max_length=max_length, **kwargs)\n\n    # Decode the output tokens into text\n    output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n\n    return output_text\n\n# Example usage\n# output_text = generate_text(model, tokenizer, input_text=\"def generate():\", max_length=200)\n", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import evaluate\n\n# Calculating similarity\ndef calculate_similarity_accuracy_v2(predicted, ground_truth, device='cuda', dataloader=None, **kwargs):\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Load accuracy metric\n    metric = evaluate.load(\"accuracy\")\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat(\n            (tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat(\n            (tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for accuracy computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Compute accuracy for each token\n    metric.add_batch(predictions=flat_predictions, references=flat_labels)\n\n    # Calculate cosine similarity between the predicted and ground truth sequences\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    print(\"\\nCosine similarity:\\n\", cosine_similarity)\n\n    accuracy = metric.compute()\n    print(\"\\nAccuracy:\\n\", accuracy)\n    \n    return cosine_similarity, accuracy['accuracy']\n\n# Example usage\nsim = calculate_similarity_accuracy_v2(output[0], tool.create_prompt_with_answer_v2(**val_data[1]), device='cuda')\nsim2 = calculate_similarity_accuracy_v2(t3, t4, device='cuda')", "execution_count": 19, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "40eef87d8d3e477e9a532167d92a6b77"}}, "metadata": {}}, {"output_type": "stream", "text": "\nCosine similarity:\n tensor([1.], device='cuda:0')\n\nAccuracy:\n {'accuracy': 1.0}\n\nCosine similarity:\n tensor([0.9672], device='cuda:0')\n\nAccuracy:\n {'accuracy': 0.9344262295081968}\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "%pip install rouge_score", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.0.0)\nCollecting nltk (from rouge_score)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: click in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (1.1.1)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (2022.3.15)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.4)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=5510be694829976f22f456d831fbfe91c08b329f2470ce6cf8b06e3ed8afe29f\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: nltk, rouge_score\nSuccessfully installed nltk-3.9.1 rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import torch\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom tqdm import tqdm\nimport evaluate\nimport torch.nn as nn\n\n\n# Calculating similarity and metrics\ndef calculate_metrics(predicted, ground_truth, device='cuda', use_print=False, **kwargs):\n    predicted = extract_answer(predicted)\n    ground_truth = extract_answer(ground_truth)\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat(\n            (tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat(\n            (tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for metric computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Load metrics\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric = evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n    rouge_metric = evaluate.load(\"rouge\")\n    bleu_metric = evaluate.load(\"bleu\")\n\n    # Add batches to metrics\n    precision_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    recall_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    f1_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    accuracy_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    \n    # For ROUGE and BLEU, you usually need sequences, so they may need to be adapted depending on how they process data.\n    # Since we're dealing with token-level metrics, you can use sequences directly.\n    rouge_metric.add_batch(predictions=[predicted], references=[ground_truth])\n    bleu_metric.add_batch(predictions=[predicted], references=[[ground_truth]])\n\n    # Compute metrics\n    precision = precision_metric.compute(average='macro')\n    recall = recall_metric.compute(average='macro')\n    f1 = f1_metric.compute(average='macro')\n    accuracy = accuracy_metric.compute()\n    rouge = rouge_metric.compute()\n    bleu = bleu_metric.compute()\n\n    # Calculate cosine similarity between the predicted and ground truth sequences\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    \n    # Calculate perplexity\n    log_probs = nn.functional.log_softmax(tokens_predicted.float(), dim=-1)\n    perplexity = torch.exp(-log_probs.mean()).item()\n\n    # Exact match\n    exact_match = (flat_predictions == flat_labels).float().mean().item()\n\n    if use_print:\n    # Print results\n        print(\"\\nCosine similarity:\\n\", cosine_similarity)\n        print(\"\\nPrecision:\\n\", precision)\n        print(\"\\nRecall:\\n\", recall)\n        print(\"\\nF1 Score:\\n\", f1)\n        print(\"\\nAccuracy:\\n\", accuracy)\n        print(\"\\nROUGE Score:\\n\", rouge)\n        print(\"\\nBLEU Score:\\n\", bleu)\n        print(\"\\nPerplexity:\\n\", perplexity)\n        print(\"\\nExact Match:\\n\", exact_match)\n\n    return {\n        \"cosine_similarity\": cosine_similarity,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n        \"accuracy\": accuracy,\n        \"rouge_score\": rouge,\n        \"bleu_score\": bleu,\n        \"perplexity\": perplexity,\n        \"exact_match\": exact_match,\n    }\n\n\n# Example usage\nmetrics = calculate_metrics(output[0], tool.create_prompt_with_answer_v2(**val_data[1]), device='cuda', use_print=False)\n", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b58452c3bfe54f9490f5d0efb977da3c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2c601a3af4c144c8bfdf70dfecd97b57"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "017fdeac044c43dfa8daf3fc27b06895"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "metrics", "execution_count": 23, "outputs": [{"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "{'cosine_similarity': tensor([1.], device='cuda:0'),\n 'precision': {'precision': 1.0},\n 'recall': {'recall': 1.0},\n 'f1_score': {'f1': 1.0},\n 'accuracy': {'accuracy': 1.0},\n 'rouge_score': {'rouge1': 1.0,\n  'rouge2': 1.0,\n  'rougeL': 1.0,\n  'rougeLsum': 1.0},\n 'bleu_score': {'bleu': 1.0,\n  'precisions': [1.0, 1.0, 1.0, 1.0],\n  'brevity_penalty': 1.0,\n  'length_ratio': 1.0,\n  'translation_length': 24,\n  'reference_length': 24},\n 'perplexity': inf,\n 'exact_match': 1.0}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# accuracy for the first 10 validation data\noutputs = [generate_text(sql_model, tokenizer, tool.create_prompt_v3(d['question'], d['context']))[0]+EOS_TOKEN for d in val_data]\nground_truths = [tool.create_prompt_with_answer_v3(**d) for d in val_data[:10]]\n\n# Calculate the accuracy for each pair of output and ground truth\n# accuracies = [calculate_similarity_v2(output, ground_truth) for output, ground_truth in zip(outputs, ground_truths)]", "execution_count": 27, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import tqdm\n# Initialize accumulators for metrics\nmetrics_accumulator = {\n    \"cosine_similarity\": 0.0,\n    \"precision\": 0.0,\n    \"recall\": 0.0,\n    \"f1_score\": 0.0,\n    \"accuracy\": 0.0,\n    \"rouge_score\": {\n        \"rouge1\": 0.0,\n        \"rouge2\": 0.0,\n        \"rougeL\": 0.0,\n        \"rougeLsum\": 0.0\n    },\n    \"bleu_score\": 0.0,\n    \"perplexity\": 0.0,\n    \"exact_match\": 0.0,\n}\n\n# Calculate the metrics for each output-ground truth pair\n# for output, ground_truth in tqdm(zip(outputs, ground_truths), total=len(outputs), desc=\"Calculating Metrics\"):\nfor output, ground_truth in zip(outputs, ground_truths):\n    metrics = calculate_metrics(output, ground_truth, device='cuda')\n\n    # Accumulate metrics\n    metrics_accumulator[\"cosine_similarity\"] += metrics[\"cosine_similarity\"].mean().item()\n    metrics_accumulator[\"precision\"] += metrics[\"precision\"][\"precision\"]\n    metrics_accumulator[\"recall\"] += metrics[\"recall\"][\"recall\"]\n    metrics_accumulator[\"f1_score\"] += metrics[\"f1_score\"][\"f1\"]\n    metrics_accumulator[\"accuracy\"] += metrics[\"accuracy\"][\"accuracy\"]\n    \n    for key in metrics[\"rouge_score\"]:\n        metrics_accumulator[\"rouge_score\"][key] += metrics[\"rouge_score\"][key]\n    \n    metrics_accumulator[\"bleu_score\"] += metrics[\"bleu_score\"][\"bleu\"]\n    metrics_accumulator[\"perplexity\"] += metrics[\"perplexity\"]\n    metrics_accumulator[\"exact_match\"] += metrics[\"exact_match\"]\n\n# Calculate the average for each metric\nnum_samples = 10  # Since we're using 10 samples\naverage_metrics = {\n    \"cosine_similarity\": metrics_accumulator[\"cosine_similarity\"] / num_samples,\n    \"precision\": metrics_accumulator[\"precision\"] / num_samples,\n    \"recall\": metrics_accumulator[\"recall\"] / num_samples,\n    \"f1_score\": metrics_accumulator[\"f1_score\"] / num_samples,\n    \"accuracy\": metrics_accumulator[\"accuracy\"] / num_samples,\n    \"rouge_score\": {\n        key: metrics_accumulator[\"rouge_score\"][key] / num_samples for key in metrics_accumulator[\"rouge_score\"]\n    },\n    \"bleu_score\": metrics_accumulator[\"bleu_score\"] / num_samples,\n    \"perplexity\": metrics_accumulator[\"perplexity\"] / num_samples,\n    \"exact_match\": metrics_accumulator[\"exact_match\"] / num_samples,\n}\n\n# Print the average metrics\nprint(\"Average Metrics for the First 10 Validation Samples:\")\nfor key, value in average_metrics.items():\n    if isinstance(value, dict):\n        print(f\"{key}:\")\n        for sub_key, sub_value in value.items():\n            print(f\"  {sub_key}: {sub_value:.4f}\")\n    else:\n        print(f\"{key}: {value:.4f}\")", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n", "name": "stderr"}, {"output_type": "stream", "text": "Average Metrics for the First 10 Validation Samples:\ncosine_similarity: 0.9543\nprecision: 0.9103\nrecall: 0.9232\nf1_score: 0.9129\naccuracy: 0.9159\nrouge_score:\n  rouge1: 0.9454\n  rouge2: 0.9264\n  rougeL: 0.9454\n  rougeLsum: 0.9454\nbleu_score: 0.9112\nperplexity: inf\nexact_match: 0.9159\n", "name": "stdout"}, {"output_type": "stream", "text": "/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}