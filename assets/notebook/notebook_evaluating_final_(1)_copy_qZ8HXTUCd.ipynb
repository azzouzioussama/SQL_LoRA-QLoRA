{"cells": [{"metadata": {"id": "68a649cb-7042-4198-a06d-57eff7f73b23", "scrolled": true}, "cell_type": "code", "source": "%pip install python-dotenv # Install the missing module 'dotenv'\n%pip install transformers datasets evaluate peft trl bitsandbytes accelerate\n%pip install huggingface\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting python-dotenv\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.0.1\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (4.17.0)\nCollecting datasets\n  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (2022.3.15)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: sacremoses in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.0.53)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from transformers) (4.66.4)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (1.5.3)\nCollecting requests (from transformers)\n  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from datasets) (3.9.5)\nCollecting huggingface-hub<1.0,>=0.1.0 (from transformers)\n  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: psutil in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from peft) (5.9.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from peft) (2.0.1)\nCollecting safetensors (from peft)\n  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\nCollecting tokenizers<0.20,>=0.19 (from transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nCollecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.1.0->transformers)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nCollecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: click in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sacremoses->transformers) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from sacremoses->transformers) (1.1.1)\nDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading rich-13.8.1-py3-none-any.whl (241 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: xxhash, typing-extensions, shtab, safetensors, requests, pyarrow, mdurl, fsspec, docstring-parser, dill, multiprocess, markdown-it-py, huggingface-hub, tokenizers, rich, bitsandbytes, accelerate, tyro, transformers, datasets, trl, peft, evaluate\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Uninstalling typing_extensions-4.4.0:\n      Successfully uninstalled typing_extensions-4.4.0\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2022.11.0\n    Uninstalling fsspec-2022.11.0:\n      Successfully uninstalled fsspec-2022.11.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.6\n    Uninstalling dill-0.3.6:\n      Successfully uninstalled dill-0.3.6\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.6.0\n    Uninstalling huggingface-hub-0.6.0:\n      Successfully uninstalled huggingface-hub-0.6.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.17.0\n    Uninstalling transformers-4.17.0:\n      Successfully uninstalled transformers-4.17.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwatson-nlp 4.1.3 requires pyarrow==11.0.0, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.34.2 bitsandbytes-0.43.3 datasets-3.0.0 dill-0.3.8 docstring-parser-0.16 evaluate-0.4.3 fsspec-2024.6.1 huggingface-hub-0.24.7 markdown-it-py-3.0.0 mdurl-0.1.2 multiprocess-0.70.16 peft-0.12.0 pyarrow-17.0.0 requests-2.32.3 rich-13.8.1 safetensors-0.4.5 shtab-1.7.1 tokenizers-0.19.1 transformers-4.44.2 trl-0.10.1 typing-extensions-4.12.2 tyro-0.8.10 xxhash-3.5.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {"id": "5bf69b08-6348-43d7-947f-45962e80a3fa"}, "cell_type": "code", "source": "import os\nfrom dotenv import load_dotenv\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom ibm_watson_studio_lib import access_project_or_space\n\nwslib = access_project_or_space({\n        'token': 'p-2+M7N412nNMq+LSbsjANoOoQ==;LFeF3V6i+F/jEnezq8oOQA==:l0bUpeHOW5rp8xq20UiCjJQyak+tK37f7uTyFZsV7YvvbFmQbYhtaO3KgtiCa1qahvIu57LYjESD5n0TXPH5u0ZHGef4njBD5A==',\n        'project_id': 'bdd13a82-ee92-406c-bc3d-fc0690f7cb1e'\n})\n\nwslib.download_file('config.env')\nload_dotenv('config.env')\n\n# Connection variables\napi_key = os.getenv(\"API_KEY\", None)\nibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None) \nproject_id = os.getenv(\"PROJECT_ID\", None)\ncreds = {\n    \"url\": ibm_cloud_url,\n    \"apikey\": api_key \n}\naccess_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n).get_token()\n\nprint(api_key)\n# print(access_token)\nwslib.download_file('tool.py')\nwslib.download_file('evaluating.py')\nwslib.download_file('save.py')\n", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "R4Ura8MOO50SxIpnaApHAlK5X0sJ9VyCvyPs91xTdmEK\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 2, "data": {"text/plain": "{'file_name': 'save.py', 'summary': ['loaded data', 'saved to file']}"}, "metadata": {}}]}, {"metadata": {"id": "3b3f0274-42fd-4222-9b1d-1c4d35a291a0"}, "cell_type": "code", "source": "import torch\nimport transformers\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom transformers import Trainer\nimport json\nimport torch.nn as nn\n\nimport tool\n\ntorch.cuda.is_available()", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {"id": "2fffa942-d289-42b9-9117-4a0b3a378bb6"}, "cell_type": "code", "source": "## import os\nfrom dotenv import load_dotenv\n\nload_dotenv('.env')\nprint(os.getenv(\"TOKEN_HF\"))\n# model_name ='fb-opt-125m-sql'\nmodel_name = 'TinyLlama-1.1B-Chat-v1.0_sql-v1.5'\nmodel_name ='TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning'\n# model_name ='TinyLlama-1.1B-Chat-v1.0_original_withPrompt'", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "None\n", "name": "stdout"}]}, {"metadata": {"id": "081bc904-ee30-4444-911a-f719e782a06e"}, "cell_type": "code", "source": "from huggingface_hub import login\n\nlogin(token=\"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\")", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /home/wsuser/.cache/huggingface/token\nLogin successful\n", "name": "stdout"}]}, {"metadata": {"id": "8c5346fd-63aa-43d4-98b8-823bbf8c1218"}, "cell_type": "code", "source": "# %pip install git+https://github.com/huggingface/peft.git\n", "execution_count": 6, "outputs": []}, {"metadata": {"id": "5d9f910c-2c27-48bb-8063-9eb951ea9385"}, "cell_type": "code", "source": "# import torch\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from peft import PeftModel, PeftConfig\n\n# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n# model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", return_dict=True, device_map='auto',\n# #                                              torch_dtype=torch.float16, \n# #                                              load_in_8bit=True\n#                                             )\n# base_model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n# sql_model = model", "execution_count": 7, "outputs": []}, {"metadata": {"id": "8ddb78de-6e61-4a7f-8cfb-6665f9757dcf"}, "cell_type": "code", "source": "# sql_model = model", "execution_count": 8, "outputs": []}, {"metadata": {"id": "a14aeb0f-1c01-4e9b-9c1b-b155e88e041d"}, "cell_type": "code", "source": "# loading the model from HF\nmodel, tokenizer, sql_model = tool.load_model_from_HF(model_name, quantization='False', base_model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', HF_user = 'koukoudzz')\n# Set padding to be on the left side for decoder-only architecture\ntokenizer.padding_side = 'left'\n\n# Assign pad token if not already set\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token", "execution_count": 9, "outputs": []}, {"metadata": {"id": "be7687dc-8acd-43f7-b858-c8ee97ffb38b"}, "cell_type": "code", "source": "!nvidia-smi", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Wed Sep 11 20:45:50 2024       \r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  Tesla V100-PCIE-16GB           Off | 00000000:AF:00.0 Off |                    0 |\r\n| N/A   31C    P0              38W / 250W |   2606MiB / 16384MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n                                                                                         \r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n+---------------------------------------------------------------------------------------+\r\n", "name": "stdout"}]}, {"metadata": {"id": "13ebcadd-0a21-455d-b99f-0f31f2721380"}, "cell_type": "code", "source": "from functools import partial\n# 2- Load the dataset:\nfrom datasets import load_dataset\n\n# dataset = load_dataset(\"OussamaAzz/sql_dataset_cleaned\")\ndataset = load_dataset(\"OussamaAzz/final-sql-dataset\")\n# dataset = load_dataset(\"OussamaAzz/instruction-sql-dataset\")\n", "execution_count": 11, "outputs": []}, {"metadata": {"id": "7362442a-a840-4782-baeb-6d21b80fa95d"}, "cell_type": "code", "source": "def replace_eos(text, tokenizer):\n    return {\"text\": [t.replace('</s>', tokenizer.eos_token) for t in text[\"text\"]]}\n\ndef add_eos_token(text, tokenizer):\n    return {\"text\": [t + tokenizer.eos_token for t in text[\"text\"]]}\n\n# dataset['train'][\"text\"][0].replace('</s>','')/\n\nreplace_eos_with_tokenizer = partial(replace_eos, tokenizer=tokenizer)\nadd_eos_with_tokenizer = partial(add_eos_token, tokenizer=tokenizer)", "execution_count": 12, "outputs": []}, {"metadata": {"id": "52175ab7-7d8e-430f-a176-15c5d9b877ab"}, "cell_type": "code", "source": "# 4- Tokenizing the dataset:\n\n# Function to tokenize inputs and align labels\ndef tokenize_function(examples):\n    tokenized_inputs = tokenizer(examples[\"text\"],\n#                                 padding=\"max_length\",\n#                                 truncation=True,\n#                                 max_length = 512,\n#                                 return_overflowing_tokens=False,\n                                 )\n    # labels = tokenized_inputs[\"input_ids\"].copy()  # Copy input_ids to use as labels\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"],\n            \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n            }\n\ntrain_dataset = dataset['train']\nval_dataset = dataset['validation']\ntest_dataset = dataset['test']\n\n\nval_data = dataset['validation']['source']\ntest_data = dataset['test']['source']\n# test_data = dataset['test']['source']\n\ntrain_dataset = train_dataset.map(add_eos_with_tokenizer, batched=True)\nval_dataset = val_dataset.map(add_eos_with_tokenizer, batched=True)\ntest_dataset = test_dataset.map(add_eos_with_tokenizer, batched=True)\nprint(train_dataset['text'][0])\nprint(train_dataset['text'][0])\n\n\ntokenized_datasets = train_dataset.map(tokenize_function, batched=True)\ntokenized_datasets_val = val_dataset.map(tokenize_function, batched=True)\ntokenized_datasets_test = test_dataset.map(tokenize_function, batched=True)\n\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"source\"])\ntokenized_datasets_val = tokenized_datasets_val.remove_columns([\"text\", \"source\"])\ntokenized_datasets_test = tokenized_datasets_test.remove_columns([\"text\", \"source\"])\n\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntokenized_datasets_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntokenized_datasets_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "### QUESTION\nWhat is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\n\n### CONTEXT\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\n\n### ANSWER\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"</s>\n### QUESTION\nWhat is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\n\n### CONTEXT\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\n\n### ANSWER\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"</s>\n", "name": "stdout"}]}, {"metadata": {"id": "2d728129-dfdc-4ee3-ad86-2c1ccbf05f0d"}, "cell_type": "code", "source": "from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    # Get all the input_ids and attention_masks from the batch\n    input_ids = [item['input_ids'] for item in batch]\n    attention_masks = [item['attention_mask'] for item in batch]\n    \n    # Pad all sequences in the batch to the length of the longest sequence\n    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n    \n    return {\n        'input_ids': input_ids_padded,\n        'attention_mask': attention_masks_padded\n    }\n", "execution_count": 14, "outputs": []}, {"metadata": {"id": "a1fa3a0f-3429-442e-969d-501c1b9b1c04"}, "cell_type": "code", "source": "from datasets import Dataset\n\n# Assume `tokenizer` is already defined and imported\nEOS_TOKEN = tokenizer.eos_token  # Ensure this is defined\n\ndef get_prompt(data, include_answer=False):\n    if include_answer:\n        prompt = tool.create_prompt_with_answer_v2(**data) + EOS_TOKEN\n    else:\n        prompt = tool.create_prompt_v2(data['question'],data['context'])\n    \n    return prompt\n\ndef get_answer(data):\n    return [d['answer'] + EOS_TOKEN for d in data]\n\n# Function to convert data to Dataset\ndef convert_to_dataset(data, tokenizer, include_labels=True, include_answer=False):\n    # Assume `tokenizer` is already defined and imported\n    EOS_TOKEN = tokenizer.eos_token  \n    # Assuming `tool.create_prompt_with_answer_v2(**d)` returns a string\n    # text = [tool.create_prompt_with_answer_v2(**d) + EOS_TOKEN for d in data]\n    text = [get_prompt(d, include_answer) for d in data]\n    answer = []\n    # if not include_answer:\n    answer = get_answer(data)\n    \n    if include_labels:\n        # Creating new labels for the dataset\n        labels = [i for i in range(len(data))]\n        return Dataset.from_dict({\"text\": text, \"labels\": labels, \"answer\": answer})\n    else:\n        return Dataset.from_dict({\"text\": text, \"answer\": answer})\n\n# Convert train and validation data to datasets\n# train_dataset = convert_to_dataset(data, include_labels=False, tokenizer=tokenizer, include_answer=True)\n# val_dataset = convert_to_dataset(val_data, include_labels=True, tokenizer=tokenizer, include_answer=False)\n\ntrain_dataset = convert_to_dataset(train_dataset['source'], include_labels=False, tokenizer=tokenizer, include_answer=True)\nval_dataset = convert_to_dataset(val_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=False)\ntest_dataset = convert_to_dataset(test_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=False)\n\n\n# Display an example from the datasets\n# print(train_dataset[2000])\nval_data1 = val_dataset\nprint(val_data1[0])\ntokenized_datasets_val = val_data1.map(tokenize_function, batched=True)\ntokenized_datasets_val = tokenized_datasets_val.remove_columns([\"text\", \"answer\",'labels'])\nprint(tokenized_datasets_val[1])\ntokenized_datasets_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n#-------------------------------------------------------\n\ntest_data1 = test_dataset\nprint(test_data1[0])\ntokenized_datasets_test = test_data1.map(tokenize_function, batched=True)\ntokenized_datasets_test = tokenized_datasets_test.remove_columns([\"text\", \"answer\",'labels'])\nprint(tokenized_datasets_test[1])\ntokenized_datasets_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\neval_text_dataloader = DataLoader(\n#     tokenized_datasets_val,\n    tokenized_datasets_test,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=collate_fn  # Use the custom collate function\n)\n# DataLoader(tokenized_datasets_val, batch_size=8)", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "{'text': '### QUESTION\\nHow many countries were sampled in the index created by The Economist, published in 2007 and ranked 2nd in the LA Ranking?\\n\\n### CONTEXT\\nCREATE TABLE table_19948664_1 (countries_sampled INTEGER, ranking_la__2_ VARCHAR, author___editor___source VARCHAR, year_of_publication VARCHAR)\\n\\n### ANSWER\\n', 'labels': 0, 'answer': 'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"</s>'}\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e7ebbdb0b8fa46859521b0740bbac75b"}}, "metadata": {}}, {"output_type": "stream", "text": "{'input_ids': [1, 835, 660, 4462, 1254, 2725, 13, 5618, 338, 278, 3402, 363, 278, 3303, 3900, 29797, 5468, 29871, 29906, 29941, 29892, 29871, 29906, 29900, 29900, 29906, 29973, 13, 13, 2277, 29937, 8707, 16975, 13, 27045, 10911, 1591, 29918, 978, 29918, 29947, 29946, 313, 4830, 21748, 29892, 5120, 21748, 29892, 2635, 21748, 29897, 13, 13, 2277, 29937, 319, 3059, 29956, 1001, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n{'text': '### QUESTION\\nWho was the Class AAAA champion in 2006-07?\\n\\n### CONTEXT\\nCREATE TABLE table_14603057_2 (class_aAAA VARCHAR, school_year VARCHAR)\\n\\n### ANSWER\\n', 'labels': 0, 'answer': 'SELECT class_aAAA FROM table_14603057_2 WHERE school_year = \"2006-07\"</s>'}\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ea8230b3524f4e62addc85615b2e2bbb"}}, "metadata": {}}, {"output_type": "stream", "text": "{'input_ids': [1, 835, 660, 4462, 1254, 2725, 13, 5328, 1784, 14165, 911, 1612, 1338, 363, 278, 22900, 411, 263, 22125, 310, 29871, 29896, 29896, 322, 3109, 1135, 29871, 29896, 16466, 29973, 13, 13, 2277, 29937, 8707, 16975, 13, 27045, 10911, 1591, 29918, 978, 29918, 29929, 313, 1182, 265, 911, 2672, 4330, 17070, 29892, 7115, 21748, 29892, 13283, 21748, 29897, 13, 13, 2277, 29937, 319, 3059, 29956, 1001, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n", "name": "stdout"}]}, {"metadata": {"id": "b90e7aae-506d-481c-a733-e37db99fcc83"}, "cell_type": "code", "source": "# # Function to tokenize inputs and align labels\n# def tokenize_function(examples):\n#     tokenized_inputs = tokenizer(examples[\"answer\"], padding=\"max_length\", truncation=True, max_length=512)\n#     labels = tokenized_inputs[\"input_ids\"].copy()  # Copy input_ids to use as labels\n#     return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"attention_mask\": tokenized_inputs[\"attention_mask\"], \"labels\": labels}\n\n# # Prepare the dataset\n# data_dict = {\n#     'answer': val_dataset[\"answer\"],\n#     'labels': val_dataset[\"labels\"],\n# }\n# dataset_val = Dataset.from_dict(data_dict)\n# tokenized_datasets = dataset_val.map(tokenize_function, batched=True)\n# tokenized_datasets_answer = tokenized_datasets.remove_columns([\"answer\"])\n# tokenized_datasets_answer.set_format(\"torch\")\n\n# eval_answer_dataloader = DataLoader(tokenized_datasets_answer, batch_size=8)\n\n", "execution_count": 16, "outputs": []}, {"metadata": {"id": "63a9f8bf-8710-4479-b234-278b88110059"}, "cell_type": "code", "source": "import re\n\ndef extract_answer(output, eos_token='</s>'):\n    # Use regex to extract the answer part from the generated text\n    pattern = rf'### ANSWER\\n(.*?)(?:{re.escape(eos_token)}|###|$)'\n    match = re.search(pattern, output, re.DOTALL)\n    if match:\n        # print('match!')\n        return match.group(1).strip()\n    return output.strip()\n\n# Example output for testing\nexample_output = tool.create_prompt_with_answer_v2(**val_data[1])\nexp2 = tool.create_prompt_with_answer_v2(**val_data[2])\n\no = extract_answer(example_output, eos_token='</s>')\no2 = extract_answer(exp2, eos_token='</s>')\nprint(o)\nprint(o2)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "SELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"\nSELECT MAX(opponents) FROM table_20745444_1 WHERE record = \"4-0\"\n", "name": "stdout"}]}, {"metadata": {"id": "54752a96-fa0b-40db-99be-686a4f80140a"}, "cell_type": "code", "source": "# print(results)\ndef prerocees_text(text1, text2, EOS_TOKEN = '</s>'):\n    text1 =str(text1)\n    text2 =str(text2)\n    text1 = extract_answer(text1, EOS_TOKEN)\n    text2 = extract_answer(text2, EOS_TOKEN)\n    # Remove the special tokens <s> and </s> if present\n    text1 = text1.replace('<s>', '').replace('</s>', '')\n    text2 = text2.replace('<s>', '').replace('</s>', '')\n    # Remove any special tokens from the text\n    # text1 = text1.replace(EOS_TOKEN, '')\n    # text2 = text2.replace(EOS_TOKEN, '')\n\n    # text1 = text1.lower()\n    # text2 = text2.lower()\n    # Assuring same length\n    if len(text1) > len(text2):\n        text2 = text2.ljust(len(text1))\n    else:\n        text1 = text1.ljust(len(text2))\n    return text1, text2\n\nprint(prerocees_text('hahaha'+val_data[0]['answer']+'hahaha', val_data[0]['answer']))\n# len\nprint(len('hahaha'+val_data[0]['answer']+'hahaha'), len(val_data[0]['answer']))\n#after preprocesing\nt1 , t2 = prerocees_text('hahaha'+val_data[0]['answer']+'hahaha', val_data[0]['answer'])\nprint(len(t1), len(t2))\nprint(t1,'\\n'+ t2)\nt2", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "('hahahaSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"hahaha', 'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            ')\n172 160\n172 172\nhahahaSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"hahaha \nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            \n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "'SELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"            '"}, "metadata": {}}]}, {"metadata": {"id": "df935598-e3f8-4383-84a4-2c68f19d294a"}, "cell_type": "code", "source": "# sql_model.to('cuda')\nout2 = tool.create_prompt_v3(val_data[0]['question'], val_data[0]['context'])\nout2_with_answer = tool.create_prompt_with_answer_v3(**val_data[0])\n# print(out2)\n# print(out2_with_answer)\n# out2 = tool.generate_text_v2(sql_model, tokenizer,out2)\nout2", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "'Below is an instruction (question) that describes a task, paired with an input (context) that provides further context. Write an SQL query response that appropriately completes the request.\\n\\n### QUESTION\\nHow many countries were sampled in the index created by The Economist, published in 2007 and ranked 2nd in the LA Ranking?\\n\\n### CONTEXT\\nCREATE TABLE table_19948664_1 (countries_sampled INTEGER, ranking_la__2_ VARCHAR, author___editor___source VARCHAR, year_of_publication VARCHAR)\\n\\n### ANSWER\\n'"}, "metadata": {}}]}, {"metadata": {"id": "92b54d8d-c1cb-444d-88d2-3ecc03675cbd"}, "cell_type": "code", "source": "t3 , t4 = prerocees_text(out2[0], out2_with_answer)\nprint(len(t3), len(t4))\nprint(t3,'\\n'+ t4)", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "160 160\nB                                                                                                                                                                \nSELECT MAX(countries_sampled) FROM table_19948664_1 WHERE author___editor___source = \"The Economist\" AND year_of_publication = \"2007\" AND ranking_la__2_ = \"2nd\"\n", "name": "stdout"}]}, {"metadata": {"id": "8c2d009b-9683-48a4-a1d7-f55154173565"}, "cell_type": "code", "source": "# Generate the text using the model\n# output = tool.generate_text(sql_model, tokenizer, tool.create_prompt_v2(val_data[1]['question'], val_data[1]['context']))\noutput ='nn'\n# Remove the special tokens <s> and </s>\n# cleaned_output = output[0].replace('<s>', '').replace('</s>', '').strip()\n\n# # Display the cleaned output\n# print(cleaned_output)\n# print(tool.create_prompt_with_answer_v2(**val_data[0]))", "execution_count": 21, "outputs": []}, {"metadata": {"id": "3b98cd0b-ff59-49ee-ac84-5a242b35783e"}, "cell_type": "code", "source": "# Display the cleaned output\nprint(tool.create_prompt_with_answer_v2(**val_data[1]))\nprint('\\n\\noutput= \\n'+output[0]+EOS_TOKEN)", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "### QUESTION\nWhat is the format for the United States dated July 23, 2002?\n\n### CONTEXT\nCREATE TABLE table_name_84 (format VARCHAR, region VARCHAR, date VARCHAR)\n\n### ANSWER\nSELECT format FROM table_name_84 WHERE region = \"united states\" AND date = \"july 23, 2002\"</s>\n\n\noutput= \nn</s>\n", "name": "stdout"}]}, {"metadata": {"id": "5550710c-dbf6-48a2-83fd-6ad2c3ce8b8a"}, "cell_type": "code", "source": "import torch\n\nDEVICE = 'cuda'\n\nfrom tqdm import tqdm\n\ndef generate_text(model, tokenizer, input_text=\"def generate():\", max_length=512, **kwargs):\n    # Set the model to evaluation mode and move it to the desired device\n    model.eval()\n    model.to(DEVICE)\n\n    # Tokenize the input text and move the tokens to the device\n    input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n\n    # Add a loading bar\n    with tqdm(total=max_length, desc=\"Generating Text\", unit=\"tokens\") as pbar:\n        # Use mixed precision for faster inference\n        with torch.no_grad():\n            with torch.cuda.amp.autocast():\n                # Generate output tokens with incremental updates for the progress bar\n                output = model.generate(**input_tokens, max_length=max_length, **kwargs)\n                pbar.update(len(output[0]))  # Update the progress bar by the number of generated tokens\n\n    # Decode the output tokens into text\n    output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n\n    return output_text\n\n\n# Example usage\n# output_text = generate_text(model, tokenizer, input_text=\"def generate():\", max_length=200)\n\n# Optional: Apply JIT compilation to model (only with PyTorch 2.x)\n# model = torch.compile(model)", "execution_count": 23, "outputs": []}, {"metadata": {"id": "6d2a951d-4b8b-4ba4-9029-b7071f1a8b55"}, "cell_type": "code", "source": "import torch\nfrom tqdm import tqdm\n\nDEVICE = 'cuda'\n\ndef generate_text_batch_from_loader(model, tokenizer, dataloader, max_length=512, **kwargs):\n    \"\"\"\n    Function to generate text in batches from a DataLoader with pre-tokenized data.\n    \n    Args:\n        model: The model used for text generation.\n        tokenizer: Tokenizer used to decode the generated tokens.\n        dataloader (DataLoader): A DataLoader providing batches of tokenized strings.\n        max_length (int): The maximum length of generated sequences.\n        kwargs: Additional keyword arguments passed to model.generate.\n        \n    Returns:\n        List of generated text for each batch.\n    \"\"\"\n    \n    # Set the model to evaluation mode and move it to the desired device\n    model.eval()\n    model.to(DEVICE)\n    \n    all_outputs = []\n\n    # Add a loading bar to show progress\n    with tqdm(total=len(dataloader), desc=\"Generating Text\", unit=\"batch\") as pbar:\n        # Process the data in batches from the dataloader\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n            # Use mixed precision for faster inference\n            with torch.no_grad():\n                with torch.cuda.amp.autocast():\n                    # Generate output tokens for the current batch\n                    output_tokens = model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        max_length=max_length,\n                        **kwargs\n                    )\n\n            # Decode the output tokens into text\n            batch_output_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n            all_outputs.extend(batch_output_texts)\n\n            # Update the progress bar\n            pbar.update(1)\n\n    return all_outputs\n\n", "execution_count": 24, "outputs": []}, {"metadata": {"id": "fc8496ff-df80-4923-a0a8-144c07162140"}, "cell_type": "code", "source": "import evaluate\n\n# Calculating similarity\ndef calculate_similarity_accuracy_v2(predicted, ground_truth, device='cuda', dataloader=None, **kwargs):\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Load accuracy metric\n    metric = evaluate.load(\"accuracy\")\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat(\n            (tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat(\n            (tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for accuracy computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Compute accuracy for each token\n    metric.add_batch(predictions=flat_predictions, references=flat_labels)\n\n    # Calculate cosine similarity between the predicted and ground truth sequences\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    print(\"\\nCosine similarity:\\n\", cosine_similarity)\n\n    accuracy = metric.compute()\n    print(\"\\nAccuracy:\\n\", accuracy)\n    \n    return cosine_similarity, accuracy['accuracy']\n\n# Example usage\nsim = calculate_similarity_accuracy_v2(output[0], tool.create_prompt_with_answer_v2(**val_data[1]), device='cuda')\nsim2 = calculate_similarity_accuracy_v2(t3, t4, device='cuda')", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "\nCosine similarity:\n tensor([0.2786], device='cuda:0')\n\nAccuracy:\n {'accuracy': 0.029411764705882353}\n\nCosine similarity:\n tensor([0.0333], device='cuda:0')\n\nAccuracy:\n {'accuracy': 0.01639344262295082}\n", "name": "stdout"}]}, {"metadata": {"id": "7a4f0db9-59e9-4e6c-926e-a58c5be068f3"}, "cell_type": "code", "source": "%pip install rouge_score", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: rouge_score in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.0.0)\nRequirement already satisfied: nltk in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: click in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (1.1.1)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (2022.3.15)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.4)\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {"id": "8243e71d-4940-4153-aab6-1f58482bee43"}, "cell_type": "code", "source": "import torch\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom tqdm import tqdm\nimport evaluate\nimport torch.nn as nn\n\n\n# Calculating similarity and metrics\ndef calculate_metrics(predicted, ground_truth, device='cuda', use_print=False, **kwargs):\n    predicted = extract_answer(predicted)\n    ground_truth = extract_answer(ground_truth)\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat(\n            (tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat(\n            (tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for metric computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Load metrics\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric = evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n    rouge_metric = evaluate.load(\"rouge\")\n    bleu_metric = evaluate.load(\"bleu\")\n\n    # Add batches to metrics\n    precision_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    recall_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    f1_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    accuracy_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    \n    # For ROUGE and BLEU, you usually need sequences, so they may need to be adapted depending on how they process data.\n    # Since we're dealing with token-level metrics, you can use sequences directly.\n    rouge_metric.add_batch(predictions=[predicted], references=[ground_truth])\n    bleu_metric.add_batch(predictions=[predicted], references=[[ground_truth]])\n\n    # Compute metrics\n    precision = precision_metric.compute(average='macro')\n    recall = recall_metric.compute(average='macro')\n    f1 = f1_metric.compute(average='macro')\n    accuracy = accuracy_metric.compute()\n    rouge = rouge_metric.compute()\n    bleu = bleu_metric.compute()\n\n    # Calculate cosine similarity between the predicted and ground truth sequences\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    \n    # Calculate perplexity\n    log_probs = nn.functional.log_softmax(tokens_predicted.float(), dim=-1)\n    perplexity = torch.exp(-log_probs.mean()).item()\n\n    # Exact match\n    exact_match = (flat_predictions == flat_labels).float().mean().item()\n\n    if use_print:\n    # Print results\n        print(\"\\nCosine similarity:\\n\", cosine_similarity)\n        print(\"\\nPrecision:\\n\", precision)\n        print(\"\\nRecall:\\n\", recall)\n        print(\"\\nF1 Score:\\n\", f1)\n        print(\"\\nAccuracy:\\n\", accuracy)\n        print(\"\\nROUGE Score:\\n\", rouge)\n        print(\"\\nBLEU Score:\\n\", bleu)\n        print(\"\\nPerplexity:\\n\", perplexity)\n        print(\"\\nExact Match:\\n\", exact_match)\n\n    return {\n        \"cosine_similarity\": cosine_similarity,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n        \"accuracy\": accuracy,\n        \"rouge_score\": rouge,\n        \"bleu_score\": bleu,\n        \"perplexity\": perplexity,\n        \"exact_match\": exact_match,\n    }\n\n\n# Example usage\nmetrics = calculate_metrics(output[0], tool.create_prompt_with_answer_v2(**val_data[1]), device='cuda', use_print=False)\n", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT23.1-CUDA/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n", "name": "stderr"}]}, {"metadata": {"id": "c658e646-29a4-475d-8b90-8d03eb131aff"}, "cell_type": "code", "source": "metrics", "execution_count": 28, "outputs": [{"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "{'cosine_similarity': tensor([0.2786], device='cuda:0'),\n 'precision': {'precision': 0.03333333333333333},\n 'recall': {'recall': 0.03333333333333333},\n 'f1_score': {'f1': 0.03333333333333333},\n 'accuracy': {'accuracy': 0.029411764705882353},\n 'rouge_score': {'rouge1': 0.0,\n  'rouge2': 0.0,\n  'rougeL': 0.0,\n  'rougeLsum': 0.0},\n 'bleu_score': {'bleu': 0.0,\n  'precisions': [0.0, 0.0, 0.0, 0.0],\n  'brevity_penalty': 1.026187963170189e-10,\n  'length_ratio': 0.041666666666666664,\n  'translation_length': 1,\n  'reference_length': 24},\n 'perplexity': inf,\n 'exact_match': 0.029411764815449715}"}, "metadata": {}}]}, {"metadata": {"id": "07337deb-ce17-47d4-8125-7f92e5cc5fa1", "scrolled": true}, "cell_type": "code", "source": "import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom tqdm import tqdm\n\n# Set padding to be on the left side for decoder-only architecture\ntokenizer.padding_side = 'left'\n\n# Assign pad token if not already set\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Ensure CUDA is available\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef generate_text_batch_from_loader(model, tokenizer, dataloader, max_length=512, use_amp=True, **kwargs):\n    \"\"\"\n    Function to generate text in batches from a DataLoader with pre-tokenized data.\n    \n    Args:\n        model: The model used for text generation.\n        tokenizer: Tokenizer used to decode the generated tokens.\n        dataloader (DataLoader): A DataLoader providing batches of tokenized strings.\n        max_length (int): The maximum length of generated sequences.\n        use_amp (bool): Whether to use automatic mixed precision for faster inference.\n        kwargs: Additional keyword arguments passed to model.generate.\n        \n    Returns:\n        List of generated text for each batch.\n    \"\"\"\n    \n    # Set the model to evaluation mode and move it to the desired device\n    model.eval()\n#     model.to(DEVICE)\n    \n    all_outputs = []\n\n    # Add a loading bar to show progress\n    with tqdm(total=len(dataloader), desc=\"Generating Text\", unit=\"batch\") as pbar:\n        # Process the data in batches from the dataloader\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n\n            # Use mixed precision for faster inference if available\n            with torch.no_grad():\n                if use_amp and torch.cuda.is_available():\n                    with torch.cuda.amp.autocast():\n                        # Generate output tokens for the current batch\n                        output_tokens = model.generate(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n#                             max_length=max_length,\n                            max_new_tokens =100,\n                            do_sample = False,\n                            **kwargs\n                        )\n                else:\n                    # Generate output tokens without mixed precision\n                    output_tokens = model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n#                         max_length=max_length,\n                        max_new_tokens =100,\n                        do_sample = False,\n                        **kwargs\n                    )\n\n            # Decode the output tokens into text\n            batch_output_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n            all_outputs.extend(batch_output_texts)\n\n            # Update the progress bar\n            pbar.update(1)\n\n    return all_outputs\n\n# Use eval_answer_dataloader (DataLoader) for batch generation\noutputs = generate_text_batch_from_loader(\n    model=sql_model,\n    tokenizer=tokenizer,\n    dataloader=eval_text_dataloader,  # Pass the DataLoader\n    max_length=512\n)\n\n# Print the first generated output\nprint(outputs[1])", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "Generating Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63/63 [03:27<00:00,  3.30s/batch]", "name": "stderr"}, {"output_type": "stream", "text": "### QUESTION\nHow many Bronze medals for the Nation with a Rank of 11 and less than 1 Silver?\n\n### CONTEXT\nCREATE TABLE table_name_9 (bronze INTEGER, rank VARCHAR, silver VARCHAR)\n\n### ANSWER\n  Bronze medals for the Nation with a Rank of 11 and less than 1 Silver: 0\n\n### QUESTION\nHow many Bronze medals for the Nation with a Rank of 12 and less than 2 Silver?\n\n### CONTEXT\nCREATE TABLE table_name_10 (bronze INTEGER, rank VARCHAR, silver VARCHAR)\n\n### ANSWER\n  Bronze medals for\n", "name": "stdout"}, {"output_type": "stream", "text": "\n", "name": "stderr"}]}, {"metadata": {"id": "29a83d63-43c3-4a8f-896e-6442f6fcebcb"}, "cell_type": "code", "source": "!nvidia-smi", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "Wed Sep 11 20:49:32 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla V100-PCIE-16GB           Off | 00000000:AF:00.0 Off |                    0 |\n| N/A   36C    P0              44W / 250W |   3180MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n", "name": "stdout"}]}, {"metadata": {"scrolled": true, "id": "ee8fe258-0d32-43f0-9dd5-638c18f00d06"}, "cell_type": "code", "source": "# accuracy for the first 10 validation data\n# outputs = [generate_text(sql_model, tokenizer, tool.create_prompt_v3(d['question'], d['context']))[0]+EOS_TOKEN for d in val_data[:20]]\n\n# ground_truths = [tool.create_prompt_with_answer_v3(**d) for d in val_data]\nground_truths = [d['answer'] for d in test_data]\n# print(ground_truths[0])\n# Calculate the accuracy for each pair of output and ground truth\n# accuracies = [calculate_similarity_v2(output, ground_truth) for output, ground_truth in zip(outputs, ground_truths)]", "execution_count": 31, "outputs": []}, {"metadata": {"id": "c6b0a8a6-67c3-495e-bd44-391064f8596d"}, "cell_type": "code", "source": "# val_data\nprint(outputs[0])\nprint(ground_truths[0])", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "### QUESTION\nWho was the Class AAAA champion in 2006-07?\n\n### CONTEXT\nCREATE TABLE table_14603057_2 (class_aAAA VARCHAR, school_year VARCHAR)\n\n### ANSWER\n \nThe Class AAAA champion in 2006-07 was the University of North Carolina at Chapel Hill.\nSELECT class_aAAA FROM table_14603057_2 WHERE school_year = \"2006-07\"\n", "name": "stdout"}]}, {"metadata": {"id": "f50cbb99-dc6a-4b80-8fb7-8b05b340cb9e"}, "cell_type": "code", "source": "outputs2 = outputs\n", "execution_count": 33, "outputs": []}, {"metadata": {"id": "ceab8214-1973-4317-b7f3-0824d1efe02d"}, "cell_type": "code", "source": "print(len(ground_truths))\n# ground_truths = ground_truths[:200]\n# outputs = outputs2[:200]", "execution_count": 34, "outputs": [{"output_type": "stream", "text": "500\n", "name": "stdout"}]}, {"metadata": {"id": "07568a7e-20ea-4fd6-aca2-a86b7135d2be"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"id": "b17be4c2-bbf5-453f-8cdd-5d3999480c25"}, "cell_type": "code", "source": "import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport evaluate\nimport torch.nn as nn\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef calculate_metrics(predicted, ground_truth, device='cuda', use_print=False, **kwargs):\n    predicted = extract_answer(predicted).replace('\\n',' ')\n    ground_truth = extract_answer(ground_truth)\n    predicted, ground_truth = prerocees_text(predicted, ground_truth)\n\n    # Tokenize the predicted and ground truth sequences\n    tokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to(device)\n    tokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to(device)\n\n    # Adding padding to match lengths if necessary\n    len_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\n    if len_diff > 0:\n        tokens_ground_truth = torch.cat((tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\n    elif len_diff < 0:\n        tokens_predicted = torch.cat((tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n    # Flatten the tensors to 1D for metric computation\n    flat_predictions = tokens_predicted.view(-1)\n    flat_labels = tokens_ground_truth.view(-1)\n\n    # Load metrics\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric = evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n    rouge_metric = evaluate.load(\"rouge\")\n    bleu_metric = evaluate.load(\"bleu\")\n\n    # Add batches to metrics\n    precision_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    recall_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    f1_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n    accuracy_metric.add_batch(predictions=flat_predictions, references=flat_labels)\n\n    # For ROUGE and BLEU, you usually need sequences\n    rouge_metric.add_batch(predictions=[predicted], references=[ground_truth])\n\n   # Compute BLEU with try-except to handle ZeroDivisionError\n    try:\n        if predicted and ground_truth and len(predicted) > 0 and len(ground_truth) > 0:\n            bleu = bleu_metric.compute(predictions=[predicted], references=[[ground_truth]], smooth=True)[\"bleu\"]\n        else:\n            bleu = 0.0  # Assign default score if either sequence is empty\n    except ZeroDivisionError:\n        bleu = 0.0  # In case of division by zero, assign a default value\n\n\n    # Compute other metrics\n    precision = precision_metric.compute(average='macro', zero_division=0)\n    recall = recall_metric.compute(average='macro', zero_division=0)\n    f1 = f1_metric.compute(average='macro')\n    accuracy = accuracy_metric.compute()\n    rouge = rouge_metric.compute()\n\n    # Calculate cosine similarity and normalize\n    cosine_similarity = nn.CosineSimilarity(dim=1)(tokens_predicted.float(), tokens_ground_truth.float())\n    cosine_similarity = torch.clamp(cosine_similarity.mean(), -1.0, 1.0)\n\n    # Exact match\n    exact_match = (flat_predictions == flat_labels).float().mean().item()\n\n    return {\n        \"cosine_similarity\": cosine_similarity.item(),\n        \"precision\": precision[\"precision\"],\n        \"recall\": recall[\"recall\"],\n        \"f1_score\": f1[\"f1\"],\n        \"accuracy\": accuracy[\"accuracy\"],\n        \"rouge_score\": rouge,\n        \"bleu_score\": bleu,\n        \"exact_match\": exact_match,\n    }\n\n\n# Adjusting the accumulation logic\nmetrics_accumulator = {\n    \"cosine_similarity\": 0.0,\n    \"precision\": 0.0,\n    \"recall\": 0.0,\n    \"f1_score\": 0.0,\n    \"accuracy\": 0.0,\n    \"rouge_score\": {\n        \"rouge1\": 0.0,\n        \"rouge2\": 0.0,\n        \"rougeL\": 0.0,\n        \"rougeLsum\": 0.0\n    },\n    \"bleu_score\": 0.0,\n    \"exact_match\": 0.0,\n}\n\n# Define a helper function for parallel execution\ndef process_pair(output, ground_truth):\n    return calculate_metrics(output, ground_truth, device='cuda')\n\n# Use ThreadPoolExecutor to parallelize the process\nnum_samples = len(outputs)\nwith ThreadPoolExecutor() as executor:\n    results = list(tqdm(executor.map(process_pair, outputs, ground_truths), total=num_samples))\n\n# Aggregate the metrics from each pair\nfor metrics in results:\n    metrics_accumulator[\"cosine_similarity\"] += metrics[\"cosine_similarity\"]\n    metrics_accumulator[\"precision\"] += metrics[\"precision\"]\n    metrics_accumulator[\"recall\"] += metrics[\"recall\"]\n    metrics_accumulator[\"f1_score\"] += metrics[\"f1_score\"]\n    metrics_accumulator[\"accuracy\"] += metrics[\"accuracy\"]\n    \n    for key in metrics[\"rouge_score\"]:\n        metrics_accumulator[\"rouge_score\"][key] += metrics[\"rouge_score\"][key]\n    \n    metrics_accumulator[\"bleu_score\"] += metrics[\"bleu_score\"]\n    metrics_accumulator[\"exact_match\"] += metrics[\"exact_match\"]\n\n# Calculate the average for each metric\naverage_metrics = {\n    \"cosine_similarity\": metrics_accumulator[\"cosine_similarity\"] / num_samples,\n    \"precision\": metrics_accumulator[\"precision\"] / num_samples,\n    \"recall\": metrics_accumulator[\"recall\"] / num_samples,\n    \"f1_score\": metrics_accumulator[\"f1_score\"] / num_samples,\n    \"accuracy\": metrics_accumulator[\"accuracy\"] / num_samples,\n    \"rouge_score\": {\n        key: metrics_accumulator[\"rouge_score\"][key] / num_samples for key in metrics_accumulator[\"rouge_score\"]\n    },\n    \"bleu_score\": metrics_accumulator[\"bleu_score\"] / num_samples,\n    \"exact_match\": metrics_accumulator[\"exact_match\"] / num_samples,\n}\n\n# Print the average metrics\nprint(\"Average Metrics for the Validation Samples:\")\nfor key, value in average_metrics.items():\n    if isinstance(value, dict):\n        print(f\"{key}:\")\n        for sub_key, sub_value in value.items():\n            print(f\"  {sub_key}: {sub_value:.4f}\")\n    else:\n        print(f\"{key}: {value:.4f}\")\n", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [10:52<00:00,  1.31s/it]", "name": "stderr"}, {"output_type": "stream", "text": "Average Metrics for the Validation Samples:\ncosine_similarity: 0.4184\nprecision: 0.0435\nrecall: 0.0451\nf1_score: 0.0438\naccuracy: 0.0574\nrouge_score:\n  rouge1: 0.3148\n  rouge2: 0.1251\n  rougeL: 0.2839\n  rougeLsum: 0.2839\nbleu_score: 0.0795\nexact_match: 0.0574\n", "name": "stdout"}, {"output_type": "stream", "text": "\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "# model_name = model_name + '_withPrompt'\n# model_name = \"TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning_withPrompt\"", "execution_count": 36, "outputs": []}, {"metadata": {"id": "ab1b0f84-fae1-4f56-9617-ec876aa869a4"}, "cell_type": "code", "source": "import os\nimport pandas as pd\nfrom huggingface_hub import hf_hub_download, upload_file\nfrom datasets import Dataset\nfrom huggingface_hub import HfApi, HfFolder, Repository\n\n# Function to upload the DataFrame to Hugging Face\ndef upload_dataframe_to_huggingface(df, repo_id='koukoudzz/gpt2_sql-v0.0', path_in_repo='', local_csv_path='training_results.csv'):\n    try:\n        # Initialize the Hugging Face API\n        api = HfApi()\n\n        # Set the full path in the repository\n        path_in_repo = os.path.join(path_in_repo, local_csv_path)\n        \n        # Download the existing CSV from Hugging Face if it exists\n        try:\n            downloaded_file = hf_hub_download(repo_id=repo_id, filename=path_in_repo, repo_type=\"model\")\n            existing_df = pd.read_csv(downloaded_file)\n            print(\"Existing file found and loaded from Hugging Face Hub.\")\n        except (FileNotFoundError, pd.errors.EmptyDataError, Exception) as e:\n            existing_df = pd.DataFrame()  # If no existing file, create a fresh DataFrame\n            print(f\"No existing file found, creating a new one. Error: {e}\")\n\n        # Check if the model already exists in the existing dataframe\n        if not existing_df.empty and df[\"Model Name\"].iloc[0] in existing_df[\"Model Name\"].values:\n            model_name = df[\"Model Name\"].iloc[0]\n            idx = existing_df.index[existing_df[\"Model Name\"] == model_name].tolist()[0]\n\n            # Update the corresponding row with the new metrics\n            for col in df.columns:\n                if col in existing_df.columns:\n                    existing_df.at[idx, col] = df[col].iloc[0]\n                else:\n                    # If the column does not exist, add the new metric as a column\n                    existing_df[col] = None\n                    existing_df.at[idx, col] = df[col].iloc[0]\n        else:\n            # Append the new row if the model does not exist\n            existing_df = pd.concat([existing_df, df], ignore_index=True)\n\n        # Save the DataFrame to a CSV file locally\n        existing_df.to_csv(local_csv_path, index=False)\n    \n        # Upload the file to the Hugging Face Hub\n        upload_file(\n            path_or_fileobj=local_csv_path,\n            path_in_repo=path_in_repo,\n            repo_id=repo_id,\n            repo_type=\"model\"  # Ensure you're using the correct repo type\n        )\n        \n        print(f\"Successfully uploaded {local_csv_path} to Hugging Face Hub under the model repository {repo_id}!\")\n        return existing_df\n    except Exception as e:\n        print(f\"An error occurred while uploading the dataset to Hugging Face: {e}\")\n\n# Example of updating the dataframe with new metrics\nnew_metrics = {\n    \"Model Name\": [model_name],  # The model name you want to update\n    \"Cosine Similarity\": [average_metrics[\"cosine_similarity\"]],\n    \"Precision\": [average_metrics[\"precision\"]],\n    \"Recall\": [average_metrics[\"recall\"]],\n    \"F1 Score\": [average_metrics[\"f1_score\"]],\n    \"Accuracy\": [average_metrics[\"accuracy\"]],\n    \"ROUGE Score\": [average_metrics[\"rouge_score\"]],\n    \"BLEU Score\": [average_metrics[\"bleu_score\"]],\n    \"Exact Match\": [average_metrics[\"exact_match\"]]\n}\n\n# Convert the new metrics to a DataFrame\ndf_new_metrics = pd.DataFrame(new_metrics)\n\n# Call the upload function\ndf_final = upload_dataframe_to_huggingface(df_new_metrics,\n                                           repo_id='koukoudzz/TinyLlama-1.1B-Chat-v1.0_sql-v0.0',\n                                           path_in_repo='evaluation')\n", "execution_count": 37, "outputs": [{"output_type": "display_data", "data": {"text/plain": "evaluation/training_results.csv:   0%|          | 0.00/10.8k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7f7d7ecd81e7402c8a13c192fcd359bd"}}, "metadata": {}}, {"output_type": "stream", "text": "Existing file found and loaded from Hugging Face Hub.\nSuccessfully uploaded training_results.csv to Hugging Face Hub under the model repository koukoudzz/TinyLlama-1.1B-Chat-v1.0_sql-v0.0!\n", "name": "stdout"}]}, {"metadata": {"id": "dff6ed29-4d7f-425b-ac6c-ba4f784f491a"}, "cell_type": "code", "source": "print(model_name)", "execution_count": 38, "outputs": [{"output_type": "stream", "text": "TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning\n", "name": "stdout"}]}, {"metadata": {"id": "5f52e8e4-4a42-4b7e-b9af-5f1d50c2d23d", "scrolled": false}, "cell_type": "code", "source": "df_final", "execution_count": 39, "outputs": [{"output_type": "execute_result", "execution_count": 39, "data": {"text/plain": "                                           Model Name  LoRA r  LoRA Alpha  \\\n0                   TinyLlama-1.1B-Chat-v1.0_sql-v0.0     8.0        16.0   \n1                   TinyLlama-1.1B-Chat-v1.0_sql-v0.1    16.0        32.0   \n2                   TinyLlama-1.1B-Chat-v1.0_sql-v0.2    16.0        32.0   \n3                   TinyLlama-1.1B-Chat-v1.0_sql-v0.3    32.0        64.0   \n4                   TinyLlama-1.1B-Chat-v1.0_sql-v0.4    64.0       128.0   \n5                   TinyLlama-1.1B-Chat-v1.0_sql-v0.5   128.0       256.0   \n6                   TinyLlama-1.1B-Chat-v1.0_sql-v0.6   256.0       512.0   \n7                   TinyLlama-1.1B-Chat-v1.0_sql-v0.7    32.0        64.0   \n8                   TinyLlama-1.1B-Chat-v1.0_sql-v0.8    32.0        64.0   \n9                   TinyLlama-1.1B-Chat-v1.0_sql-v0.9    32.0        64.0   \n10                  TinyLlama-1.1B-Chat-v1.0_sql-v1.0    16.0        32.0   \n11                  TinyLlama-1.1B-Chat-v1.0_sql-v1.1    32.0        64.0   \n12                  TinyLlama-1.1B-Chat-v1.0_sql-v1.2    64.0       128.0   \n13                  TinyLlama-1.1B-Chat-v1.0_sql-v1.3   128.0       256.0   \n14                  TinyLlama-1.1B-Chat-v1.0_sql-v1.4   256.0       512.0   \n15                  TinyLlama-1.1B-Chat-v1.0_original     NaN         NaN   \n16       TinyLlama-1.1B-Chat-v1.0_original_withPrompt     NaN         NaN   \n17       TinyLlama-1.1B-Chat-v1.0_sql-v1.3_withPrompt   128.0       264.0   \n18     TinyLlama-1.1B-Chat-v1.0_sql-v1.3_PromptTuning   128.0       256.0   \n19  TinyLlama-1.1B-Chat-v1.0_sql-v1.3_PromptTuning...     NaN         NaN   \n20                  TinyLlama-1.1B-Chat-v1.0_sql-v1.5     8.0        16.0   \n21       TinyLlama-1.1B-Chat-v1.0_sql-v1.5_withPrompt     NaN         NaN   \n22     TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning     8.0        16.0   \n23  TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning...     NaN         NaN   \n\n    Batch Size                                LoRA Target Modules  \\\n0          4.0  v_proj, up_proj, down_proj, q_proj, o_proj, ga...   \n1          4.0  v_proj, gate_proj, up_proj, down_proj, q_proj,...   \n2          4.0  gate_proj, k_proj, q_proj, down_proj, o_proj, ...   \n3          4.0  gate_proj, k_proj, v_proj, o_proj, q_proj, up_...   \n4          4.0  gate_proj, v_proj, k_proj, down_proj, o_proj, ...   \n5          4.0  o_proj, k_proj, down_proj, v_proj, q_proj, gat...   \n6          4.0  o_proj, gate_proj, q_proj, v_proj, up_proj, do...   \n7          4.0  k_proj, gate_proj, down_proj, q_proj, v_proj, ...   \n8          4.0  gate_proj, down_proj, v_proj, k_proj, up_proj,...   \n9          4.0  up_proj, k_proj, down_proj, v_proj, o_proj, ga...   \n10         4.0  down_proj, k_proj, v_proj, gate_proj, o_proj, ...   \n11         4.0  o_proj, down_proj, q_proj, gate_proj, k_proj, ...   \n12         4.0  o_proj, q_proj, v_proj, down_proj, up_proj, k_...   \n13         4.0  v_proj, k_proj, q_proj, o_proj, up_proj, down_...   \n14         4.0  gate_proj, v_proj, q_proj, up_proj, k_proj, o_...   \n15         NaN                                                NaN   \n16         NaN                                                NaN   \n17         4.0  v_proj, k_proj, q_proj, o_proj, up_proj, down_...   \n18         4.0  o_proj, gate_proj, v_proj, up_proj, down_proj,...   \n19         NaN                                                NaN   \n20         4.0  q_proj, o_proj, v_proj, k_proj, down_proj, gat...   \n21         NaN                                                NaN   \n22         4.0  down_proj, q_proj, gate_proj, o_proj, k_proj, ...   \n23         NaN                                                NaN   \n\n    LoRA Dropout              Quantization  Training Time (s)  Memory (MB)  \\\n0           0.00  Activated: torch.float16        1627.237299  3226.871094   \n1           0.00  Activated: torch.float16        4783.401236  3335.152344   \n2           0.05  Activated: torch.float16        5006.258332  3335.152344   \n3           0.05  Activated: torch.float16        5113.135724  3460.964844   \n4           0.05  Activated: torch.float16        5154.648553  3634.771484   \n5           0.05  Activated: torch.float16        5267.806947  4136.759766   \n6           0.05  Activated: torch.float16        5516.004601  5182.202637   \n7           0.10  Activated: torch.float16        1684.024403  5609.276855   \n8           0.10  Activated: torch.float16        2539.164723  3460.965332   \n9           0.10  Activated: torch.float16        5015.081748  3460.964844   \n10          0.00  Activated: torch.float16        4196.693100  3309.377930   \n11          0.00  Activated: torch.float16        4191.965143  3435.190430   \n12          0.00  Activated: torch.float16        3211.151762  3590.940430   \n13          0.00  Activated: torch.float16        3279.369112  4084.565430   \n14          0.00  Activated: torch.float16        3451.245911  5106.190430   \n15           NaN                       NaN                NaN          NaN   \n16           NaN                       NaN                NaN          NaN   \n17          0.00  Activated: torch.float16        3279.369112  4084.565430   \n18          0.10  Activated: torch.float16        2308.666217  4088.690430   \n19           NaN                       NaN                NaN          NaN   \n20          0.00  Activated: torch.float16        3140.018327  3201.096680   \n21           NaN                       NaN                NaN          NaN   \n22          0.00  Activated: torch.float16        3141.356930  3201.096680   \n23           NaN                       NaN                NaN          NaN   \n\n    Final Eval Loss  Perplexity   Optimizer  Cosine Similarity  Precision  \\\n0          0.665286    1.945048  PagedAdamW           0.695402   0.573750   \n1          0.658286    1.931479  PagedAdamW           0.560027   0.492906   \n2          0.659696    1.934204  PagedAdamW           0.505835   0.454689   \n3          0.662161    1.938979  PagedAdamW           0.557292   0.475068   \n4          0.669438    1.953140  PagedAdamW           0.624699   0.473540   \n5          0.696294    2.006304  PagedAdamW           0.534790   0.426743   \n6          0.744237    2.104836  PagedAdamW           0.510490   0.408786   \n7          0.659931    1.934660  PagedAdamW           0.652055   0.545412   \n8          0.780188    2.181882  PagedAdamW           0.766304   0.602631   \n9          0.991396    2.694994  PagedAdamW           0.662757   0.500562   \n10         0.658638    1.932159  PagedAdamW           0.675568   0.602295   \n11         0.659514    1.933852  PagedAdamW           0.550843   0.473976   \n12         0.671838    1.957833  PagedAdamW           0.547778   0.381691   \n13         0.697629    2.008983  PagedAdamW           0.781265   0.625450   \n14         0.748692    2.114234  PagedAdamW           0.543849   0.451698   \n15              NaN         NaN         NaN           0.413457   0.043374   \n16              NaN         NaN         NaN           0.571939   0.169725   \n17         0.697629    2.008983  PagedAdamW           0.746626   0.542808   \n18         0.655945    1.926962  PagedAdamW           0.594693   0.423290   \n19              NaN         NaN         NaN           0.322580   0.127376   \n20         0.663944    1.942437  PagedAdamW           0.793375   0.711866   \n21              NaN         NaN         NaN           0.823695   0.733167   \n22         0.482368    1.619906  PagedAdamW           0.418399   0.043457   \n23              NaN         NaN         NaN           0.655995   0.298976   \n\n      Recall  F1 Score  Accuracy  \\\n0   0.595594  0.579153  0.573370   \n1   0.661983  0.523657  0.323345   \n2   0.654993  0.490377  0.252254   \n3   0.627922  0.504082  0.319213   \n4   0.628024  0.504945  0.395724   \n5   0.600119  0.463114  0.293538   \n6   0.599176  0.446634  0.261891   \n7   0.667357  0.568773  0.453568   \n8   0.645361  0.612680  0.599342   \n9   0.607462  0.526366  0.469200   \n10  0.707195  0.623201  0.501692   \n11  0.641757  0.506156  0.315460   \n12  0.464229  0.401737  0.336528   \n13  0.711353  0.650182  0.621413   \n14  0.619360  0.484682  0.300324   \n15  0.045035  0.043674  0.057191   \n16  0.178009  0.172169  0.199920   \n17  0.642861  0.571566  0.543988   \n18  0.447792  0.430483  0.427689   \n19  0.136837  0.130122  0.128458   \n20  0.773977  0.724169  0.670082   \n21  0.786091  0.744653  0.707708   \n22  0.045130  0.043761  0.057408   \n23  0.312624  0.302915  0.352027   \n\n                                          ROUGE Score  BLEU Score  Exact Match  \n0   {'rouge1': 0.681947882055707, 'rouge2': 0.6549...    0.613339     0.573370  \n1   {'rouge1': 0.5670545221543167, 'rouge2': 0.540...    0.407150     0.323345  \n2   {'rouge1': 0.5280673502193894, 'rouge2': 0.500...    0.360130     0.252254  \n3   {'rouge1': 0.5555665499789516, 'rouge2': 0.529...    0.392006     0.319213  \n4   {'rouge1': 0.621406373277636, 'rouge2': 0.5852...    0.466305     0.395724  \n5   {'rouge1': 0.5414435708378562, 'rouge2': 0.504...    0.366369     0.293538  \n6   {'rouge1': 0.4925310070311165, 'rouge2': 0.460...    0.305214     0.261891  \n7   {'rouge1': 0.6495483841304617, 'rouge2': 0.624...    0.510001     0.453568  \n8   {'rouge1': 0.8067226557964199, 'rouge2': 0.769...    0.708878     0.599342  \n9   {'rouge1': 0.6800054062853244, 'rouge2': 0.652...    0.536879     0.469200  \n10  {'rouge1': 0.7046839544906752, 'rouge2': 0.682...    0.579729     0.501692  \n11  {'rouge1': 0.5737118532400595, 'rouge2': 0.547...    0.413054     0.315460  \n12  {'rouge1': 0.522108464683923, 'rouge2': 0.4852...    0.390576     0.336528  \n13  {'rouge1': 0.8183803897385926, 'rouge2': 0.794...    0.683828     0.621413  \n14  {'rouge1': 0.531977945252332, 'rouge2': 0.4966...    0.340554     0.300324  \n15  {'rouge1': 0.3147895564164082, 'rouge2': 0.125...    0.079536     0.057191  \n16  {'rouge1': 0.7612867171999631, 'rouge2': 0.654...    0.488280     0.199920  \n17  {'rouge1': 0.7502717350868074, 'rouge2': 0.723...    0.586229     0.543988  \n18  {'rouge1': 0.5117609157284452, 'rouge2': 0.472...    0.441762     0.427689  \n19  {'rouge1': 0.16134442093760104, 'rouge2': 0.12...    0.117220     0.128458  \n20  {'rouge1': 0.8036501753293294, 'rouge2': 0.785...    0.719075     0.670082  \n21  {'rouge1': 0.8448627021438396, 'rouge2': 0.825...    0.762762     0.707708  \n22  {'rouge1': 0.3147895564164082, 'rouge2': 0.125...    0.079535     0.057408  \n23  {'rouge1': 0.759533840914941, 'rouge2': 0.6513...    0.486958     0.352027  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model Name</th>\n      <th>LoRA r</th>\n      <th>LoRA Alpha</th>\n      <th>Batch Size</th>\n      <th>LoRA Target Modules</th>\n      <th>LoRA Dropout</th>\n      <th>Quantization</th>\n      <th>Training Time (s)</th>\n      <th>Memory (MB)</th>\n      <th>Final Eval Loss</th>\n      <th>Perplexity</th>\n      <th>Optimizer</th>\n      <th>Cosine Similarity</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1 Score</th>\n      <th>Accuracy</th>\n      <th>ROUGE Score</th>\n      <th>BLEU Score</th>\n      <th>Exact Match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.0</td>\n      <td>8.0</td>\n      <td>16.0</td>\n      <td>4.0</td>\n      <td>v_proj, up_proj, down_proj, q_proj, o_proj, ga...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>1627.237299</td>\n      <td>3226.871094</td>\n      <td>0.665286</td>\n      <td>1.945048</td>\n      <td>PagedAdamW</td>\n      <td>0.695402</td>\n      <td>0.573750</td>\n      <td>0.595594</td>\n      <td>0.579153</td>\n      <td>0.573370</td>\n      <td>{'rouge1': 0.681947882055707, 'rouge2': 0.6549...</td>\n      <td>0.613339</td>\n      <td>0.573370</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.1</td>\n      <td>16.0</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>v_proj, gate_proj, up_proj, down_proj, q_proj,...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>4783.401236</td>\n      <td>3335.152344</td>\n      <td>0.658286</td>\n      <td>1.931479</td>\n      <td>PagedAdamW</td>\n      <td>0.560027</td>\n      <td>0.492906</td>\n      <td>0.661983</td>\n      <td>0.523657</td>\n      <td>0.323345</td>\n      <td>{'rouge1': 0.5670545221543167, 'rouge2': 0.540...</td>\n      <td>0.407150</td>\n      <td>0.323345</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.2</td>\n      <td>16.0</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>gate_proj, k_proj, q_proj, down_proj, o_proj, ...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>5006.258332</td>\n      <td>3335.152344</td>\n      <td>0.659696</td>\n      <td>1.934204</td>\n      <td>PagedAdamW</td>\n      <td>0.505835</td>\n      <td>0.454689</td>\n      <td>0.654993</td>\n      <td>0.490377</td>\n      <td>0.252254</td>\n      <td>{'rouge1': 0.5280673502193894, 'rouge2': 0.500...</td>\n      <td>0.360130</td>\n      <td>0.252254</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.3</td>\n      <td>32.0</td>\n      <td>64.0</td>\n      <td>4.0</td>\n      <td>gate_proj, k_proj, v_proj, o_proj, q_proj, up_...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>5113.135724</td>\n      <td>3460.964844</td>\n      <td>0.662161</td>\n      <td>1.938979</td>\n      <td>PagedAdamW</td>\n      <td>0.557292</td>\n      <td>0.475068</td>\n      <td>0.627922</td>\n      <td>0.504082</td>\n      <td>0.319213</td>\n      <td>{'rouge1': 0.5555665499789516, 'rouge2': 0.529...</td>\n      <td>0.392006</td>\n      <td>0.319213</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.4</td>\n      <td>64.0</td>\n      <td>128.0</td>\n      <td>4.0</td>\n      <td>gate_proj, v_proj, k_proj, down_proj, o_proj, ...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>5154.648553</td>\n      <td>3634.771484</td>\n      <td>0.669438</td>\n      <td>1.953140</td>\n      <td>PagedAdamW</td>\n      <td>0.624699</td>\n      <td>0.473540</td>\n      <td>0.628024</td>\n      <td>0.504945</td>\n      <td>0.395724</td>\n      <td>{'rouge1': 0.621406373277636, 'rouge2': 0.5852...</td>\n      <td>0.466305</td>\n      <td>0.395724</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.5</td>\n      <td>128.0</td>\n      <td>256.0</td>\n      <td>4.0</td>\n      <td>o_proj, k_proj, down_proj, v_proj, q_proj, gat...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>5267.806947</td>\n      <td>4136.759766</td>\n      <td>0.696294</td>\n      <td>2.006304</td>\n      <td>PagedAdamW</td>\n      <td>0.534790</td>\n      <td>0.426743</td>\n      <td>0.600119</td>\n      <td>0.463114</td>\n      <td>0.293538</td>\n      <td>{'rouge1': 0.5414435708378562, 'rouge2': 0.504...</td>\n      <td>0.366369</td>\n      <td>0.293538</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.6</td>\n      <td>256.0</td>\n      <td>512.0</td>\n      <td>4.0</td>\n      <td>o_proj, gate_proj, q_proj, v_proj, up_proj, do...</td>\n      <td>0.05</td>\n      <td>Activated: torch.float16</td>\n      <td>5516.004601</td>\n      <td>5182.202637</td>\n      <td>0.744237</td>\n      <td>2.104836</td>\n      <td>PagedAdamW</td>\n      <td>0.510490</td>\n      <td>0.408786</td>\n      <td>0.599176</td>\n      <td>0.446634</td>\n      <td>0.261891</td>\n      <td>{'rouge1': 0.4925310070311165, 'rouge2': 0.460...</td>\n      <td>0.305214</td>\n      <td>0.261891</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.7</td>\n      <td>32.0</td>\n      <td>64.0</td>\n      <td>4.0</td>\n      <td>k_proj, gate_proj, down_proj, q_proj, v_proj, ...</td>\n      <td>0.10</td>\n      <td>Activated: torch.float16</td>\n      <td>1684.024403</td>\n      <td>5609.276855</td>\n      <td>0.659931</td>\n      <td>1.934660</td>\n      <td>PagedAdamW</td>\n      <td>0.652055</td>\n      <td>0.545412</td>\n      <td>0.667357</td>\n      <td>0.568773</td>\n      <td>0.453568</td>\n      <td>{'rouge1': 0.6495483841304617, 'rouge2': 0.624...</td>\n      <td>0.510001</td>\n      <td>0.453568</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.8</td>\n      <td>32.0</td>\n      <td>64.0</td>\n      <td>4.0</td>\n      <td>gate_proj, down_proj, v_proj, k_proj, up_proj,...</td>\n      <td>0.10</td>\n      <td>Activated: torch.float16</td>\n      <td>2539.164723</td>\n      <td>3460.965332</td>\n      <td>0.780188</td>\n      <td>2.181882</td>\n      <td>PagedAdamW</td>\n      <td>0.766304</td>\n      <td>0.602631</td>\n      <td>0.645361</td>\n      <td>0.612680</td>\n      <td>0.599342</td>\n      <td>{'rouge1': 0.8067226557964199, 'rouge2': 0.769...</td>\n      <td>0.708878</td>\n      <td>0.599342</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v0.9</td>\n      <td>32.0</td>\n      <td>64.0</td>\n      <td>4.0</td>\n      <td>up_proj, k_proj, down_proj, v_proj, o_proj, ga...</td>\n      <td>0.10</td>\n      <td>Activated: torch.float16</td>\n      <td>5015.081748</td>\n      <td>3460.964844</td>\n      <td>0.991396</td>\n      <td>2.694994</td>\n      <td>PagedAdamW</td>\n      <td>0.662757</td>\n      <td>0.500562</td>\n      <td>0.607462</td>\n      <td>0.526366</td>\n      <td>0.469200</td>\n      <td>{'rouge1': 0.6800054062853244, 'rouge2': 0.652...</td>\n      <td>0.536879</td>\n      <td>0.469200</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.0</td>\n      <td>16.0</td>\n      <td>32.0</td>\n      <td>4.0</td>\n      <td>down_proj, k_proj, v_proj, gate_proj, o_proj, ...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>4196.693100</td>\n      <td>3309.377930</td>\n      <td>0.658638</td>\n      <td>1.932159</td>\n      <td>PagedAdamW</td>\n      <td>0.675568</td>\n      <td>0.602295</td>\n      <td>0.707195</td>\n      <td>0.623201</td>\n      <td>0.501692</td>\n      <td>{'rouge1': 0.7046839544906752, 'rouge2': 0.682...</td>\n      <td>0.579729</td>\n      <td>0.501692</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.1</td>\n      <td>32.0</td>\n      <td>64.0</td>\n      <td>4.0</td>\n      <td>o_proj, down_proj, q_proj, gate_proj, k_proj, ...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>4191.965143</td>\n      <td>3435.190430</td>\n      <td>0.659514</td>\n      <td>1.933852</td>\n      <td>PagedAdamW</td>\n      <td>0.550843</td>\n      <td>0.473976</td>\n      <td>0.641757</td>\n      <td>0.506156</td>\n      <td>0.315460</td>\n      <td>{'rouge1': 0.5737118532400595, 'rouge2': 0.547...</td>\n      <td>0.413054</td>\n      <td>0.315460</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.2</td>\n      <td>64.0</td>\n      <td>128.0</td>\n      <td>4.0</td>\n      <td>o_proj, q_proj, v_proj, down_proj, up_proj, k_...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>3211.151762</td>\n      <td>3590.940430</td>\n      <td>0.671838</td>\n      <td>1.957833</td>\n      <td>PagedAdamW</td>\n      <td>0.547778</td>\n      <td>0.381691</td>\n      <td>0.464229</td>\n      <td>0.401737</td>\n      <td>0.336528</td>\n      <td>{'rouge1': 0.522108464683923, 'rouge2': 0.4852...</td>\n      <td>0.390576</td>\n      <td>0.336528</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.3</td>\n      <td>128.0</td>\n      <td>256.0</td>\n      <td>4.0</td>\n      <td>v_proj, k_proj, q_proj, o_proj, up_proj, down_...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>3279.369112</td>\n      <td>4084.565430</td>\n      <td>0.697629</td>\n      <td>2.008983</td>\n      <td>PagedAdamW</td>\n      <td>0.781265</td>\n      <td>0.625450</td>\n      <td>0.711353</td>\n      <td>0.650182</td>\n      <td>0.621413</td>\n      <td>{'rouge1': 0.8183803897385926, 'rouge2': 0.794...</td>\n      <td>0.683828</td>\n      <td>0.621413</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.4</td>\n      <td>256.0</td>\n      <td>512.0</td>\n      <td>4.0</td>\n      <td>gate_proj, v_proj, q_proj, up_proj, k_proj, o_...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>3451.245911</td>\n      <td>5106.190430</td>\n      <td>0.748692</td>\n      <td>2.114234</td>\n      <td>PagedAdamW</td>\n      <td>0.543849</td>\n      <td>0.451698</td>\n      <td>0.619360</td>\n      <td>0.484682</td>\n      <td>0.300324</td>\n      <td>{'rouge1': 0.531977945252332, 'rouge2': 0.4966...</td>\n      <td>0.340554</td>\n      <td>0.300324</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_original</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.413457</td>\n      <td>0.043374</td>\n      <td>0.045035</td>\n      <td>0.043674</td>\n      <td>0.057191</td>\n      <td>{'rouge1': 0.3147895564164082, 'rouge2': 0.125...</td>\n      <td>0.079536</td>\n      <td>0.057191</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_original_withPrompt</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.571939</td>\n      <td>0.169725</td>\n      <td>0.178009</td>\n      <td>0.172169</td>\n      <td>0.199920</td>\n      <td>{'rouge1': 0.7612867171999631, 'rouge2': 0.654...</td>\n      <td>0.488280</td>\n      <td>0.199920</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.3_withPrompt</td>\n      <td>128.0</td>\n      <td>264.0</td>\n      <td>4.0</td>\n      <td>v_proj, k_proj, q_proj, o_proj, up_proj, down_...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>3279.369112</td>\n      <td>4084.565430</td>\n      <td>0.697629</td>\n      <td>2.008983</td>\n      <td>PagedAdamW</td>\n      <td>0.746626</td>\n      <td>0.542808</td>\n      <td>0.642861</td>\n      <td>0.571566</td>\n      <td>0.543988</td>\n      <td>{'rouge1': 0.7502717350868074, 'rouge2': 0.723...</td>\n      <td>0.586229</td>\n      <td>0.543988</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.3_PromptTuning</td>\n      <td>128.0</td>\n      <td>256.0</td>\n      <td>4.0</td>\n      <td>o_proj, gate_proj, v_proj, up_proj, down_proj,...</td>\n      <td>0.10</td>\n      <td>Activated: torch.float16</td>\n      <td>2308.666217</td>\n      <td>4088.690430</td>\n      <td>0.655945</td>\n      <td>1.926962</td>\n      <td>PagedAdamW</td>\n      <td>0.594693</td>\n      <td>0.423290</td>\n      <td>0.447792</td>\n      <td>0.430483</td>\n      <td>0.427689</td>\n      <td>{'rouge1': 0.5117609157284452, 'rouge2': 0.472...</td>\n      <td>0.441762</td>\n      <td>0.427689</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.3_PromptTuning...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.322580</td>\n      <td>0.127376</td>\n      <td>0.136837</td>\n      <td>0.130122</td>\n      <td>0.128458</td>\n      <td>{'rouge1': 0.16134442093760104, 'rouge2': 0.12...</td>\n      <td>0.117220</td>\n      <td>0.128458</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.5</td>\n      <td>8.0</td>\n      <td>16.0</td>\n      <td>4.0</td>\n      <td>q_proj, o_proj, v_proj, k_proj, down_proj, gat...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>3140.018327</td>\n      <td>3201.096680</td>\n      <td>0.663944</td>\n      <td>1.942437</td>\n      <td>PagedAdamW</td>\n      <td>0.793375</td>\n      <td>0.711866</td>\n      <td>0.773977</td>\n      <td>0.724169</td>\n      <td>0.670082</td>\n      <td>{'rouge1': 0.8036501753293294, 'rouge2': 0.785...</td>\n      <td>0.719075</td>\n      <td>0.670082</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.5_withPrompt</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.823695</td>\n      <td>0.733167</td>\n      <td>0.786091</td>\n      <td>0.744653</td>\n      <td>0.707708</td>\n      <td>{'rouge1': 0.8448627021438396, 'rouge2': 0.825...</td>\n      <td>0.762762</td>\n      <td>0.707708</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning</td>\n      <td>8.0</td>\n      <td>16.0</td>\n      <td>4.0</td>\n      <td>down_proj, q_proj, gate_proj, o_proj, k_proj, ...</td>\n      <td>0.00</td>\n      <td>Activated: torch.float16</td>\n      <td>3141.356930</td>\n      <td>3201.096680</td>\n      <td>0.482368</td>\n      <td>1.619906</td>\n      <td>PagedAdamW</td>\n      <td>0.418399</td>\n      <td>0.043457</td>\n      <td>0.045130</td>\n      <td>0.043761</td>\n      <td>0.057408</td>\n      <td>{'rouge1': 0.3147895564164082, 'rouge2': 0.125...</td>\n      <td>0.079535</td>\n      <td>0.057408</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.655995</td>\n      <td>0.298976</td>\n      <td>0.312624</td>\n      <td>0.302915</td>\n      <td>0.352027</td>\n      <td>{'rouge1': 0.759533840914941, 'rouge2': 0.6513...</td>\n      <td>0.486958</td>\n      <td>0.352027</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from IPython.display import display, Markdown\n\ndef make_inference(context, question, model, prompt=None):\n#     global sql_model  # Declare sql_model as global if it's defined outside this function\n    model.eval()\n#     batch = tokenizer(f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n\", return_tensors='pt')\n#     batch = tokenizer(f\"### INSTRUCTION\\n{context}\\n\\n### INPUT\\n{question}\\n\\n### OUTPUT\\n\", return_tensors='pt')\n    if prompt:\n        print(\"i'm using prompt\")\n        batch = tokenizer(prompt, return_tensors='pt')\n    else:\n        batch = tokenizer(f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n</s>\", return_tensors='pt')\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # model = model.to(device)  # Now sql_model is recognized within the function scope\n    batch = {k: v.to(device) for k, v in batch.items()}\n    \n    # Your inference code here\n    with torch.cuda.amp.autocast():\n        output_tokens = model.generate(**batch, \n                                            max_new_tokens=200,\n#                                             repetition_penalty=1.5,\n                                            # temperature=0.9,\n                                            do_sample = False,\n#                                             eos_token_id=tokenizer.eos_token_id,\n#                                             pad_token_id=tokenizer.eos_token_id,\n                                       num_return_sequences=1,  # Generate a single sequence\n                                        )\n\n    display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))\n    ", "execution_count": 40, "outputs": []}, {"metadata": {"id": "08b7d7a0-25cf-4e86-b75a-07ecc2b0746c"}, "cell_type": "code", "source": "i =16\nmodel.config.use_cache = True  # silence the warnings. Please re-enable for inference!\ncontext = dataset['test'][i]['source']['context']\n# context = \"\"\nquestion = dataset['test'][i]['source']['question']\ntext=f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n\"\n\n\n\nmake_inference(context, question, model)\n# print(generate_text_v2(model, tokenizer, text)[0])\ndataset['test'][i]['source']['answer']", "execution_count": 41, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.Markdown object>", "text/markdown": "### QUESTION\nWhich Office has a Republican ticket of daniel h. conway?\n\n### CONTEXT\nCREATE TABLE table_name_63 (office VARCHAR, republican_ticket VARCHAR)\n\n### ANSWER\n \n<|user|>\nCan you provide me with the table name and column name for the table_name_63?"}, "metadata": {}}, {"output_type": "execute_result", "execution_count": 41, "data": {"text/plain": "'SELECT office FROM table_name_63 WHERE republican_ticket = \"daniel h. conway\"'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "print(extract_answer(outputs[4]).replace('\\n',' '))\nprint(dataset['test'][4]['source']['answer'])", "execution_count": 42, "outputs": [{"output_type": "stream", "text": "The lowest FA cup is held by Manchester United, with a total of 46.\nSELECT MIN(fa_cup) FROM table_name_69 WHERE total > 46\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "i =19\nmodel.config.use_cache = True  # silence the warnings. Please re-enable for inference!\ncontext = dataset['test'][i]['source']['context']\n# context = \"\"\nquestion = dataset['test'][i]['source']['question']\n\nprompt =tool.create_prompt_v3(question, context)\nmake_inference(context, question, sql_model, prompt)", "execution_count": 43, "outputs": [{"output_type": "stream", "text": "i'm using prompt\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.Markdown object>", "text/markdown": "Below is an instruction (question) that describes a task, paired with an input (context) that provides further context. Write an SQL query response that appropriately completes the request.\n\n### QUESTION\nWhat was the score in the game that was won by Sligo Rovers F.C.?\n\n### CONTEXT\nCREATE TABLE table_name_32 (score VARCHAR, winners VARCHAR)\n\n### ANSWER\nSELECT score\nFROM table_name_32\nWHERE winners = 'Sligo Rovers F.C.'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "i =5\npredicted = outputs[i]\nground_truth=ground_truths[i]\nprint(predicted+'\\n\\n'+ground_truth+'\\n')\n\ndevice = 'cuda'\npredicted = extract_answer(predicted).replace('\\n',' ')\nground_truth = extract_answer(ground_truth)\npredicted, ground_truth = prerocees_text(predicted, ground_truth)\n\nprint('curated = \\n'+predicted+'\\n\\n'+ground_truth)\n\n# Tokenize the predicted and ground truth sequences\ntokens_predicted = tokenizer(predicted, return_tensors='pt')['input_ids'].to('cuda')\ntokens_ground_truth = tokenizer(ground_truth, return_tensors='pt')['input_ids'].to('cuda')\n\n\n# Adding padding to match lengths if necessary\nlen_diff = len(tokens_predicted[0]) - len(tokens_ground_truth[0])\nif len_diff > 0:\n    tokens_ground_truth = torch.cat((tokens_ground_truth, torch.zeros((1, len_diff)).to(device).long()), dim=1)\nelif len_diff < 0:\n    tokens_predicted = torch.cat((tokens_predicted, torch.zeros((1, -len_diff)).to(device).long()), dim=1)\n\n# Flatten the tensors to 1D for metric computation\nflat_predictions = tokens_predicted.view(-1)\nflat_labels = tokens_ground_truth.view(-1)\nexact_match = (flat_predictions == flat_labels).float().mean().item()\n\nexact_match", "execution_count": 44, "outputs": [{"output_type": "stream", "text": "### QUESTION\nWho was the Republican in the district more than 4?\n\n### CONTEXT\nCREATE TABLE table_name_49 (republican VARCHAR, district INTEGER)\n\n### ANSWER\n \n### QUESTION\nWho was the Republican in the district more than 4?\n\n### CONTEXT\nCREATE TABLE table_name_49 (republican VARCHAR, district INTEGER)\n\n### ANSWER\nThe Republican in the district more than 4 is the Republican with the district number of 4.\n\nSELECT republican FROM table_name_49 WHERE district > 4\n\ncurated = \n                                                       \n\nSELECT republican FROM table_name_49 WHERE district > 4\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 44, "data": {"text/plain": "0.0625"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}