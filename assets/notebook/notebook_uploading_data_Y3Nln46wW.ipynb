{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "%pip install python-dotenv # Install the missing module 'dotenv'\n%pip install datasets", "metadata": {"id": "cc2f2a36-d378-4f94-a2d3-f1f05e7de826"}, "outputs": [{"name": "stdout", "text": "Requirement already satisfied: python-dotenv in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (1.0.1)\nNote: you may need to restart the kernel to use updated packages.\nCollecting datasets\n  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (15.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (2.32.2)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (4.66.4)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (3.9.5)\nCollecting huggingface-hub>=0.21.2 (from datasets)\n  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.7\n    Uninstalling dill-0.3.7:\n      Successfully uninstalled dill-0.3.7\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\nSuccessfully installed datasets-2.21.0 dill-0.3.8 huggingface-hub-0.24.6 multiprocess-0.70.16 xxhash-3.5.0\nNote: you may need to restart the kernel to use updated packages.\n", "output_type": "stream"}], "execution_count": 1}, {"cell_type": "code", "source": "import os\nfrom dotenv import load_dotenv\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom ibm_watson_studio_lib import access_project_or_space\n\nwslib = access_project_or_space({\n        'token': 'p-2+kQvRm/s+j4gr3kN/L8nE9w==;jo3EuWo2iO+1+F0egvXSew==:ibH3fFKnohJx8UOS9q9Vf+Ewhuye4ERRlqD3CqbrUpUQvU3alsEPblep77PGbqbFrl+zlVsvRbeoVTX4TtP803SeBdM9yhyA2Q==',\n        'project_id': '512eaa42-cac0-46c4-a80c-d138ac7ccccc'\n})\n\nwslib.download_file('config.env')\nload_dotenv('config.env')\n\n# Connection variables\napi_key = os.getenv(\"API_KEY\", None)\nibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None) \nproject_id = os.getenv(\"PROJECT_ID\", None)\ncreds = {\n    \"url\": ibm_cloud_url,\n    \"apikey\": api_key \n}\naccess_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n).get_token()", "metadata": {"id": "0b23e777-0f07-46a0-ac0c-60fdcdda24f5"}, "outputs": [], "execution_count": 2}, {"cell_type": "code", "source": "# 2- Load the dataset:\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"OussamaAzz/sql_dataset_cleaned\")\n# pd_dataset = pd.DataFrame(dataset)\n# pd_dataset.head()\ndataset,'\\n',dataset['train'][0]", "metadata": {"id": "b173e19f-662f-4457-99e3-726a16321280"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading readme:   0%|          | 0.00/538 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f148bf44da8140a5a74fad9f49a77768"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/2.03M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a104b3ec790649cc817199187fbce78c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/203k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "754aa363d5a34a82855e8f9beb69cb2d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating train split:   0%|          | 0/8244 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ac06c538200944c7b1746d9883481280"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating validation split:   0%|          | 0/825 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2108b7108a244b3295258b5a50358f0f"}}, "metadata": {}}, {"execution_count": 3, "output_type": "execute_result", "data": {"text/plain": "(DatasetDict({\n     train: Dataset({\n         features: ['text', 'source'],\n         num_rows: 8244\n     })\n     validation: Dataset({\n         features: ['text', 'source'],\n         num_rows: 825\n     })\n }),\n '\\n',\n {'text': '### QUESTION\\nwhere pageant is elite model look and year is bigger than 1993.0, who is the delegate?\\n\\n### CONTEXT\\nCREATE TABLE table_1825751_14 (delegate VARCHAR, pageant VARCHAR, year VARCHAR)\\n\\n### ANSWER\\nSELECT delegate FROM table_1825751_14 WHERE pageant = \"Elite Model Look\" AND year > 1993.0</s>',\n  'source': {'answer': 'SELECT delegate FROM table_1825751_14 WHERE pageant = \"Elite Model Look\" AND year > 1993.0',\n   'context': 'CREATE TABLE table_1825751_14 (delegate VARCHAR, pageant VARCHAR, year VARCHAR)',\n   'question': 'where pageant is elite model look and year is bigger than 1993.0, who is the delegate?'}})"}, "metadata": {}}], "execution_count": 3}, {"cell_type": "code", "source": "# import json\n\n# def convert_to_json(dataset):\n#     json_data = []\n#     for example in dataset['train']['source']:\n#         json_data.append({\n#             'output': f\"### ANSWER\\n{example['answer']}\",\n#             'input': f\"### QUESTION\\n{example['question']}\\n\\n### CONTEXT\\n{example['context']}\",\n#             'instruction':f\"### CONTEXT\\n{example['context']}\"\n#         })\n#     return json_data\n\n# json_data = convert_to_json(dataset)\n# with open('sql_dataset.json', 'w') as f:\n#     json.dump(json_data, f, indent=4)", "metadata": {"id": "178d7c3c-5048-4f58-935b-f82d8210ec45"}, "outputs": [], "execution_count": 4}, {"cell_type": "code", "source": "!ls", "metadata": {"id": "3d504916-0a70-4536-a8fb-d02a5b76350d"}, "outputs": [{"name": "stdout", "text": "config.env\n", "output_type": "stream"}], "execution_count": 5}, {"cell_type": "code", "source": "import json\nfrom ibm_watson_studio_lib import access_project_or_space\n\n# Assuming your JSON file is named \"sql_dataset.json\"\nwith open(\"sql_dataset.json\", \"r\") as f:\n    json_data = json.load(f)\n\n\n# Upload the JSON file to your project as a data asset\nwslib.upload_file(\n    file_path=\"sql_dataset.json\",\n    asset_name=\"sql_dataset.json\",  # You can customize the name\n    overwrite=True  # Overwrite existing file with the same name (optional)\n)\n\nprint(\"Successfully uploaded sql_dataset.json to your project!\")", "metadata": {"id": "d19802c4-f5e8-42c3-83cd-2a1c28bbc3df"}, "outputs": [{"output_type": "stream", "text": "Successfully uploaded sql_dataset.json to your project!\n", "name": "stdout"}], "execution_count": 29}, {"cell_type": "code", "source": "from datasets import load_dataset\nimport json\n\n# Load the original dataset\ndataset = load_dataset(\"b-mc2/sql-create-context\")\nwslib.download_file('sql_dataset_cleaned.json')\n# Load your existing dataset to avoid duplicates\nwith open('sql_dataset_cleaned.json', 'r') as f:\n    existing_dataset = json.load(f)\n\n# Create a set of existing items (assuming each item is a dictionary and can be converted to a frozenset)\nexisting_set = set(frozenset(item.items()) for item in existing_dataset)\n\n# Filter the original dataset to find new unique samples\nunique_samples = []\nfor sample in dataset['train']:\n    sample_frozenset = frozenset(sample.items())\n    if sample_frozenset not in existing_set:\n        unique_samples.append(sample)\n    if len(unique_samples) == 500:\n        break\n\n# Save the unique samples to a new JSON file\nwith open('unique_sql_samples.json', 'w') as f:\n    json.dump(unique_samples, f, indent=4)\n\nprint(f\"Saved {len(unique_samples)} unique samples to 'unique_sql_samples.json'\")\n", "metadata": {"id": "f9665d76-7e52-4357-a2c2-4cc8a2fffc98"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading readme:   0%|          | 0.00/4.43k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1524aed98cca442497825a711b3be0fb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/21.8M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0c3545b77b01426eabca2d4c157cec86"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating train split:   0%|          | 0/78577 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3685032ce13340a89d3d512fbe548ce1"}}, "metadata": {}}, {"name": "stdout", "text": "Saved 500 unique samples to 'unique_sql_samples.json'\n", "output_type": "stream"}], "execution_count": 6}, {"cell_type": "code", "source": "!ls", "metadata": {"id": "61fc2c0d-fbf5-449f-9b6d-3d8ae9b8ef94"}, "outputs": [{"name": "stdout", "text": "config.env  sql_dataset_cleaned.json  unique_sql_samples.json\n", "output_type": "stream"}], "execution_count": 7}, {"cell_type": "code", "source": "# Upload the JSON file to your project as a data asset\nwslib.upload_file(\n    file_path=\"unique_sql_samples.json\",\n    asset_name=\"unique_sql_samples.json\",  # You can customize the name\n    overwrite=True  # Overwrite existing file with the same name (optional)\n)\n\nprint(\"Successfully uploaded sql_dataset.json to your project!\")", "metadata": {"id": "27605b88-1a91-4810-b3cb-79501cedcbdf"}, "outputs": [{"name": "stdout", "text": "Successfully uploaded sql_dataset.json to your project!\n", "output_type": "stream"}], "execution_count": 8}, {"cell_type": "code", "source": "import pandas as pd\nfrom datasets import Dataset\n\n# Your input data\ninput_data = [\n    {\n        \"output\": \"SELECT delegate FROM table_1825751_14 WHERE pageant = \\\"Elite Model Look\\\" AND year > 1993.0\",\n        \"input\": \"CREATE TABLE table_1825751_14 (delegate VARCHAR, pageant VARCHAR, year VARCHAR)\",\n        \"instruction\": \"where pageant is elite model look and year is bigger than 1993.0, who is the delegate?\"\n    }\n    # Add more items as needed\n]\n\n# Function to transform data\ndef transform_to_qca(input_data):\n    transformed_data = []\n\n    for item in input_data:\n        question = item[\"instruction\"]\n        context = item[\"input\"]\n        answer = item[\"output\"]\n\n        qca_text = f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n{answer}\"\n        qca_source = {\n            \"question\": question,\n            \"context\": context,\n            \"answer\": answer\n        }\n\n        transformed_data.append({\n            \"text\": qca_text,\n            \"source\": qca_source\n        })\n\n    return transformed_data\n\n# Transform the input data\ntransformed_data = transform_to_qca(input_data)\n\n# Create a single Dataset\ndataset = Dataset.from_pandas(pd.DataFrame(transformed_data))\n\n# Save the dataset\ndataset.save_to_disk(\"path_to_save_dataset\")\n\n# Upload to Hugging Face\nfrom datasets import load_from_disk\n\ndataset = load_from_disk(\"path_to_save_dataset\")\ndataset.push_to_hub(\"your-username/my-dataset\")\n", "metadata": {"id": "f7ddec95-bb96-4fd4-b450-5974df389ffc"}, "outputs": [], "execution_count": null}]}