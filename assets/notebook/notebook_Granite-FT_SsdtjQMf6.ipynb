{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "# %pip install -q -U bitsandbytes\n# %pip install datasets\n# %pip install\n# %pip install peft\n%pip install python-dotenv # Install the missing module 'dotenv'\n%pip install transformers datasets evaluate peft trl bitsandbytes accelerate\n%pip install huggingface\n", "metadata": {"scrolled": true, "id": "f7cc998e-bf23-48d4-987e-4e54f9c0aa6c"}, "outputs": [{"name": "stdout", "text": "Requirement already satisfied: python-dotenv in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (1.0.1)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (4.37.2)\nRequirement already satisfied: datasets in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (2.18.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: peft in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (0.6.0)\nCollecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: accelerate in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (0.33.0)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (0.23.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: requests in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (2.32.2)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from datasets) (15.0.1)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from datasets) (3.10.5)\nRequirement already satisfied: psutil in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from peft) (5.9.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from peft) (2.1.2)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (13.7.1)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.15.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, tyro, bitsandbytes, trl, evaluate\nSuccessfully installed bitsandbytes-0.43.3 evaluate-0.4.3 shtab-1.7.1 trl-0.10.1 tyro-0.8.10\nNote: you may need to restart the kernel to use updated packages.\nCollecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\nNote: you may need to restart the kernel to use updated packages.\n", "output_type": "stream"}], "execution_count": 1}, {"cell_type": "code", "source": "import os\nfrom dotenv import load_dotenv\nfrom ibm_cloud_sdk_core import IAMTokenManager\nfrom ibm_watson_studio_lib import access_project_or_space\n\nwslib = access_project_or_space({\n        'token': 'p-2+M7N412nNMq+LSbsjANoOoQ==;LFeF3V6i+F/jEnezq8oOQA==:l0bUpeHOW5rp8xq20UiCjJQyak+tK37f7uTyFZsV7YvvbFmQbYhtaO3KgtiCa1qahvIu57LYjESD5n0TXPH5u0ZHGef4njBD5A==',\n        'project_id': 'bdd13a82-ee92-406c-bc3d-fc0690f7cb1e'\n})\n\nwslib.download_file('config.env')\nload_dotenv('config.env')\n\n# Connection variables\napi_key = os.getenv(\"API_KEY\", None)\nibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None) \nproject_id = os.getenv(\"PROJECT_ID\", None)\ncreds = {\n    \"url\": ibm_cloud_url,\n    \"apikey\": api_key \n}\naccess_token = IAMTokenManager(\n    apikey = api_key,\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n).get_token()", "metadata": {"id": "a3c21715-c78b-4c78-a923-ec52e9e165db"}, "outputs": [], "execution_count": 1}, {"cell_type": "code", "source": "print(api_key)\n# print(access_token)\nwslib.download_file('tool.py')\nwslib.download_file('evaluating.py')\nwslib.download_file('save.py')\n# wslib.download_file('model_tracking.csv')", "metadata": {"id": "2f5c9e64-b762-478e-bbb5-aec558641639"}, "outputs": [{"name": "stdout", "text": "R4Ura8MOO50SxIpnaApHAlK5X0sJ9VyCvyPs91xTdmEK\n", "output_type": "stream"}, {"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "{'file_name': 'save.py', 'summary': ['loaded data', 'saved to file']}"}, "metadata": {}}], "execution_count": 2}, {"cell_type": "code", "source": "!ls", "metadata": {"id": "8841e45b-4343-4b52-85c5-be7874b08dc8"}, "outputs": [{"name": "stdout", "text": "config.env  evaluating.py  save.py  tool.py\n", "output_type": "stream"}], "execution_count": 3}, {"cell_type": "code", "source": "from huggingface_hub import login\n\nlogin(token=\"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\")", "metadata": {"scrolled": true, "id": "964690c3-41cf-45c0-aa43-937f5cf2d3f6"}, "outputs": [{"name": "stdout", "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /home/wsuser/.cache/huggingface/token\nLogin successful\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "code", "source": "def create_prompt_v3(question, context=None, EOS_TOKEN='</s>'):\n    PROMPT_DICT = {\n        \"prompt_context\": (\n            \"Below is an instruction (question) that describes a task, paired with an input (context) that provides further context. \"\n            \"Write an SQL query response that appropriately completes the request.\\n\\n\"\n            f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n\"\n        ),\n        \"prompt_no_context\": (\n            \"Below is an instruction (question) that describes a task. \"\n            \"Write an SQL query response that appropriately completes the request.\\n\\n\"\n            f\"### QUESTION\\n{question}\\n\\n### ANSWER\\n\"\n        ),\n    }\n    return PROMPT_DICT[\"prompt_context\"] if (context != None or context!='') else PROMPT_DICT[\"prompt_no_context\"]", "metadata": {"id": "45ca2aa3-743a-46f2-97df-0eaa547e80ee"}, "outputs": [], "execution_count": 5}, {"cell_type": "code", "source": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\n# Setup model and tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", return_dict=True, device_map='auto',\n#                                              torch_dtype=torch.float16, \n#                                              load_in_8bit=True\n                                            )\n", "metadata": {"scrolled": true, "id": "06c47334-4166-4bd0-8723-2cee94677802"}, "outputs": [{"name": "stderr", "text": "/opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8826f974f64044d3b4283620bbe22d8e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a5fe4ad1274144eca2659db37c80ef9f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c29c15548caf424890d4fb0b9006b84b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5dfae04dcc08493f885841d418708ee4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5d0d001bf4414196ac9827addff062f0"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "abc24904851047a38ca6c3b71af19e2e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "db9ad53b691945d2b67f13ef0a18e91f"}}, "metadata": {}}], "execution_count": 6}, {"cell_type": "code", "source": "HUGGING_FACE_USER_NAME = \"koukoudzz\"\n# HUGGING_FACE_USER_NAME='OussamaAzz'\n# model_name = \"granite-7b-base_sql-v0.2\"\nmodel_name = \"TinyLlama-1.1B-Chat-v1.0_sql-v1.3\"\n# # del sql_model\npeft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\nconfig2 = PeftConfig.from_pretrained(peft_model_id)\nsql_model = PeftModel.from_pretrained(model, peft_model_id)", "metadata": {"id": "b0444749-e940-4400-b376-59b1356334a6"}, "outputs": [], "execution_count": 7}, {"cell_type": "code", "source": "# import torch\n# from peft import PeftModel, PeftConfig\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from IPython.display import display, Markdown\n\n# def setup_and_infer(context, question, peft_model_id, model_name, HUGGING_FACE_USER_NAME, prompt =None):\n    \n#     def make_inference(context, question, prompt=prompt):\n#         global model  # Use nonlocal to modify sql_model within the nested function\n#         if prompt:\n#             print(\"i'm using prompt\")\n#             batch = tokenizer(prompt, return_tensors='pt')\n#         else:\n#             batch = tokenizer(f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n</s>\", return_tensors='pt')\n        \n#         device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#         # sql_model = sql_model.to(device)\n#         batch = {k: v.to(device) for k, v in batch.items()}\n        \n#         with torch.cuda.amp.autocast():\n#             output_tokens = model.generate(**batch, \n#                                                 max_new_tokens=100,\n#                                                 do_sample=True,\n#                                             )\n#         display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))\n    \n#     # Call the inference function\n#     make_inference(context, question)\n# #     del sql_model  # Delete the model to free up GPU memory\n# #     del model  # Delete the model to free up GPU memory\n# #     del tokenizer  # Delete the tokenizer to free up GPU memory\n\n\n# HUGGING_FACE_USER_NAME = \"koukoudzz\"\n# model_name = \"granite-7b-base_sql-v0.2\"\n# model_name = \"TinyLlama-1.1B-Chat-v1.0_sql-v0.9\"\n\n\n# i = 0\n# # Example usage\n# context = \"CREATE TABLE table_1818254_1 (violent_crime VARCHAR, robbery VARCHAR)\"\n# question = \"What was the violent crime rate in the city where the robbery rate was 201.4?\"\n\n# from datasets import load_dataset\n# dataset = load_dataset(\"OussamaAzz/final-sql-dataset\")\n# # setup_and_infer(context, question, \"peft_model_id\", model_name, HUGGING_FACE_USER_NAME)", "metadata": {"id": "2daa2cef-884a-4de5-8eec-8893763f8c8c"}, "outputs": [], "execution_count": 8}, {"cell_type": "code", "source": "# i = 13\n# context = dataset['test'][i]['source']['context']\n# question = dataset['test'][i]['source']['question']\n\n# prompt = create_prompt_v3(question, context)\n\n\n# setup_and_infer(context, question, \"peft_model_id\", model_name, HUGGING_FACE_USER_NAME, \n#                 prompt = prompt\n#                )\n# # !nvidia-smi", "metadata": {"id": "03e8ef6e-0937-4901-b59d-31f809b1d236"}, "outputs": [], "execution_count": 9}, {"cell_type": "code", "source": "# dataset['test'][i]['source']['answer']", "metadata": {"id": "0f00731f-db54-43da-bf0b-7875e90e5353"}, "outputs": [], "execution_count": 10}, {"cell_type": "code", "source": "# text = 'what is your name'\n# batch = tokenizer(prompt, return_tensors='pt')\n# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# # sql_model = sql_model.to(device)\n# batch = {k: v.to(device) for k, v in batch.items()}\n\n# with torch.cuda.amp.autocast():\n#         output_tokens = sql_model.generate(**batch, \n#                                             max_new_tokens=100,\n#                                             do_sample=False,\n#                                         )\n# display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))", "metadata": {"id": "bd52e5b5-03a5-4300-8113-84983ad33c5d"}, "outputs": [], "execution_count": 11}, {"cell_type": "code", "source": "# 1- Import the necessary libraries, and check if the GPU is available:\nimport torch\nimport transformers\nfrom datasets import Dataset\nfrom transformers import Trainer\nimport torch.nn as nn\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport json\nimport time\nimport os\nimport re\n\nimport tool, save\n# import evaluating\n\n\ntorch.cuda.is_available()", "metadata": {"id": "7cf00681-5519-4c65-a124-904f4e3aaf0a"}, "outputs": [{"name": "stderr", "text": "2024-09-12 06:54:33.463887: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-12 06:54:33.463917: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-12 06:54:33.463922: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n", "output_type": "stream"}, {"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "True"}, "metadata": {}}], "execution_count": 12}, {"cell_type": "code", "source": "# 2- Load the dataset:\n\nfrom datasets import load_dataset\n# dataset = load_dataset(\"OussamaAzz/sql_dataset_cleaned\")\ndataset = load_dataset(\"OussamaAzz/final-sql-dataset\")\n# dataset = load_dataset(\"OussamaAzz/instruction-sql-dataset\")b1\ndataset,'\\n',dataset['train'][0]", "metadata": {"id": "d1438292-5b01-44cb-8f3f-45e3104fb1e4"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading readme:   0%|          | 0.00/635 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bbe74666749a4841ad76d0a508687404"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/2.64M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "756f6bc970d34698960b617dd0321f30"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/145k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7ba7ff19205f4983ae2f0ea8797a315c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/104k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4c6c52c5092542719f1a8d38373f0179"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating train split:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "729309479e884b3f9909ee0a1913b2fe"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bf99441b916d44eba8c47e5a73089751"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2cd5c4662874484db419247e3fb93452"}}, "metadata": {}}, {"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "(DatasetDict({\n     train: Dataset({\n         features: ['text', 'source'],\n         num_rows: 9490\n     })\n     validation: Dataset({\n         features: ['text', 'source'],\n         num_rows: 500\n     })\n     test: Dataset({\n         features: ['text', 'source'],\n         num_rows: 500\n     })\n }),\n '\\n',\n {'text': '### QUESTION\\nWhat is the result on Sunday that\\'s \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\\n\\n### CONTEXT\\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\\n\\n### ANSWER\\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"',\n  'source': {'answer': 'SELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"',\n   'context': 'CREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)',\n   'question': \"What is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\"}})"}, "metadata": {}}], "execution_count": 13}, {"cell_type": "code", "source": "", "metadata": {"id": "866ad44a-c1b7-42c3-b1dc-5fba838fa732"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def replace_eos(text, tokenizer):\n    return {\"text\": [t.replace('</s>', tokenizer.eos_token) for t in text[\"text\"]]}\n\ndef add_eos_token(text, tokenizer):\n    return {\"text\": [t + tokenizer.eos_token for t in text[\"text\"]]}\n\ndataset['train'][\"text\"][0].replace('</s>','')\n", "metadata": {"id": "cb7a5c87-4323-4d61-a7ca-9483f11ebc15"}, "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "'### QUESTION\\nWhat is the result on Sunday that\\'s \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\\n\\n### CONTEXT\\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\\n\\n### ANSWER\\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"'"}, "metadata": {}}], "execution_count": 14}, {"cell_type": "code", "source": "# 3- Loading the model and tokenizer:\nload_in_16bit=torch.float16\nquantization = load_in_16bit\n# quantization = '8bit'\n# quantization = '4bit'\n# quantization = 'None'\n# base_model_name = 'TinyLlama/TinyLlama_v1.1'\n# base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nbase_model_name = 'ibm-granite/granite-7b-base'\n# base_model_name = 'granite-7b-base'\n# base_model_name = 'gpt2'\n# model_name = 'gpt2_sql-v0.7'\n# model_name = 'TinyLlama-1.1B-Chat-v1.0_sql-v1.5'\ntry:\n    model_name = base_model_name.split(\"/\")[1]\nexcept:\n    model_name=base_model_name\nmodel_name, _ = save.save_model_name(model_name, increment_version=False)\n\nmodel_params = tool.Model_params(\n    model_name=base_model_name,\n    transformer_from= 'auto',\n    quatization= quantization,\n    load_in_16bit=load_in_16bit,\n    token=\"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\"\n)\nmodel = model_params.load_model()\ntokenizer = model_params.load_tokenizer()", "metadata": {"id": "c2e79c15-51e6-4ca3-a1b8-cf8cde7c0c1a"}, "outputs": [{"name": "stderr", "text": "/opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7be72c1626254099b0abe81712d3489c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6ad246553ab94febbf198dc5baea21c2"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "144950b62d284698a87e2ee47c694766"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c452bef26a6d42b6bf227e13b407f700"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8acc02aefaee44af9796d51d475cdf30"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "53776ea80f6d4b918f9682a7c06e10f4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c97d3274094047c989cffa1b4766e69a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fae7852c83d44e13b1c396d59d1d5c00"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "model-00006-of-00006.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7c8a6299c01b4b7fb490f0a4dd7ef48e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a8302d7dd5014b51957a8746db121568"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ba9d60c16f6444f4bdeb6714023670a8"}}, "metadata": {}}, {"name": "stdout", "text": "Model loaded successfully\n", "output_type": "stream"}, {"name": "stderr", "text": "/opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/872 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "994a0565c6114025bed19c6fb89ded8f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d7bd5f3bc37b4284b4785e0d660e47ef"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0716a93223d44460a2359a597c9fc4a3"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2f99e64532e3460fb474063a2f6ddefd"}}, "metadata": {}}, {"name": "stdout", "text": "Tokenizer loaded successfully\n", "output_type": "stream"}], "execution_count": 15}, {"cell_type": "code", "source": "!nvidia-smi", "metadata": {"id": "7280c890-9963-417a-b843-2f3a9ce0c19e"}, "outputs": [{"name": "stdout", "text": "Thu Sep 12 07:09:16 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla V100-PCIE-16GB           Off |   00000000:AF:00.0 Off |                    0 |\n| N/A   33C    P0             39W /  250W |    6770MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla V100-PCIE-16GB           Off |   00000000:D8:00.0 Off |                    0 |\n| N/A   34C    P0             37W /  250W |    6770MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n", "output_type": "stream"}], "execution_count": 16}, {"cell_type": "code", "source": "# model_name = 'TinyLlama-1.1B-Chat-v1.0_sql-v1.5_PromptTuning'\n", "metadata": {"id": "032b5050-6f14-4468-a867-7b40da148ff0"}, "outputs": [], "execution_count": 17}, {"cell_type": "code", "source": "print(model)\nprint(tokenizer.eos_token)", "metadata": {"id": "22884bfb-1986-4bc6-8711-0e5c1d7cf472"}, "outputs": [{"name": "stdout", "text": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n</s>\n", "output_type": "stream"}], "execution_count": 18}, {"cell_type": "code", "source": "from functools import partial\n\nreplace_eos_with_tokenizer = partial(replace_eos, tokenizer=tokenizer)\nadd_eos_with_tokenizer = partial(add_eos_token, tokenizer=tokenizer)", "metadata": {"id": "da213fef-63a1-48b9-afe2-db0a5720fff4"}, "outputs": [], "execution_count": 19}, {"cell_type": "code", "source": "# 4- Tokenizing the dataset:\n\n# Function to tokenize inputs and align labels\ndef tokenize_function(examples):\n    tokenized_inputs = tokenizer(examples[\"text\"],\n                                padding=\"max_length\",\n                                truncation=True,\n                                max_length = 512,\n                                return_overflowing_tokens=False,\n                                 )\n    # labels = tokenized_inputs[\"input_ids\"].copy()  # Copy input_ids to use as labels\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"],\n            \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n            }\n\ntrain_dataset = dataset['train']\nval_dataset = dataset['validation']\n\ntrain_dataset = train_dataset.map(add_eos_with_tokenizer, batched=True)\nval_dataset = val_dataset.map(add_eos_with_tokenizer, batched=True)\nprint(train_dataset['text'][0])\n", "metadata": {"id": "b4881afa-1955-4edc-b919-7f67652dd153"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c7704bb313bc4e4abde5719c19c690d8"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "00c45793e77f4b939f3732907ff6da76"}}, "metadata": {}}, {"name": "stdout", "text": "### QUESTION\nWhat is the result on Sunday that's \u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r on Monday and \u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r on Tuesday?\n\n### CONTEXT\nCREATE TABLE table_name_29 (sunday_surya__the_sun_ VARCHAR, monday_soma__the_moon_ VARCHAR, tuesday_mangala__mars_ VARCHAR)\n\n### ANSWER\nSELECT sunday_surya__the_sun_ FROM table_name_29 WHERE monday_soma__the_moon_ = \"\u0938\u094b\u092e\u0935\u093e\u0930 somav\u0101r\" AND tuesday_mangala__mars_ = \"\u092e\u0902\u0917\u0932\u0935\u093e\u0930 mangalav\u0101r\"</s>\n", "output_type": "stream"}], "execution_count": 20}, {"cell_type": "code", "source": "# test_dataset = dataset['test']\n# test_dataset = test_dataset.map(add_eos_with_tokenizer, batched=True)\n# output = tool.generate_text(sql_model, tokenizer, tool.create_prompt_v2(test_dataset['source'][3]['question'], test_dataset['source'][3]['context']))\n# print(tool.create_prompt_with_answer_v2(**test_dataset['source'][3]))\n# print('\\n\\noutput= \\n'+output[0]+'</s>')", "metadata": {"id": "415e88f0-56ca-436c-aefb-299c87c6a802"}, "outputs": [], "execution_count": 21}, {"cell_type": "code", "source": "from datasets import Dataset\n\ntrain_dataset = dataset['train']\nval_dataset = dataset['validation']\n# Assume `tokenizer` is already defined and imported\nEOS_TOKEN = tokenizer.eos_token  # Ensure this is defined\n\ndef get_prompt(data, include_answer=False):\n    if include_answer:\n        prompt = tool.create_prompt_with_answer_v2(**data) + EOS_TOKEN\n    else:\n        prompt = tool.create_prompt_v2(data['question'],data['context'])\n    \n    return prompt\n\ndef get_answer(data):\n    return [d['answer'] + EOS_TOKEN for d in data]\n\n# Function to convert data to Dataset\ndef convert_to_dataset(data, tokenizer, include_labels=True, include_answer=False):\n    # Assume `tokenizer` is already defined and imported\n    EOS_TOKEN = tokenizer.eos_token  \n    # Assuming `tool.create_prompt_with_answer_v2(**d)` returns a string\n    # text = [tool.create_prompt_with_answer_v2(**d) + EOS_TOKEN for d in data]\n    text = [get_prompt(d, include_answer) for d in data]\n    answer = []\n    # if not include_answer:\n    answer = get_answer(data)\n    \n    if include_labels:\n        # Creating new labels for the dataset\n        labels = [i for i in range(len(data))]\n        return Dataset.from_dict({\"text\": text, \"labels\": labels, \"answer\": answer})\n    else:\n        return Dataset.from_dict({\"text\": text, \"answer\": answer})\n\n# Convert train and validation data to datasets\n# train_dataset = convert_to_dataset(data, include_labels=False, tokenizer=tokenizer, include_answer=True)\n# val_dataset = convert_to_dataset(val_data, include_labels=True, tokenizer=tokenizer, include_answer=False)\n\ntrain_dataset = convert_to_dataset(train_dataset['source'], include_labels=False, tokenizer=tokenizer, include_answer=True)\nval_dataset = convert_to_dataset(val_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=True)\n# test_dataset = convert_to_dataset(test_dataset['source'], include_labels=True, tokenizer=tokenizer, include_answer=False)\n\n", "metadata": {"id": "cb20a1bd-ff4d-4ec8-b9e7-4f09e57ae74e"}, "outputs": [], "execution_count": 22}, {"cell_type": "code", "source": "train_dataset[1]", "metadata": {"id": "9760c9c5-39ef-4af1-a175-40aedb719198"}, "outputs": [{"execution_count": 23, "output_type": "execute_result", "data": {"text/plain": "{'text': '### QUESTION\\nWhich player plays forward and is from Kentucky?\\n\\n### CONTEXT\\nCREATE TABLE table_name_43 (player VARCHAR, position VARCHAR, school_country VARCHAR)\\n\\n### ANSWER\\nSELECT player FROM table_name_43 WHERE position = \"forward\" AND school_country = \"kentucky\"</s></s>',\n 'answer': 'SELECT player FROM table_name_43 WHERE position = \"forward\" AND school_country = \"kentucky\"</s>'}"}, "metadata": {}}], "execution_count": 23}, {"cell_type": "code", "source": "tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\ntokenized_datasets_val = val_dataset.map(tokenize_function, batched=True)\n\n\n# tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"source\"])\n# tokenized_datasets_val = tokenized_datasets_val.remove_columns([\"text\", \"source\"])\n\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"answer\"])\ntokenized_datasets_val = tokenized_datasets_val.remove_columns([\"text\", \"answer\"])\n\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntokenized_datasets_val.set_format(type='torch', columns=['input_ids', 'attention_mask'])", "metadata": {"id": "75496771-e01c-427e-8598-d01686a695c9"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/9490 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f5a7d00a721f4faea09906ffc861727a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "733aacef3c5c44be9209240b3574998a"}}, "metadata": {}}], "execution_count": 24}, {"cell_type": "code", "source": "for param in model.parameters():\n  param.requires_grad = False  # freezing the model - train adapters later\n  if param.ndim == 1:\n    # casting the small parameters (e.g. layernorm) to fp32 for stability\n    param.data = param.data.to(torch.float32)\n\nmodel = model.train()\nmodel.gradient_checkpointing_enable()  # reducing number of stored activations\nmodel.enable_input_require_grads()\ntool.print_trainable_parameters(model)", "metadata": {"id": "b92d3ade-51bb-449b-826c-f3e5b925d778"}, "outputs": [{"name": "stdout", "text": "trainable params: 0 || all params: 6738415616 || trainable%: 0.0\n", "output_type": "stream"}], "execution_count": 25}, {"cell_type": "code", "source": "class CastOutputToFloat(nn.Sequential):\n    def forward(self, x):\n        # Ensure the conversion to float32 is out-of-place\n        return super().forward(x).to(torch.float32)\n\nmodel.lm_head = CastOutputToFloat(model.lm_head)", "metadata": {"id": "db85af24-e098-4399-ad4d-31cbe6b7c02e"}, "outputs": [], "execution_count": 26}, {"cell_type": "code", "source": "#### 5- setting up the LoRA Config:\nfrom peft import LoraConfig, get_peft_model\n# Hyperparameters for Lora\nr = [1,4,8,16,32,64,128,256,512]\nlora_alpha = [1,8,16,32,64,128,256,512,1024]\n\nconfig = LoraConfig(\n    r=8,# rank of Lora so matrices will have either LHS or RHS dimension of 64\n    lora_alpha=16, # multiplier of Lora output when its added to the full forward output\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],# list of modules to be replaced by Lora\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    # target_modules=[\"q_proj\"],\n#     target_modules=['c_attn','c_proj'],\n    # target_modules=['c_attn', 'c_proj', 'q_proj', 'k_proj', 'v_proj'],\n#     lora_dropout=0.10,# with a probability of 10% it will set random Lora output to 0\n    bias=\"none\", # no bias in Lora\n    task_type=\"CAUSAL_LM\" # task type of the model\n)\n\nmodel = get_peft_model(model, config)\n\ntool.print_trainable_parameters(model)\nmodel.print_trainable_parameters()", "metadata": {"id": "2a495ff7-8312-489d-99b4-f9dd3e92e49b"}, "outputs": [{"name": "stdout", "text": "trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2957573965106688\ntrainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2957573965106688\n", "output_type": "stream"}], "execution_count": 27}, {"cell_type": "code", "source": "# 6- Training arguments:\n\n# hyperparameters\nlr = 3e-4\n# lr = 5e-3\nbatch_size = 4\nnum_epochs = 10\n\n# define training arguments\ntraining_args = transformers.TrainingArguments(\n    output_dir= \"Tinyllama-lm\", # output directory\n    learning_rate=lr, # learning rate\n    per_device_train_batch_size=batch_size, # batch size per device during training\n    per_device_eval_batch_size=batch_size, # batch size for evaluation\n    num_train_epochs=num_epochs, # number of training epochs\n    weight_decay=0.01, # strength of weight decay - regularizes the weights (L2 regularization)\n    logging_strategy=\"steps\", # log results every 1 step\n    evaluation_strategy=\"steps\", # evaluate every 100 step\n    save_strategy=\"steps\", # save model every 100 steps\n    load_best_model_at_end=True, # load the best model when finished training (default is True)\n    # eval_on_start=True, # evaluate the model at the start of training\n    gradient_accumulation_steps=4, # accumulate gradients every 4 steps (equivalent to effective batch size of 64)\n    # max_steps=30,\n    warmup_steps=2, # number of warmup steps for learning rate scheduler\n    logging_steps=1, # log every 10 steps\n    eval_steps=100, # evaluate every 100 steps\n    fp16=True,\n    lr_scheduler_type=\"cosine\", # learning rate scheduler type\n    save_on_each_node=True,\n    # bf16=True,\n    # optim=\"paged_adamw_8bit\",\n    # save_on_each_node = True\n)\n\nplotting_callback = tool.PlottingCallback(model_name) # callback to plot metrics during training", "metadata": {"id": "15fda70e-1b2a-4827-957c-d068c517564f"}, "outputs": [], "execution_count": 28}, {"cell_type": "code", "source": "!nvidia-smi", "metadata": {"id": "997657d3-ae6a-4db3-b0fb-4787f01bbe0d"}, "outputs": [{"name": "stdout", "text": "Thu Sep 12 07:09:21 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla V100-PCIE-16GB           Off |   00000000:AF:00.0 Off |                    0 |\n| N/A   33C    P0             40W /  250W |    6868MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla V100-PCIE-16GB           Off |   00000000:D8:00.0 Off |                    0 |\n| N/A   34C    P0             37W /  250W |    6868MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n", "output_type": "stream"}], "execution_count": 29}, {"cell_type": "code", "source": "###### 7- Training the model:\nfrom bitsandbytes.optim import PagedAdamW8bit, AdamW8bit, SGD8bit, RMSprop8bit, PagedAdamW, AdamW, SGD, RMSprop\noptimizer = PagedAdamW(model.parameters(), lr=lr)\n# optimizer = AdamW(model.parameters(), lr=lr)\n# optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n# optimizer = RMSprop(model.parameters(), lr=lr)\n# optimizer = SGD8bit(model.parameters(), lr=lr, momentum=0.9)\n# optimizer = AdamW8bit(model.parameters(), lr=lr)\n# optimizer = RMSprop8bit(model.parameters(), lr=lr)\n# optimizer = PagedAdamW8bit(model.parameters(), lr=lr)\n# optimizer = PagedAdamW16bit(model.parameters(), lr=lr)\n\nfrom transformers import EarlyStoppingCallback\n\n# Define the early stopping callback\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=5,  # Number of evaluation steps with no improvement after which training will be stopped\n    early_stopping_threshold=0.01  # Minimum change in the monitored metric to qualify as an improvement\n)\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_datasets,\n    eval_dataset= tokenized_datasets_val,\n    args=training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # data collator for language modeling, mlm=False since we're not training a masked language model\n    optimizers=(optimizer, None),  # Use the 8-bit optimizer\n    callbacks=[plotting_callback,\n              early_stopping_callback,\n              ],    \n)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!bb", "metadata": {"id": "0b33b21c-9f8f-4637-b2f3-0acc0cf632c0"}, "outputs": [{"name": "stderr", "text": "/opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n  warnings.warn(\nDetected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n", "output_type": "stream"}], "execution_count": 30}, {"cell_type": "code", "source": "# !pip install --upgrade transformers\n# del model\n# del trainer\n# torch.clear_autocast_cache()\nmodel_name", "metadata": {"id": "c01e7329-bce7-4074-8409-93f2f669b871"}, "outputs": [{"execution_count": 31, "output_type": "execute_result", "data": {"text/plain": "'granite-7b-base_sql-v0.9'"}, "metadata": {}}], "execution_count": 31}, {"cell_type": "code", "source": "start_time = time.time()\n# with torch.autocast(\"cuda\"):\ntrainer.train()\ntraining_time = time.time() - start_time", "metadata": {"id": "89a4a164-9b6b-486f-af11-e53ce2a17e24", "msg_id": "f998d146-daad-4493-8ff9-cc5b0b92e1f3"}, "outputs": [{"name": "stderr", "text": "/opt/conda/envs/Python-RT24.1-CUDA/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "\n    <div>\n      \n      <progress value='301' max='5930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 301/5930 38:53 < 12:12:12, 0.13 it/s, Epoch 0.51/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.772600</td>\n      <td>0.738534</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.677500</td>\n      <td>0.698258</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='9' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  9/125 00:04 < 01:07, 1.72 it/s]\n    </div>\n    "}, "metadata": {}}], "execution_count": null}, {"cell_type": "code", "source": "# Get final evaluation metrics\nfinal_eval_metrics = trainer.evaluate()\n\n# Calculate perplexity from the final evaluation loss\nfinal_eval_loss = final_eval_metrics.get(\"eval_loss\")\nperplexity = torch.exp(torch.tensor(final_eval_loss)).item() if final_eval_loss is not None else None\n\n# 361100", "metadata": {"id": "eb3d4eb7-30e4-4bd0-8db4-ecb2e9d60a8c", "msg_id": "c5ada78c-7906-49bc-b3c8-28623e9f403b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Prepare data to save, including LoRA parameters\n# quantization = torch.float16\n# quantization = None\nresult_data = {\n    \"Model Name\": [model_name],\n    \"LoRA r\": [config.r],\n    \"LoRA Alpha\": [config.lora_alpha],\n    \"Batch Size\": batch_size,\n    \"LoRA Target Modules\": [\", \".join(config.target_modules)],\n    \"LoRA Dropout\": [config.lora_dropout],\n    \"Quantization\": [f\"Activated: {quantization}\" if quantization == '4bit' or quantization == '8bit' or quantization == torch.float16 else \"Not Activated\"],\n    \"Training Time (s)\": [training_time],\n    \"Memory (MB)\": [torch.cuda.max_memory_allocated() / (1024 ** 2)],\n    \"Final Eval Loss\": [final_eval_loss],\n    \"Perplexity\": [perplexity],\n    \"Optimizer\": [type(optimizer).__name__],\n}", "metadata": {"id": "44e625fd-d26b-4cd8-8e2b-bb56e67c2a98", "msg_id": "e998d0f7-7344-45f3-ab51-1d886f3f495b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import pandas as pd\n# Convert to DataFrame\ndf = pd.DataFrame(result_data)\n\n# Save to CSV file, append if the file exists\ncsv_file = \"training_results.csv\"\ntry:\n    existing_df = pd.read_csv(csv_file)\n#     df = pd.concat([existing_df, df], ignore_index=True)\nexcept FileNotFoundError:\n    pass  # No existing file, just write the new data\n\ndf.to_csv(csv_file, index=False)", "metadata": {"id": "cf4cc600-a3fb-4d04-bcfb-602aa45e00d7", "msg_id": "f9bd83e1-d13e-4fc5-a58f-1eab73151cf6"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# After training, save and upload plots\nplotting_callback.plot_final_metrics(\n    repo_id='koukoudzz/TinyLlama-1.1B-Chat-v1.0_sql-v0.0',\n    path_in_repo=f'plots/{model_name}'\n)\n", "metadata": {"id": "0c3933ae-5bfa-4196-b369-ba653945db55", "msg_id": "d0c5e617-6c7a-43b2-8776-56230c956c0b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "df", "metadata": {"id": "1ed6ed8e-f272-4ca6-95b3-acf3ec2f2eeb", "msg_id": "dc4484fd-26f2-4043-a865-4645c1933091"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "UN = 'koukoudzz'\nrepo_name = f'{UN}/{model_name}'\nmodel.push_to_hub(repo_name)", "metadata": {"id": "6150b71c-3fa5-40b5-8e19-8aaf7ed6714d", "msg_id": "50997493-6394-4e40-8ea1-ba4cc1e4fa6b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "save.upload_dataframe_to_huggingface(df,\n                                     repo_id='koukoudzz/TinyLlama-1.1B-Chat-v1.0_sql-v0.0',\n                                     path_in_repo='evaluation')", "metadata": {"id": "eb951d37-9228-41d8-a482-aa3365491a59", "msg_id": "f6901b79-22af-4c67-8a95-c7d9c134b48e"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from IPython.display import display, Markdown\n\ndef make_inference(context, question, model):\n#     global sql_model  # Declare sql_model as global if it's defined outside this function\n    model.eval()\n    batch = tokenizer(f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n\", return_tensors='pt')\n#     batch = tokenizer(f\"### INSTRUCTION\\n{context}\\n\\n### INPUT\\n{question}\\n\\n### OUTPUT\\n\", return_tensors='pt')\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # model = model.to(device)  # Now sql_model is recognized within the function scope\n    batch = {k: v.to(device) for k, v in batch.items()}\n    \n    # Your inference code here\n    with torch.cuda.amp.autocast():\n        output_tokens = model.generate(**batch, \n                                            max_new_tokens=200,\n#                                             repetition_penalty=1.5,\n                                            # temperature=0.9,\n#                                             do_sample = False,\n#                                             eos_token_id=tokenizer.eos_token_id,\n#                                             pad_token_id=tokenizer.eos_token_id,\n                                       num_return_sequences=1,  # Generate a single sequence\n                                        )\n\n    display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))\n    ", "metadata": {"id": "be335b64-7dfa-40f5-91d5-ef21fdaaf836", "msg_id": "684b3e96-602a-4849-8d90-acb889262bc8"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "i =17\nmodel.config.use_cache = True  # silence the warnings. Please re-enable for inference!\ncontext = dataset['test'][i]['source']['context']\n# context = \"\"\nquestion = dataset['test'][i]['source']['question']\ntext=f\"### QUESTION\\n{question}\\n\\n### CONTEXT\\n{context}\\n\\n### ANSWER\\n\"\n\nmake_inference(context, question, model)\n# print(generate_text_v2(model, tokenizer, text)[0])\ndataset['test'][i]['source']['answer']", "metadata": {"id": "dbb55837-73ce-4837-aff9-451bdc00a488", "msg_id": "49d71c3a-32a0-41f7-8684-721293840e64"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "f928bf8f-4947-4c34-a3df-5f5863b18739"}, "outputs": [], "execution_count": null}]}