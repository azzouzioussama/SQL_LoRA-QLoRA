Model Name,LoRA r,LoRA Alpha,Batch Size,LoRA Target Modules,LoRA Dropout,Quantization,Training Time (s),Memory (MB),Final Eval Loss,Perplexity,Optimizer
gpt2_sql-v3.4,8,16,4,c_attn,0.0,Not Activated,1801.6603746414185,1465.34423828125,1.0442343950271606,2.841222286224365,AdamW
gpt2_sql-v3.4,8,16,16,c_attn,0.0,Not Activated,731.4697597026825,4967.81591796875,1.2124439477920532,3.3616905212402344,AdamW
distilgpt2_sql-v3.4,8,16,16,c_attn,0.0,Not Activated,911.7397892475128,4803.91748046875,1.2905056476593018,3.6346240043640137,AdamW
distilgpt2_sql-v3.4,8,16,16,c_attn,0.0,Not Activated,1351.0101544857023,4803.91748046875,1.2396844625473022,3.4545233249664307,AdamW
distilgpt2_sql-v3.4,16,32,16,c_attn,0.0,Not Activated,450.359402179718,4806.19873046875,1.3838642835617063,3.990291595458984,AdamW
distilgpt2_sql-v3.4,16,32,16,c_attn,0.0,Not Activated,1734.8511171340942,4806.44873046875,1.275287389755249,3.579730033874512,AdamW
distilgpt2_sql-v3.4,16,32,4,"c_attn, c_proj",0.0,Not Activated,2411.0439732074738,4806.44873046875,0.945856750011444,2.5750186443328857,AdamW
distilgpt2_sql-v3.4,16,32,4,"c_attn, c_proj",0.0,Not Activated,2485.9317665100098,1354.61767578125,0.9433879852294922,2.568669319152832,AdamW
distilgpt2_sql-v3.4,16,32,4,"c_attn, c_proj",0.0,Not Activated,2416.629090309143,1354.61767578125,0.9607000946998596,2.613525629043579,AdamW
distilgpt2_sql-v3.4,16,32,16,"c_proj, c_attn",0.05,Not Activated,3135.8621730804443,5351.90625,1.0809980630874634,2.947619915008545,AdamW
TinyLlama-lm_sql-v0.0,16,32,16,"q_proj, v_proj, k_proj",0.05,Not Activated,6100.803897380829,9132.37890625,0.7189128398895264,2.05220103263855,AdamW
TinyLlama-lm_sql-v0.0,16,32,4,"k_proj, v_proj, q_proj",0.05,Not Activated,1810.5833933353424,5576.23486328125,0.7071629762649536,2.028228998184204,AdamW
TinyLlama-lm_sql-v0.0,16,32,4,"v_proj, q_proj, k_proj",0.05,Not Activated,4127.666873455048,5576.23486328125,0.7070037722587585,2.0279061794281006,AdamW
